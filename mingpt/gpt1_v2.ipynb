{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # context length\n",
    "n_head = 8 # number of heads in each block\n",
    "n_embd = 40 # embedding dimension\n",
    "# ~added later~\n",
    "block_size = 32 # how many independent seq in parallel?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "eval_iters = 200\n",
    "learning_rate = 1e-3\n",
    "n_layer = 4 # number of blocks to use\n",
    "dropout = 0.0 # portion of neurons to turn off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "with open('../input.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n"
     ]
    }
   ],
   "source": [
    "# gpu stuff\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(\"using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10dcfe550>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensure reproducibility\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode/decode\n",
    "stoi = {ch: i for i, ch in enumerate(sorted(list(set(text))))}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "encode = lambda x: [stoi[ch] for ch in x]\n",
    "decode = lambda x: ''.join([itos[i] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "train_portion = int(0.9 * len(text))\n",
    "train_data = torch.tensor(encode(text[:train_portion]), dtype=torch.long)\n",
    "test_data = torch.tensor(encode(text[train_portion:]), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else test_data\n",
    "    idx = torch.randint(len(data) - block_size, (batch_size,)) # MODIFIED: this is data, not text\n",
    "    x = torch.stack([data[i:i+block_size] for i in idx])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split)\n",
    "            logits, loss = model(X,Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        # head_embd = n_embd // n_head\n",
    "        # self.query = nn.Linear(head_size, head_embd)\n",
    "        # self.key = nn.Linear(head_size, head_embd)\n",
    "        # self.value = nn.Linear(head_size, head_embd)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        # self.register_buffer('tril', torch.tril(torch.ones(head_embd, head_embd)).float())\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape # MODIFIED: this is an attribute and not module\n",
    "        q,k,v = self.query(x), self.key(x), self.value(x)\n",
    "\n",
    "        wei = (q @ k.transpose(-2, -1)) * (C ** -0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # MODIFIED: (B, T, T); :T ensures that the mask accounts for the fact that there can be less than max number of elems in time dimension\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        out = wei @ v\n",
    "\n",
    "        return out\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd) # ???\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd * 4),\n",
    "            nn.ReLU(), # MODIFIED: forgot to add activation in betn\n",
    "            nn.Linear(n_embd * 4, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head # ??? n_embd used in multi headed attention as well whereas\n",
    "        self.sa = MultiHeadedAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "\n",
    "        token_embedding = self.embedding(idx)\n",
    "        pos_embedding = self.position_embedding_table(torch.arange(T, device=device)) # ???; why time arange\n",
    "        x = token_embedding + pos_embedding\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1) # softmax at channel dimension\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # predict next token\n",
    "            idx = torch.concat([idx, idx_next], dim=1)\n",
    "        \n",
    "        return idx\n",
    "    \n",
    "    def get_regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
    "        reg_loss = 0\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, KANLinear):\n",
    "                reg_loss += module.regularization_loss(regularize_activation, regularize_entropy)\n",
    "        return reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.086345 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# print no. of parameters\n",
    "print(sum([p.numel() for p in m.parameters()]) / 1e6, 'M parameters')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: train loss 4.285674571990967, val loss 4.289457321166992\n",
      "step 2: train loss 4.196816444396973, val loss 4.207517623901367\n",
      "step 3: train loss 4.11887788772583, val loss 4.130380153656006\n",
      "step 4: train loss 4.0449018478393555, val loss 4.060515880584717\n",
      "step 5: train loss 3.9812417030334473, val loss 3.993649959564209\n",
      "step 6: train loss 3.923485040664673, val loss 3.9397077560424805\n",
      "step 7: train loss 3.866415023803711, val loss 3.8789801597595215\n",
      "step 8: train loss 3.821033239364624, val loss 3.834258794784546\n",
      "step 9: train loss 3.772547721862793, val loss 3.788651466369629\n",
      "step 10: train loss 3.7325491905212402, val loss 3.7481331825256348\n",
      "step 11: train loss 3.6951141357421875, val loss 3.7103090286254883\n",
      "step 12: train loss 3.6581296920776367, val loss 3.678115129470825\n",
      "step 13: train loss 3.6305978298187256, val loss 3.6451568603515625\n",
      "step 14: train loss 3.5982468128204346, val loss 3.6149377822875977\n",
      "step 15: train loss 3.5652356147766113, val loss 3.594982624053955\n",
      "step 16: train loss 3.544816255569458, val loss 3.569385051727295\n",
      "step 17: train loss 3.5275824069976807, val loss 3.542753219604492\n",
      "step 18: train loss 3.5040740966796875, val loss 3.5168352127075195\n",
      "step 19: train loss 3.473661422729492, val loss 3.494313955307007\n",
      "step 20: train loss 3.4488298892974854, val loss 3.480104923248291\n",
      "step 21: train loss 3.4363186359405518, val loss 3.464702844619751\n",
      "step 22: train loss 3.4243531227111816, val loss 3.439445734024048\n",
      "step 23: train loss 3.4113636016845703, val loss 3.423147678375244\n",
      "step 24: train loss 3.388136863708496, val loss 3.416378974914551\n",
      "step 25: train loss 3.3772475719451904, val loss 3.4033212661743164\n",
      "step 26: train loss 3.366849422454834, val loss 3.390608310699463\n",
      "step 27: train loss 3.3560540676116943, val loss 3.3720951080322266\n",
      "step 28: train loss 3.3361268043518066, val loss 3.3627374172210693\n",
      "step 29: train loss 3.3386456966400146, val loss 3.3560595512390137\n",
      "step 30: train loss 3.313793659210205, val loss 3.3444747924804688\n",
      "step 31: train loss 3.311095952987671, val loss 3.337656259536743\n",
      "step 32: train loss 3.300563097000122, val loss 3.3368477821350098\n",
      "step 33: train loss 3.3033552169799805, val loss 3.3154213428497314\n",
      "step 34: train loss 3.2849884033203125, val loss 3.3189496994018555\n",
      "step 35: train loss 3.2840116024017334, val loss 3.3126015663146973\n",
      "step 36: train loss 3.2703235149383545, val loss 3.300647020339966\n",
      "step 37: train loss 3.269808292388916, val loss 3.296388626098633\n",
      "step 38: train loss 3.2464683055877686, val loss 3.288816213607788\n",
      "step 39: train loss 3.244873285293579, val loss 3.275496244430542\n",
      "step 40: train loss 3.2402820587158203, val loss 3.2746050357818604\n",
      "step 41: train loss 3.2339975833892822, val loss 3.2739815711975098\n",
      "step 42: train loss 3.2265894412994385, val loss 3.270082473754883\n",
      "step 43: train loss 3.2184085845947266, val loss 3.2471697330474854\n",
      "step 44: train loss 3.2109875679016113, val loss 3.2499890327453613\n",
      "step 45: train loss 3.2081515789031982, val loss 3.237111806869507\n",
      "step 46: train loss 3.1889734268188477, val loss 3.223069667816162\n",
      "step 47: train loss 3.185399055480957, val loss 3.2099838256835938\n",
      "step 48: train loss 3.180039644241333, val loss 3.211883544921875\n",
      "step 49: train loss 3.172248601913452, val loss 3.20501708984375\n",
      "step 50: train loss 3.1606433391571045, val loss 3.1953160762786865\n",
      "step 51: train loss 3.158432722091675, val loss 3.185175895690918\n",
      "step 52: train loss 3.1430504322052, val loss 3.176781415939331\n",
      "step 53: train loss 3.1322262287139893, val loss 3.1622073650360107\n",
      "step 54: train loss 3.144012451171875, val loss 3.1616406440734863\n",
      "step 55: train loss 3.114100933074951, val loss 3.1528425216674805\n",
      "step 56: train loss 3.112680435180664, val loss 3.1477813720703125\n",
      "step 57: train loss 3.103262424468994, val loss 3.1399121284484863\n",
      "step 58: train loss 3.1003525257110596, val loss 3.120965003967285\n",
      "step 59: train loss 3.0826666355133057, val loss 3.1291327476501465\n",
      "step 60: train loss 3.0835952758789062, val loss 3.115631103515625\n",
      "step 61: train loss 3.06123423576355, val loss 3.1019787788391113\n",
      "step 62: train loss 3.064572811126709, val loss 3.0914783477783203\n",
      "step 63: train loss 3.052945613861084, val loss 3.0870795249938965\n",
      "step 64: train loss 3.04921293258667, val loss 3.0694940090179443\n",
      "step 65: train loss 3.0484306812286377, val loss 3.0674524307250977\n",
      "step 66: train loss 3.050205707550049, val loss 3.0672788619995117\n",
      "step 67: train loss 3.0298242568969727, val loss 3.056565284729004\n",
      "step 68: train loss 3.0305092334747314, val loss 3.0508272647857666\n",
      "step 69: train loss 3.011810064315796, val loss 3.0502922534942627\n",
      "step 70: train loss 3.0149011611938477, val loss 3.0376904010772705\n",
      "step 71: train loss 3.0051462650299072, val loss 3.023344039916992\n",
      "step 72: train loss 2.9987587928771973, val loss 3.0202112197875977\n",
      "step 73: train loss 2.9881839752197266, val loss 3.0125200748443604\n",
      "step 74: train loss 2.9780383110046387, val loss 3.0027430057525635\n",
      "step 75: train loss 2.9742321968078613, val loss 2.994687080383301\n",
      "step 76: train loss 2.9733359813690186, val loss 2.9806630611419678\n",
      "step 77: train loss 2.958867073059082, val loss 2.9793920516967773\n",
      "step 78: train loss 2.953376531600952, val loss 2.973783016204834\n",
      "step 79: train loss 2.940931797027588, val loss 2.9621896743774414\n",
      "step 80: train loss 2.93265438079834, val loss 2.9524452686309814\n",
      "step 81: train loss 2.935920476913452, val loss 2.9465091228485107\n",
      "step 82: train loss 2.921956777572632, val loss 2.9537711143493652\n",
      "step 83: train loss 2.926389694213867, val loss 2.9422659873962402\n",
      "step 84: train loss 2.9199440479278564, val loss 2.9350640773773193\n",
      "step 85: train loss 2.89707088470459, val loss 2.924067974090576\n",
      "step 86: train loss 2.9008185863494873, val loss 2.919405937194824\n",
      "step 87: train loss 2.900665521621704, val loss 2.918464422225952\n",
      "step 88: train loss 2.8882381916046143, val loss 2.9111979007720947\n",
      "step 89: train loss 2.8870744705200195, val loss 2.902719020843506\n",
      "step 90: train loss 2.879157066345215, val loss 2.8962180614471436\n",
      "step 91: train loss 2.8712899684906006, val loss 2.895533800125122\n",
      "step 92: train loss 2.8625905513763428, val loss 2.898959159851074\n",
      "step 93: train loss 2.8698227405548096, val loss 2.8865115642547607\n",
      "step 94: train loss 2.8665390014648438, val loss 2.889247417449951\n",
      "step 95: train loss 2.860588788986206, val loss 2.8710591793060303\n",
      "step 96: train loss 2.858527898788452, val loss 2.869189977645874\n",
      "step 97: train loss 2.84883189201355, val loss 2.8594915866851807\n",
      "step 98: train loss 2.8421010971069336, val loss 2.8719592094421387\n",
      "step 99: train loss 2.8431854248046875, val loss 2.8549628257751465\n",
      "step 101: train loss 2.8345117568969727, val loss 2.8513662815093994\n",
      "step 102: train loss 2.824409246444702, val loss 2.8327388763427734\n",
      "step 103: train loss 2.822316884994507, val loss 2.83681583404541\n",
      "step 104: train loss 2.8180694580078125, val loss 2.8298652172088623\n",
      "step 105: train loss 2.8101015090942383, val loss 2.827214241027832\n",
      "step 106: train loss 2.8091177940368652, val loss 2.8239026069641113\n",
      "step 107: train loss 2.801520347595215, val loss 2.8111395835876465\n",
      "step 108: train loss 2.7932217121124268, val loss 2.8084230422973633\n",
      "step 109: train loss 2.79549503326416, val loss 2.8104915618896484\n",
      "step 110: train loss 2.794809579849243, val loss 2.7995946407318115\n",
      "step 111: train loss 2.778014898300171, val loss 2.802168846130371\n",
      "step 112: train loss 2.783201217651367, val loss 2.801605463027954\n",
      "step 113: train loss 2.7824862003326416, val loss 2.794776678085327\n",
      "step 114: train loss 2.7720258235931396, val loss 2.7903690338134766\n",
      "step 115: train loss 2.7684028148651123, val loss 2.7796196937561035\n",
      "step 116: train loss 2.7591726779937744, val loss 2.7820053100585938\n",
      "step 117: train loss 2.776798963546753, val loss 2.775954008102417\n",
      "step 118: train loss 2.760493516921997, val loss 2.7810912132263184\n",
      "step 119: train loss 2.7626571655273438, val loss 2.780149459838867\n",
      "step 120: train loss 2.7559659481048584, val loss 2.774021625518799\n",
      "step 121: train loss 2.748509168624878, val loss 2.7650322914123535\n",
      "step 122: train loss 2.7533605098724365, val loss 2.766418218612671\n",
      "step 123: train loss 2.7447595596313477, val loss 2.7605292797088623\n",
      "step 124: train loss 2.7429141998291016, val loss 2.7510335445404053\n",
      "step 125: train loss 2.740175485610962, val loss 2.7488443851470947\n",
      "step 126: train loss 2.736828088760376, val loss 2.7591381072998047\n",
      "step 127: train loss 2.7371044158935547, val loss 2.7424349784851074\n",
      "step 128: train loss 2.7314867973327637, val loss 2.7491164207458496\n",
      "step 129: train loss 2.731703519821167, val loss 2.7341229915618896\n",
      "step 130: train loss 2.7278547286987305, val loss 2.737435817718506\n",
      "step 131: train loss 2.7174582481384277, val loss 2.733218193054199\n",
      "step 132: train loss 2.713555097579956, val loss 2.735264301300049\n",
      "step 133: train loss 2.729701519012451, val loss 2.7376646995544434\n",
      "step 134: train loss 2.721130132675171, val loss 2.7332558631896973\n",
      "step 135: train loss 2.7200984954833984, val loss 2.7243106365203857\n",
      "step 136: train loss 2.7119534015655518, val loss 2.7214653491973877\n",
      "step 137: train loss 2.7100753784179688, val loss 2.721388339996338\n",
      "step 138: train loss 2.704361915588379, val loss 2.7220592498779297\n",
      "step 139: train loss 2.695702314376831, val loss 2.7168710231781006\n",
      "step 140: train loss 2.7024686336517334, val loss 2.7205328941345215\n",
      "step 141: train loss 2.699927568435669, val loss 2.7088747024536133\n",
      "step 142: train loss 2.7024085521698, val loss 2.7044451236724854\n",
      "step 143: train loss 2.6868958473205566, val loss 2.702075719833374\n",
      "step 144: train loss 2.6876838207244873, val loss 2.6956045627593994\n",
      "step 145: train loss 2.6919989585876465, val loss 2.6968624591827393\n",
      "step 146: train loss 2.6887242794036865, val loss 2.691847324371338\n",
      "step 147: train loss 2.685093402862549, val loss 2.6919643878936768\n",
      "step 148: train loss 2.682893753051758, val loss 2.6917035579681396\n",
      "step 149: train loss 2.676027536392212, val loss 2.6885335445404053\n",
      "step 150: train loss 2.6727828979492188, val loss 2.6830103397369385\n",
      "step 151: train loss 2.6754114627838135, val loss 2.6817362308502197\n",
      "step 152: train loss 2.6700377464294434, val loss 2.6825640201568604\n",
      "step 153: train loss 2.6719765663146973, val loss 2.6843883991241455\n",
      "step 154: train loss 2.677809238433838, val loss 2.6843996047973633\n",
      "step 155: train loss 2.676999568939209, val loss 2.6759324073791504\n",
      "step 156: train loss 2.671539306640625, val loss 2.671409845352173\n",
      "step 157: train loss 2.6602611541748047, val loss 2.675881862640381\n",
      "step 158: train loss 2.668118953704834, val loss 2.6742968559265137\n",
      "step 159: train loss 2.664660692214966, val loss 2.665706157684326\n",
      "step 160: train loss 2.664477586746216, val loss 2.656465530395508\n",
      "step 161: train loss 2.659475088119507, val loss 2.659144878387451\n",
      "step 162: train loss 2.6585700511932373, val loss 2.6649162769317627\n",
      "step 163: train loss 2.6588797569274902, val loss 2.6605844497680664\n",
      "step 164: train loss 2.6619536876678467, val loss 2.6582155227661133\n",
      "step 165: train loss 2.658003568649292, val loss 2.656583547592163\n",
      "step 166: train loss 2.6567916870117188, val loss 2.6536059379577637\n",
      "step 167: train loss 2.644907236099243, val loss 2.6528077125549316\n",
      "step 168: train loss 2.6458940505981445, val loss 2.655365228652954\n",
      "step 169: train loss 2.649334669113159, val loss 2.6494951248168945\n",
      "step 170: train loss 2.6583070755004883, val loss 2.6574485301971436\n",
      "step 171: train loss 2.658902645111084, val loss 2.657111406326294\n",
      "step 172: train loss 2.648655414581299, val loss 2.6540443897247314\n",
      "step 173: train loss 2.6435136795043945, val loss 2.6483516693115234\n",
      "step 174: train loss 2.6300625801086426, val loss 2.6483395099639893\n",
      "step 175: train loss 2.6329565048217773, val loss 2.6502861976623535\n",
      "step 176: train loss 2.6459102630615234, val loss 2.643418073654175\n",
      "step 177: train loss 2.6408050060272217, val loss 2.6525139808654785\n",
      "step 178: train loss 2.6400146484375, val loss 2.652794599533081\n",
      "step 179: train loss 2.6265485286712646, val loss 2.6477770805358887\n",
      "step 180: train loss 2.6349382400512695, val loss 2.64658784866333\n",
      "step 181: train loss 2.6321778297424316, val loss 2.6385855674743652\n",
      "step 182: train loss 2.6290395259857178, val loss 2.623818874359131\n",
      "step 183: train loss 2.631953477859497, val loss 2.635314702987671\n",
      "step 184: train loss 2.626880407333374, val loss 2.635477304458618\n",
      "step 185: train loss 2.6221208572387695, val loss 2.6275320053100586\n",
      "step 186: train loss 2.612187385559082, val loss 2.6229357719421387\n",
      "step 187: train loss 2.630244493484497, val loss 2.623370409011841\n",
      "step 188: train loss 2.6237564086914062, val loss 2.631220817565918\n",
      "step 189: train loss 2.620676279067993, val loss 2.628977060317993\n",
      "step 190: train loss 2.6254396438598633, val loss 2.6415655612945557\n",
      "step 191: train loss 2.6168134212493896, val loss 2.627342939376831\n",
      "step 192: train loss 2.618562936782837, val loss 2.6392738819122314\n",
      "step 193: train loss 2.62021541595459, val loss 2.631063461303711\n",
      "step 194: train loss 2.6181702613830566, val loss 2.6219122409820557\n",
      "step 195: train loss 2.605560302734375, val loss 2.6171562671661377\n",
      "step 196: train loss 2.6206252574920654, val loss 2.6177031993865967\n",
      "step 197: train loss 2.6163697242736816, val loss 2.619442939758301\n",
      "step 198: train loss 2.6085801124572754, val loss 2.6145331859588623\n",
      "step 199: train loss 2.608440637588501, val loss 2.60514760017395\n",
      "step 201: train loss 2.606905937194824, val loss 2.616553544998169\n",
      "step 202: train loss 2.6055397987365723, val loss 2.606233596801758\n",
      "step 203: train loss 2.6047003269195557, val loss 2.6014904975891113\n",
      "step 204: train loss 2.6070940494537354, val loss 2.603983163833618\n",
      "step 205: train loss 2.5987603664398193, val loss 2.601356267929077\n",
      "step 206: train loss 2.5871782302856445, val loss 2.6078295707702637\n",
      "step 207: train loss 2.5950756072998047, val loss 2.6048471927642822\n",
      "step 208: train loss 2.5922014713287354, val loss 2.592256546020508\n",
      "step 209: train loss 2.5964715480804443, val loss 2.5904700756073\n",
      "step 210: train loss 2.5917458534240723, val loss 2.596160888671875\n",
      "step 211: train loss 2.5923967361450195, val loss 2.599161148071289\n",
      "step 212: train loss 2.5945982933044434, val loss 2.59912109375\n",
      "step 213: train loss 2.594784736633301, val loss 2.601104736328125\n",
      "step 214: train loss 2.5876352787017822, val loss 2.6010260581970215\n",
      "step 215: train loss 2.594209909439087, val loss 2.593829870223999\n",
      "step 216: train loss 2.594111919403076, val loss 2.598914384841919\n",
      "step 217: train loss 2.594158887863159, val loss 2.5994462966918945\n",
      "step 218: train loss 2.592406988143921, val loss 2.5960562229156494\n",
      "step 219: train loss 2.5919060707092285, val loss 2.5955569744110107\n",
      "step 220: train loss 2.585902214050293, val loss 2.5968592166900635\n",
      "step 221: train loss 2.5818352699279785, val loss 2.590818405151367\n",
      "step 222: train loss 2.5842831134796143, val loss 2.5899388790130615\n",
      "step 223: train loss 2.58573055267334, val loss 2.5883069038391113\n",
      "step 224: train loss 2.583386182785034, val loss 2.5928893089294434\n",
      "step 225: train loss 2.5791988372802734, val loss 2.587270736694336\n",
      "step 226: train loss 2.58561372756958, val loss 2.5876967906951904\n",
      "step 227: train loss 2.585440158843994, val loss 2.586184024810791\n",
      "step 228: train loss 2.5836260318756104, val loss 2.585374116897583\n",
      "step 229: train loss 2.5841639041900635, val loss 2.590174913406372\n",
      "step 230: train loss 2.5881054401397705, val loss 2.587291955947876\n",
      "step 231: train loss 2.568678855895996, val loss 2.586334466934204\n",
      "step 232: train loss 2.5771844387054443, val loss 2.5838000774383545\n",
      "step 233: train loss 2.579935312271118, val loss 2.5955588817596436\n",
      "step 234: train loss 2.585970401763916, val loss 2.5895190238952637\n",
      "step 235: train loss 2.578118085861206, val loss 2.5886740684509277\n",
      "step 236: train loss 2.5708069801330566, val loss 2.5843682289123535\n",
      "step 237: train loss 2.570781946182251, val loss 2.579857110977173\n",
      "step 238: train loss 2.572789192199707, val loss 2.582224130630493\n",
      "step 239: train loss 2.5768556594848633, val loss 2.583446741104126\n",
      "step 240: train loss 2.5726935863494873, val loss 2.5876150131225586\n",
      "step 241: train loss 2.5749707221984863, val loss 2.5757229328155518\n",
      "step 242: train loss 2.5686798095703125, val loss 2.5794951915740967\n",
      "step 243: train loss 2.5722906589508057, val loss 2.5715179443359375\n",
      "step 244: train loss 2.5766096115112305, val loss 2.580080032348633\n",
      "step 245: train loss 2.575809955596924, val loss 2.581963539123535\n",
      "step 246: train loss 2.5697412490844727, val loss 2.5726735591888428\n",
      "step 247: train loss 2.5613510608673096, val loss 2.571216344833374\n",
      "step 248: train loss 2.551180839538574, val loss 2.5621860027313232\n",
      "step 249: train loss 2.5619707107543945, val loss 2.5790910720825195\n",
      "step 250: train loss 2.565823554992676, val loss 2.5645902156829834\n",
      "step 251: train loss 2.5531387329101562, val loss 2.5724730491638184\n",
      "step 252: train loss 2.5532703399658203, val loss 2.570896863937378\n",
      "step 253: train loss 2.5611090660095215, val loss 2.568114995956421\n",
      "step 254: train loss 2.5557618141174316, val loss 2.569009304046631\n",
      "step 255: train loss 2.558497667312622, val loss 2.566668748855591\n",
      "step 256: train loss 2.555600643157959, val loss 2.5658557415008545\n",
      "step 257: train loss 2.5515599250793457, val loss 2.5695807933807373\n",
      "step 258: train loss 2.5525028705596924, val loss 2.567633867263794\n",
      "step 259: train loss 2.556166887283325, val loss 2.558384418487549\n",
      "step 260: train loss 2.549225091934204, val loss 2.5686545372009277\n",
      "step 261: train loss 2.5547804832458496, val loss 2.5713741779327393\n",
      "step 262: train loss 2.556541681289673, val loss 2.5625829696655273\n",
      "step 263: train loss 2.5548131465911865, val loss 2.567037582397461\n",
      "step 264: train loss 2.5583596229553223, val loss 2.56496524810791\n",
      "step 265: train loss 2.553828716278076, val loss 2.5587666034698486\n",
      "step 266: train loss 2.5485424995422363, val loss 2.5622341632843018\n",
      "step 267: train loss 2.5567586421966553, val loss 2.567108392715454\n",
      "step 268: train loss 2.553786516189575, val loss 2.565476894378662\n",
      "step 269: train loss 2.5575289726257324, val loss 2.5687954425811768\n",
      "step 270: train loss 2.5481836795806885, val loss 2.570873498916626\n",
      "step 271: train loss 2.547194004058838, val loss 2.566610097885132\n",
      "step 272: train loss 2.5533535480499268, val loss 2.55881404876709\n",
      "step 273: train loss 2.5493578910827637, val loss 2.5567634105682373\n",
      "step 274: train loss 2.5522301197052, val loss 2.5612921714782715\n",
      "step 275: train loss 2.551736354827881, val loss 2.5500295162200928\n",
      "step 276: train loss 2.54898738861084, val loss 2.5535600185394287\n",
      "step 277: train loss 2.547956943511963, val loss 2.555039644241333\n",
      "step 278: train loss 2.5435683727264404, val loss 2.5532774925231934\n",
      "step 279: train loss 2.5498781204223633, val loss 2.5545480251312256\n",
      "step 280: train loss 2.540529251098633, val loss 2.5488317012786865\n",
      "step 281: train loss 2.5379505157470703, val loss 2.5498342514038086\n",
      "step 282: train loss 2.549456834793091, val loss 2.561873435974121\n",
      "step 283: train loss 2.5426816940307617, val loss 2.5620694160461426\n",
      "step 284: train loss 2.5455098152160645, val loss 2.551954746246338\n",
      "step 285: train loss 2.532851219177246, val loss 2.5574288368225098\n",
      "step 286: train loss 2.537381649017334, val loss 2.5559685230255127\n",
      "step 287: train loss 2.539799690246582, val loss 2.552415132522583\n",
      "step 288: train loss 2.5407676696777344, val loss 2.552493095397949\n",
      "step 289: train loss 2.5334110260009766, val loss 2.5394887924194336\n",
      "step 290: train loss 2.536696434020996, val loss 2.54660701751709\n",
      "step 291: train loss 2.542844772338867, val loss 2.552884578704834\n",
      "step 292: train loss 2.5499372482299805, val loss 2.554192543029785\n",
      "step 293: train loss 2.5493032932281494, val loss 2.55094051361084\n",
      "step 294: train loss 2.544854164123535, val loss 2.5508060455322266\n",
      "step 295: train loss 2.5406439304351807, val loss 2.545668363571167\n",
      "step 296: train loss 2.532505989074707, val loss 2.5490269660949707\n",
      "step 297: train loss 2.524399995803833, val loss 2.5390191078186035\n",
      "step 298: train loss 2.5324811935424805, val loss 2.5465619564056396\n",
      "step 299: train loss 2.5378971099853516, val loss 2.5400094985961914\n",
      "step 301: train loss 2.5319156646728516, val loss 2.5359601974487305\n",
      "step 302: train loss 2.527130126953125, val loss 2.5345187187194824\n",
      "step 303: train loss 2.5237722396850586, val loss 2.530212163925171\n",
      "step 304: train loss 2.5271666049957275, val loss 2.5400969982147217\n",
      "step 305: train loss 2.5267505645751953, val loss 2.5333852767944336\n",
      "step 306: train loss 2.511693239212036, val loss 2.5261950492858887\n",
      "step 307: train loss 2.516737461090088, val loss 2.5267670154571533\n",
      "step 308: train loss 2.5253801345825195, val loss 2.5286011695861816\n",
      "step 309: train loss 2.526491165161133, val loss 2.5314393043518066\n",
      "step 310: train loss 2.5357935428619385, val loss 2.5284688472747803\n",
      "step 311: train loss 2.5247273445129395, val loss 2.5354528427124023\n",
      "step 312: train loss 2.530508518218994, val loss 2.5344972610473633\n",
      "step 313: train loss 2.5228939056396484, val loss 2.5353856086730957\n",
      "step 314: train loss 2.529543876647949, val loss 2.5353755950927734\n",
      "step 315: train loss 2.5239920616149902, val loss 2.535352945327759\n",
      "step 316: train loss 2.515549421310425, val loss 2.535137891769409\n",
      "step 317: train loss 2.5240044593811035, val loss 2.5286099910736084\n",
      "step 318: train loss 2.5193910598754883, val loss 2.530089855194092\n",
      "step 319: train loss 2.5148873329162598, val loss 2.52229380607605\n",
      "step 320: train loss 2.518035650253296, val loss 2.521266222000122\n",
      "step 321: train loss 2.5189530849456787, val loss 2.513700246810913\n",
      "step 322: train loss 2.517688035964966, val loss 2.5160858631134033\n",
      "step 323: train loss 2.5112967491149902, val loss 2.51446270942688\n",
      "step 324: train loss 2.5124399662017822, val loss 2.5259954929351807\n",
      "step 325: train loss 2.5104446411132812, val loss 2.5264012813568115\n",
      "step 326: train loss 2.5021753311157227, val loss 2.5219435691833496\n",
      "step 327: train loss 2.504441738128662, val loss 2.5194437503814697\n",
      "step 328: train loss 2.504767417907715, val loss 2.5143654346466064\n",
      "step 329: train loss 2.5066730976104736, val loss 2.5183498859405518\n",
      "step 330: train loss 2.516998291015625, val loss 2.5273494720458984\n",
      "step 331: train loss 2.5073091983795166, val loss 2.5141937732696533\n",
      "step 332: train loss 2.5197060108184814, val loss 2.5095255374908447\n",
      "step 333: train loss 2.5020089149475098, val loss 2.5057363510131836\n",
      "step 334: train loss 2.4996497631073, val loss 2.5173845291137695\n",
      "step 335: train loss 2.508690595626831, val loss 2.5141472816467285\n",
      "step 336: train loss 2.511017322540283, val loss 2.509890079498291\n",
      "step 337: train loss 2.502286434173584, val loss 2.504518747329712\n",
      "step 338: train loss 2.5068979263305664, val loss 2.511547088623047\n",
      "step 339: train loss 2.5100574493408203, val loss 2.508462905883789\n",
      "step 340: train loss 2.509629726409912, val loss 2.5070691108703613\n",
      "step 341: train loss 2.5050880908966064, val loss 2.518019676208496\n",
      "step 342: train loss 2.4982786178588867, val loss 2.5095181465148926\n",
      "step 343: train loss 2.5099546909332275, val loss 2.5058021545410156\n",
      "step 344: train loss 2.496521234512329, val loss 2.503185510635376\n",
      "step 345: train loss 2.4992549419403076, val loss 2.5107054710388184\n",
      "step 346: train loss 2.5045864582061768, val loss 2.505261182785034\n",
      "step 347: train loss 2.503821849822998, val loss 2.5117766857147217\n",
      "step 348: train loss 2.502615213394165, val loss 2.511758327484131\n",
      "step 349: train loss 2.506357192993164, val loss 2.515943765640259\n",
      "step 350: train loss 2.4928832054138184, val loss 2.500154972076416\n",
      "step 351: train loss 2.480424165725708, val loss 2.496567726135254\n",
      "step 352: train loss 2.4858639240264893, val loss 2.5078279972076416\n",
      "step 353: train loss 2.506162166595459, val loss 2.500983238220215\n",
      "step 354: train loss 2.489028215408325, val loss 2.504559278488159\n",
      "step 355: train loss 2.497468948364258, val loss 2.495626926422119\n",
      "step 356: train loss 2.495116710662842, val loss 2.4966063499450684\n",
      "step 357: train loss 2.4934463500976562, val loss 2.493666172027588\n",
      "step 358: train loss 2.492727756500244, val loss 2.502006769180298\n",
      "step 359: train loss 2.489438533782959, val loss 2.495328426361084\n",
      "step 360: train loss 2.4877405166625977, val loss 2.5064423084259033\n",
      "step 361: train loss 2.4882924556732178, val loss 2.491035223007202\n",
      "step 362: train loss 2.489759922027588, val loss 2.494697093963623\n",
      "step 363: train loss 2.4923818111419678, val loss 2.4934847354888916\n",
      "step 364: train loss 2.484269618988037, val loss 2.495459794998169\n",
      "step 365: train loss 2.4858851432800293, val loss 2.48685359954834\n",
      "step 366: train loss 2.4832754135131836, val loss 2.4906766414642334\n",
      "step 367: train loss 2.4811742305755615, val loss 2.4857709407806396\n",
      "step 368: train loss 2.4874095916748047, val loss 2.4904732704162598\n",
      "step 369: train loss 2.4833767414093018, val loss 2.492544651031494\n",
      "step 370: train loss 2.4894790649414062, val loss 2.5006957054138184\n",
      "step 371: train loss 2.4833905696868896, val loss 2.498995780944824\n",
      "step 372: train loss 2.48524808883667, val loss 2.487863302230835\n",
      "step 373: train loss 2.475724697113037, val loss 2.489492416381836\n",
      "step 374: train loss 2.4803268909454346, val loss 2.4922666549682617\n",
      "step 375: train loss 2.486823081970215, val loss 2.491426706314087\n",
      "step 376: train loss 2.475743055343628, val loss 2.4850690364837646\n",
      "step 377: train loss 2.4851644039154053, val loss 2.5003104209899902\n",
      "step 378: train loss 2.5028154850006104, val loss 2.508199453353882\n",
      "step 379: train loss 2.494035005569458, val loss 2.507020950317383\n",
      "step 380: train loss 2.4834370613098145, val loss 2.498077869415283\n",
      "step 381: train loss 2.484651565551758, val loss 2.4954493045806885\n",
      "step 382: train loss 2.485013961791992, val loss 2.481217861175537\n",
      "step 383: train loss 2.4857637882232666, val loss 2.4913949966430664\n",
      "step 384: train loss 2.4769530296325684, val loss 2.492051124572754\n",
      "step 385: train loss 2.4810547828674316, val loss 2.4859259128570557\n",
      "step 386: train loss 2.4688658714294434, val loss 2.4832351207733154\n",
      "step 387: train loss 2.4732871055603027, val loss 2.4775986671447754\n",
      "step 388: train loss 2.471205949783325, val loss 2.4794132709503174\n",
      "step 389: train loss 2.4724652767181396, val loss 2.48264217376709\n",
      "step 390: train loss 2.4698872566223145, val loss 2.4783501625061035\n",
      "step 391: train loss 2.471454620361328, val loss 2.477832078933716\n",
      "step 392: train loss 2.4664697647094727, val loss 2.4804205894470215\n",
      "step 393: train loss 2.4737987518310547, val loss 2.480442762374878\n",
      "step 394: train loss 2.4669744968414307, val loss 2.4786994457244873\n",
      "step 395: train loss 2.471499443054199, val loss 2.4840657711029053\n",
      "step 396: train loss 2.4706859588623047, val loss 2.478266954421997\n",
      "step 397: train loss 2.4729325771331787, val loss 2.4783005714416504\n",
      "step 398: train loss 2.469229221343994, val loss 2.4710452556610107\n",
      "step 399: train loss 2.4697208404541016, val loss 2.4708120822906494\n",
      "step 401: train loss 2.4685616493225098, val loss 2.4792282581329346\n",
      "step 402: train loss 2.47031307220459, val loss 2.472090721130371\n",
      "step 403: train loss 2.470012664794922, val loss 2.482499599456787\n",
      "step 404: train loss 2.4673643112182617, val loss 2.486058235168457\n",
      "step 405: train loss 2.456521987915039, val loss 2.4720029830932617\n",
      "step 406: train loss 2.4666924476623535, val loss 2.4670562744140625\n",
      "step 407: train loss 2.467519998550415, val loss 2.474214553833008\n",
      "step 408: train loss 2.464073657989502, val loss 2.4742937088012695\n",
      "step 409: train loss 2.4647204875946045, val loss 2.4696526527404785\n",
      "step 410: train loss 2.462141990661621, val loss 2.4796481132507324\n",
      "step 411: train loss 2.462986946105957, val loss 2.4763383865356445\n",
      "step 412: train loss 2.4690213203430176, val loss 2.479555368423462\n",
      "step 413: train loss 2.4784271717071533, val loss 2.4774763584136963\n",
      "step 414: train loss 2.4714272022247314, val loss 2.477736473083496\n",
      "step 415: train loss 2.4583749771118164, val loss 2.4666526317596436\n",
      "step 416: train loss 2.456071376800537, val loss 2.4693658351898193\n",
      "step 417: train loss 2.463568687438965, val loss 2.4703869819641113\n",
      "step 418: train loss 2.4677093029022217, val loss 2.4722084999084473\n",
      "step 419: train loss 2.4641268253326416, val loss 2.473958969116211\n",
      "step 420: train loss 2.463587760925293, val loss 2.47062611579895\n",
      "step 421: train loss 2.458284378051758, val loss 2.468862295150757\n",
      "step 422: train loss 2.4485390186309814, val loss 2.469762086868286\n",
      "step 423: train loss 2.461660146713257, val loss 2.474168062210083\n",
      "step 424: train loss 2.461010217666626, val loss 2.4659218788146973\n",
      "step 425: train loss 2.4744739532470703, val loss 2.470440626144409\n",
      "step 426: train loss 2.46441912651062, val loss 2.4708268642425537\n",
      "step 427: train loss 2.4706575870513916, val loss 2.4782822132110596\n",
      "step 428: train loss 2.4709579944610596, val loss 2.4733786582946777\n",
      "step 429: train loss 2.4615259170532227, val loss 2.4681944847106934\n",
      "step 430: train loss 2.4637744426727295, val loss 2.4652974605560303\n",
      "step 431: train loss 2.452934741973877, val loss 2.4682700634002686\n",
      "step 432: train loss 2.456122636795044, val loss 2.4654979705810547\n",
      "step 433: train loss 2.4572603702545166, val loss 2.4635977745056152\n",
      "step 434: train loss 2.4589250087738037, val loss 2.464155912399292\n",
      "step 435: train loss 2.452044725418091, val loss 2.4672060012817383\n",
      "step 436: train loss 2.453324794769287, val loss 2.4696106910705566\n",
      "step 437: train loss 2.4647083282470703, val loss 2.459399700164795\n",
      "step 438: train loss 2.461521863937378, val loss 2.470527172088623\n",
      "step 439: train loss 2.456273317337036, val loss 2.4574615955352783\n",
      "step 440: train loss 2.454634428024292, val loss 2.463658332824707\n",
      "step 441: train loss 2.4508795738220215, val loss 2.4575836658477783\n",
      "step 442: train loss 2.452939987182617, val loss 2.461224317550659\n",
      "step 443: train loss 2.4529294967651367, val loss 2.460930824279785\n",
      "step 444: train loss 2.453648090362549, val loss 2.4563117027282715\n",
      "step 445: train loss 2.4554834365844727, val loss 2.455197334289551\n",
      "step 446: train loss 2.4497110843658447, val loss 2.4570043087005615\n",
      "step 447: train loss 2.446805000305176, val loss 2.458517074584961\n",
      "step 448: train loss 2.4548821449279785, val loss 2.4523909091949463\n",
      "step 449: train loss 2.4436402320861816, val loss 2.4563028812408447\n",
      "step 450: train loss 2.445103406906128, val loss 2.455042839050293\n",
      "step 451: train loss 2.4501235485076904, val loss 2.4469282627105713\n",
      "step 452: train loss 2.435899257659912, val loss 2.4465460777282715\n",
      "step 453: train loss 2.4432358741760254, val loss 2.4543962478637695\n",
      "step 454: train loss 2.4392855167388916, val loss 2.454979181289673\n",
      "step 455: train loss 2.453341007232666, val loss 2.4577927589416504\n",
      "step 456: train loss 2.4434471130371094, val loss 2.4529030323028564\n",
      "step 457: train loss 2.44083309173584, val loss 2.4604339599609375\n",
      "step 458: train loss 2.442105770111084, val loss 2.458456039428711\n",
      "step 459: train loss 2.448287010192871, val loss 2.456611394882202\n",
      "step 460: train loss 2.4480857849121094, val loss 2.454928398132324\n",
      "step 461: train loss 2.4477102756500244, val loss 2.4524292945861816\n",
      "step 462: train loss 2.435471773147583, val loss 2.451293706893921\n",
      "step 463: train loss 2.443284034729004, val loss 2.4479148387908936\n",
      "step 464: train loss 2.4400365352630615, val loss 2.451920509338379\n",
      "step 465: train loss 2.4407153129577637, val loss 2.4487192630767822\n",
      "step 466: train loss 2.434502601623535, val loss 2.4506349563598633\n",
      "step 467: train loss 2.442835807800293, val loss 2.4508285522460938\n",
      "step 468: train loss 2.434892416000366, val loss 2.4595794677734375\n",
      "step 469: train loss 2.4386518001556396, val loss 2.4509639739990234\n",
      "step 470: train loss 2.4417476654052734, val loss 2.458360195159912\n",
      "step 471: train loss 2.439568519592285, val loss 2.449500322341919\n",
      "step 472: train loss 2.434419870376587, val loss 2.459794044494629\n",
      "step 473: train loss 2.441283941268921, val loss 2.4543204307556152\n",
      "step 474: train loss 2.4478087425231934, val loss 2.4684460163116455\n",
      "step 475: train loss 2.4610745906829834, val loss 2.4569387435913086\n",
      "step 476: train loss 2.4383111000061035, val loss 2.457176446914673\n",
      "step 477: train loss 2.436497688293457, val loss 2.4390532970428467\n",
      "step 478: train loss 2.4352853298187256, val loss 2.4413206577301025\n",
      "step 479: train loss 2.4426393508911133, val loss 2.4402565956115723\n",
      "step 480: train loss 2.4379138946533203, val loss 2.4487650394439697\n",
      "step 481: train loss 2.431342124938965, val loss 2.445458173751831\n",
      "step 482: train loss 2.430307149887085, val loss 2.441551923751831\n",
      "step 483: train loss 2.4256813526153564, val loss 2.448525905609131\n",
      "step 484: train loss 2.426572561264038, val loss 2.448831081390381\n",
      "step 485: train loss 2.4314374923706055, val loss 2.45045804977417\n",
      "step 486: train loss 2.433016538619995, val loss 2.4441583156585693\n",
      "step 487: train loss 2.4325966835021973, val loss 2.441208839416504\n",
      "step 488: train loss 2.4292616844177246, val loss 2.4353160858154297\n",
      "step 489: train loss 2.434727430343628, val loss 2.4459452629089355\n",
      "step 490: train loss 2.4325788021087646, val loss 2.4292380809783936\n",
      "step 491: train loss 2.4287078380584717, val loss 2.4375107288360596\n",
      "step 492: train loss 2.4355998039245605, val loss 2.4371559619903564\n",
      "step 493: train loss 2.4298157691955566, val loss 2.4331603050231934\n",
      "step 494: train loss 2.4330074787139893, val loss 2.450839042663574\n",
      "step 495: train loss 2.429338216781616, val loss 2.436173677444458\n",
      "step 496: train loss 2.43815016746521, val loss 2.4386205673217773\n",
      "step 497: train loss 2.4350979328155518, val loss 2.447336435317993\n",
      "step 498: train loss 2.431300640106201, val loss 2.4430060386657715\n",
      "step 499: train loss 2.433759927749634, val loss 2.442347288131714\n",
      "step 501: train loss 2.4265832901000977, val loss 2.436540126800537\n",
      "step 502: train loss 2.425143003463745, val loss 2.4339635372161865\n",
      "step 503: train loss 2.4240589141845703, val loss 2.4448511600494385\n",
      "step 504: train loss 2.4265778064727783, val loss 2.4451398849487305\n",
      "step 505: train loss 2.4323627948760986, val loss 2.444422721862793\n",
      "step 506: train loss 2.4356796741485596, val loss 2.4444870948791504\n",
      "step 507: train loss 2.4245221614837646, val loss 2.4393486976623535\n",
      "step 508: train loss 2.4305877685546875, val loss 2.4408180713653564\n",
      "step 509: train loss 2.4289581775665283, val loss 2.4503519535064697\n",
      "step 510: train loss 2.436063289642334, val loss 2.457406997680664\n",
      "step 511: train loss 2.4342398643493652, val loss 2.4558353424072266\n",
      "step 512: train loss 2.4272987842559814, val loss 2.455388069152832\n",
      "step 513: train loss 2.4367589950561523, val loss 2.4519705772399902\n",
      "step 514: train loss 2.4308183193206787, val loss 2.4514620304107666\n",
      "step 515: train loss 2.4194629192352295, val loss 2.442807197570801\n",
      "step 516: train loss 2.42427134513855, val loss 2.442613124847412\n",
      "step 517: train loss 2.424142360687256, val loss 2.438103437423706\n",
      "step 518: train loss 2.4263181686401367, val loss 2.4381587505340576\n",
      "step 519: train loss 2.431924819946289, val loss 2.4473438262939453\n",
      "step 520: train loss 2.4259653091430664, val loss 2.439614772796631\n",
      "step 521: train loss 2.4285728931427, val loss 2.4413139820098877\n",
      "step 522: train loss 2.4244790077209473, val loss 2.441092014312744\n",
      "step 523: train loss 2.430124282836914, val loss 2.439943313598633\n",
      "step 524: train loss 2.4312903881073, val loss 2.450164794921875\n",
      "step 525: train loss 2.4278104305267334, val loss 2.447070837020874\n",
      "step 526: train loss 2.4211947917938232, val loss 2.439603567123413\n",
      "step 527: train loss 2.415926933288574, val loss 2.4288299083709717\n",
      "step 528: train loss 2.4215810298919678, val loss 2.443422317504883\n",
      "step 529: train loss 2.417124032974243, val loss 2.4416868686676025\n",
      "step 530: train loss 2.422926664352417, val loss 2.4427919387817383\n",
      "step 531: train loss 2.419288396835327, val loss 2.4365711212158203\n",
      "step 532: train loss 2.4105424880981445, val loss 2.431135416030884\n",
      "step 533: train loss 2.4103410243988037, val loss 2.4269213676452637\n",
      "step 534: train loss 2.4191319942474365, val loss 2.427828788757324\n",
      "step 535: train loss 2.416996717453003, val loss 2.4319815635681152\n",
      "step 536: train loss 2.4218130111694336, val loss 2.429002285003662\n",
      "step 537: train loss 2.418123245239258, val loss 2.4253427982330322\n",
      "step 538: train loss 2.415757179260254, val loss 2.4283671379089355\n",
      "step 539: train loss 2.4084227085113525, val loss 2.417853593826294\n",
      "step 540: train loss 2.40779447555542, val loss 2.4180634021759033\n",
      "step 541: train loss 2.408475875854492, val loss 2.430094003677368\n",
      "step 542: train loss 2.412013053894043, val loss 2.4276814460754395\n",
      "step 543: train loss 2.3979878425598145, val loss 2.424208402633667\n",
      "step 544: train loss 2.416992425918579, val loss 2.4391117095947266\n",
      "step 545: train loss 2.412839651107788, val loss 2.4369165897369385\n",
      "step 546: train loss 2.4163410663604736, val loss 2.4267120361328125\n",
      "step 547: train loss 2.4105985164642334, val loss 2.420732021331787\n",
      "step 548: train loss 2.4119365215301514, val loss 2.429825782775879\n",
      "step 549: train loss 2.4125497341156006, val loss 2.4236721992492676\n",
      "step 550: train loss 2.4110536575317383, val loss 2.4247875213623047\n",
      "step 551: train loss 2.4114367961883545, val loss 2.423382043838501\n",
      "step 552: train loss 2.405675172805786, val loss 2.41841197013855\n",
      "step 553: train loss 2.4090139865875244, val loss 2.419867992401123\n",
      "step 554: train loss 2.412764310836792, val loss 2.4286577701568604\n",
      "step 555: train loss 2.4016175270080566, val loss 2.41567063331604\n",
      "step 556: train loss 2.397796392440796, val loss 2.4227547645568848\n",
      "step 557: train loss 2.4015555381774902, val loss 2.4227592945098877\n",
      "step 558: train loss 2.3986010551452637, val loss 2.4230546951293945\n",
      "step 559: train loss 2.401454210281372, val loss 2.427517890930176\n",
      "step 560: train loss 2.4060757160186768, val loss 2.4317879676818848\n",
      "step 561: train loss 2.401780843734741, val loss 2.4190683364868164\n",
      "step 562: train loss 2.4014766216278076, val loss 2.4204294681549072\n",
      "step 563: train loss 2.406827926635742, val loss 2.423849582672119\n",
      "step 564: train loss 2.406707286834717, val loss 2.4225213527679443\n",
      "step 565: train loss 2.4036219120025635, val loss 2.4280498027801514\n",
      "step 566: train loss 2.410841464996338, val loss 2.428718328475952\n",
      "step 567: train loss 2.4101712703704834, val loss 2.4178647994995117\n",
      "step 568: train loss 2.406977653503418, val loss 2.4242610931396484\n",
      "step 569: train loss 2.4056708812713623, val loss 2.4202680587768555\n",
      "step 570: train loss 2.4037625789642334, val loss 2.413823127746582\n",
      "step 571: train loss 2.3991799354553223, val loss 2.42440128326416\n",
      "step 572: train loss 2.401392936706543, val loss 2.415585517883301\n",
      "step 573: train loss 2.4040606021881104, val loss 2.4301154613494873\n",
      "step 574: train loss 2.4064993858337402, val loss 2.4190149307250977\n",
      "step 575: train loss 2.4056694507598877, val loss 2.4161689281463623\n",
      "step 576: train loss 2.403963088989258, val loss 2.4170587062835693\n",
      "step 577: train loss 2.4061977863311768, val loss 2.418313503265381\n",
      "step 578: train loss 2.4060490131378174, val loss 2.4217302799224854\n",
      "step 579: train loss 2.4024479389190674, val loss 2.4199745655059814\n",
      "step 580: train loss 2.3983893394470215, val loss 2.413855791091919\n",
      "step 581: train loss 2.4015941619873047, val loss 2.410696029663086\n",
      "step 582: train loss 2.39375638961792, val loss 2.4117276668548584\n",
      "step 583: train loss 2.4034082889556885, val loss 2.4094715118408203\n",
      "step 584: train loss 2.404322862625122, val loss 2.402982234954834\n",
      "step 585: train loss 2.3928911685943604, val loss 2.4148993492126465\n",
      "step 586: train loss 2.3973214626312256, val loss 2.405646562576294\n",
      "step 587: train loss 2.3965704441070557, val loss 2.4041965007781982\n",
      "step 588: train loss 2.387589693069458, val loss 2.409085273742676\n",
      "step 589: train loss 2.396759033203125, val loss 2.402568817138672\n",
      "step 590: train loss 2.4033751487731934, val loss 2.404876947402954\n",
      "step 591: train loss 2.401742458343506, val loss 2.4163098335266113\n",
      "step 592: train loss 2.405904531478882, val loss 2.4073565006256104\n",
      "step 593: train loss 2.404853105545044, val loss 2.41611647605896\n",
      "step 594: train loss 2.4003334045410156, val loss 2.4181432723999023\n",
      "step 595: train loss 2.4074294567108154, val loss 2.418534994125366\n",
      "step 596: train loss 2.4018824100494385, val loss 2.4098260402679443\n",
      "step 597: train loss 2.39412784576416, val loss 2.4096450805664062\n",
      "step 598: train loss 2.4036760330200195, val loss 2.4100687503814697\n",
      "step 599: train loss 2.4018144607543945, val loss 2.410884141921997\n",
      "step 601: train loss 2.396125555038452, val loss 2.400240659713745\n",
      "step 602: train loss 2.3929758071899414, val loss 2.403315544128418\n",
      "step 603: train loss 2.3973891735076904, val loss 2.4189939498901367\n",
      "step 604: train loss 2.395552158355713, val loss 2.4062485694885254\n",
      "step 605: train loss 2.3931119441986084, val loss 2.4110288619995117\n",
      "step 606: train loss 2.395277738571167, val loss 2.4136056900024414\n",
      "step 607: train loss 2.3893988132476807, val loss 2.397026538848877\n",
      "step 608: train loss 2.3859870433807373, val loss 2.4011425971984863\n",
      "step 609: train loss 2.3933067321777344, val loss 2.4047274589538574\n",
      "step 610: train loss 2.396547317504883, val loss 2.411766767501831\n",
      "step 611: train loss 2.4009039402008057, val loss 2.4059908390045166\n",
      "step 612: train loss 2.3936662673950195, val loss 2.4050133228302\n",
      "step 613: train loss 2.395592451095581, val loss 2.4068427085876465\n",
      "step 614: train loss 2.3842766284942627, val loss 2.4076154232025146\n",
      "step 615: train loss 2.3896398544311523, val loss 2.395423650741577\n",
      "step 616: train loss 2.385551691055298, val loss 2.396327018737793\n",
      "step 617: train loss 2.379704713821411, val loss 2.3988380432128906\n",
      "step 618: train loss 2.3926424980163574, val loss 2.3931634426116943\n",
      "step 619: train loss 2.390516757965088, val loss 2.3960578441619873\n",
      "step 620: train loss 2.3789353370666504, val loss 2.4037909507751465\n",
      "step 621: train loss 2.389094829559326, val loss 2.3987114429473877\n",
      "step 622: train loss 2.383613348007202, val loss 2.400451898574829\n",
      "step 623: train loss 2.3882367610931396, val loss 2.4085397720336914\n",
      "step 624: train loss 2.389843225479126, val loss 2.405282497406006\n",
      "step 625: train loss 2.394503593444824, val loss 2.4042882919311523\n",
      "step 626: train loss 2.3912084102630615, val loss 2.402468204498291\n",
      "step 627: train loss 2.3878860473632812, val loss 2.4085259437561035\n",
      "step 628: train loss 2.3928024768829346, val loss 2.3959946632385254\n",
      "step 629: train loss 2.388695478439331, val loss 2.3997886180877686\n",
      "step 630: train loss 2.379067897796631, val loss 2.397336006164551\n",
      "step 631: train loss 2.382356882095337, val loss 2.3916096687316895\n",
      "step 632: train loss 2.383419990539551, val loss 2.3937082290649414\n",
      "step 633: train loss 2.3880605697631836, val loss 2.404024124145508\n",
      "step 634: train loss 2.3899412155151367, val loss 2.4019484519958496\n",
      "step 635: train loss 2.385057210922241, val loss 2.4063186645507812\n",
      "step 636: train loss 2.3927342891693115, val loss 2.4046785831451416\n",
      "step 637: train loss 2.3839914798736572, val loss 2.406022548675537\n",
      "step 638: train loss 2.386026382446289, val loss 2.402153968811035\n",
      "step 639: train loss 2.391099452972412, val loss 2.40311861038208\n",
      "step 640: train loss 2.382521867752075, val loss 2.409982681274414\n",
      "step 641: train loss 2.399256944656372, val loss 2.414964199066162\n",
      "step 642: train loss 2.383883237838745, val loss 2.404885768890381\n",
      "step 643: train loss 2.3881402015686035, val loss 2.3999135494232178\n",
      "step 644: train loss 2.3855724334716797, val loss 2.4079418182373047\n",
      "step 645: train loss 2.371959924697876, val loss 2.408159017562866\n",
      "step 646: train loss 2.3879027366638184, val loss 2.416947841644287\n",
      "step 647: train loss 2.3878285884857178, val loss 2.4114444255828857\n",
      "step 648: train loss 2.37982439994812, val loss 2.3965444564819336\n",
      "step 649: train loss 2.3768560886383057, val loss 2.3955206871032715\n",
      "step 650: train loss 2.390399694442749, val loss 2.3976707458496094\n",
      "step 651: train loss 2.370368719100952, val loss 2.3963828086853027\n",
      "step 652: train loss 2.371513605117798, val loss 2.3873231410980225\n",
      "step 653: train loss 2.3738467693328857, val loss 2.3917653560638428\n",
      "step 654: train loss 2.3877055644989014, val loss 2.393190383911133\n",
      "step 655: train loss 2.3778879642486572, val loss 2.389272689819336\n",
      "step 656: train loss 2.376941204071045, val loss 2.397067070007324\n",
      "step 657: train loss 2.3777174949645996, val loss 2.391444206237793\n",
      "step 658: train loss 2.380669593811035, val loss 2.389867067337036\n",
      "step 659: train loss 2.376461982727051, val loss 2.3827624320983887\n",
      "step 660: train loss 2.3801932334899902, val loss 2.394134283065796\n",
      "step 661: train loss 2.382455348968506, val loss 2.3868401050567627\n",
      "step 662: train loss 2.376344919204712, val loss 2.390599250793457\n",
      "step 663: train loss 2.372995376586914, val loss 2.3787801265716553\n",
      "step 664: train loss 2.373518943786621, val loss 2.3872175216674805\n",
      "step 665: train loss 2.3624956607818604, val loss 2.3829476833343506\n",
      "step 666: train loss 2.3731887340545654, val loss 2.3885467052459717\n",
      "step 667: train loss 2.3835372924804688, val loss 2.395890235900879\n",
      "step 668: train loss 2.373990535736084, val loss 2.3970677852630615\n",
      "step 669: train loss 2.3763632774353027, val loss 2.3904120922088623\n",
      "step 670: train loss 2.3730809688568115, val loss 2.397083282470703\n",
      "step 671: train loss 2.3763296604156494, val loss 2.391148567199707\n",
      "step 672: train loss 2.3809852600097656, val loss 2.3957579135894775\n",
      "step 673: train loss 2.3729841709136963, val loss 2.3928890228271484\n",
      "step 674: train loss 2.3730456829071045, val loss 2.3933753967285156\n",
      "step 675: train loss 2.3786308765411377, val loss 2.4006435871124268\n",
      "step 676: train loss 2.3750362396240234, val loss 2.3910250663757324\n",
      "step 677: train loss 2.3747286796569824, val loss 2.3940374851226807\n",
      "step 678: train loss 2.3727407455444336, val loss 2.398747444152832\n",
      "step 679: train loss 2.3810372352600098, val loss 2.396890163421631\n",
      "step 680: train loss 2.3769571781158447, val loss 2.3912181854248047\n",
      "step 681: train loss 2.369828462600708, val loss 2.3964452743530273\n",
      "step 682: train loss 2.3718390464782715, val loss 2.3878073692321777\n",
      "step 683: train loss 2.3706188201904297, val loss 2.391477108001709\n",
      "step 684: train loss 2.3744378089904785, val loss 2.3840067386627197\n",
      "step 685: train loss 2.3734986782073975, val loss 2.391842842102051\n",
      "step 686: train loss 2.3759706020355225, val loss 2.385709762573242\n",
      "step 687: train loss 2.3703033924102783, val loss 2.3859920501708984\n",
      "step 688: train loss 2.368717908859253, val loss 2.3896431922912598\n",
      "step 689: train loss 2.3750977516174316, val loss 2.3853859901428223\n",
      "step 690: train loss 2.375925064086914, val loss 2.3873746395111084\n",
      "step 691: train loss 2.3819122314453125, val loss 2.3872756958007812\n",
      "step 692: train loss 2.382556200027466, val loss 2.3865041732788086\n",
      "step 693: train loss 2.374201774597168, val loss 2.3813319206237793\n",
      "step 694: train loss 2.3682734966278076, val loss 2.3871047496795654\n",
      "step 695: train loss 2.3724541664123535, val loss 2.3712751865386963\n",
      "step 696: train loss 2.370588541030884, val loss 2.3818562030792236\n",
      "step 697: train loss 2.3723790645599365, val loss 2.391634464263916\n",
      "step 698: train loss 2.3807506561279297, val loss 2.3919570446014404\n",
      "step 699: train loss 2.377044439315796, val loss 2.393282890319824\n",
      "step 701: train loss 2.3735592365264893, val loss 2.386744260787964\n",
      "step 702: train loss 2.3749728202819824, val loss 2.386197090148926\n",
      "step 703: train loss 2.368117332458496, val loss 2.3841915130615234\n",
      "step 704: train loss 2.3844423294067383, val loss 2.390833616256714\n",
      "step 705: train loss 2.3805506229400635, val loss 2.3970155715942383\n",
      "step 706: train loss 2.383096218109131, val loss 2.39088773727417\n",
      "step 707: train loss 2.3714728355407715, val loss 2.3866982460021973\n",
      "step 708: train loss 2.374704599380493, val loss 2.3854806423187256\n",
      "step 709: train loss 2.373793363571167, val loss 2.3859426975250244\n",
      "step 710: train loss 2.372891664505005, val loss 2.381908893585205\n",
      "step 711: train loss 2.3695807456970215, val loss 2.395348310470581\n",
      "step 712: train loss 2.370807409286499, val loss 2.3777406215667725\n",
      "step 713: train loss 2.375554084777832, val loss 2.382668972015381\n",
      "step 714: train loss 2.366386651992798, val loss 2.380208969116211\n",
      "step 715: train loss 2.363480806350708, val loss 2.3833720684051514\n",
      "step 716: train loss 2.3567917346954346, val loss 2.368129014968872\n",
      "step 717: train loss 2.36037278175354, val loss 2.3713836669921875\n",
      "step 718: train loss 2.3557093143463135, val loss 2.37742018699646\n",
      "step 719: train loss 2.3686094284057617, val loss 2.3856122493743896\n",
      "step 720: train loss 2.371528148651123, val loss 2.385061264038086\n",
      "step 721: train loss 2.3741283416748047, val loss 2.3962514400482178\n",
      "step 722: train loss 2.376615285873413, val loss 2.3759238719940186\n",
      "step 723: train loss 2.3698253631591797, val loss 2.3806960582733154\n",
      "step 724: train loss 2.355435371398926, val loss 2.3771674633026123\n",
      "step 725: train loss 2.367363691329956, val loss 2.381568670272827\n",
      "step 726: train loss 2.3754031658172607, val loss 2.3868231773376465\n",
      "step 727: train loss 2.372427463531494, val loss 2.377056837081909\n",
      "step 728: train loss 2.375372886657715, val loss 2.3778743743896484\n",
      "step 729: train loss 2.362349033355713, val loss 2.3802988529205322\n",
      "step 730: train loss 2.3620378971099854, val loss 2.3686394691467285\n",
      "step 731: train loss 2.3687691688537598, val loss 2.3833868503570557\n",
      "step 732: train loss 2.3653371334075928, val loss 2.3894619941711426\n",
      "step 733: train loss 2.367180824279785, val loss 2.3829851150512695\n",
      "step 734: train loss 2.3564200401306152, val loss 2.373431921005249\n",
      "step 735: train loss 2.3495800495147705, val loss 2.3697288036346436\n",
      "step 736: train loss 2.356595754623413, val loss 2.3629140853881836\n",
      "step 737: train loss 2.349565029144287, val loss 2.36917781829834\n",
      "step 738: train loss 2.3647069931030273, val loss 2.3632493019104004\n",
      "step 739: train loss 2.3545665740966797, val loss 2.369779586791992\n",
      "step 740: train loss 2.3502261638641357, val loss 2.371654510498047\n",
      "step 741: train loss 2.353192090988159, val loss 2.3659653663635254\n",
      "step 742: train loss 2.3498172760009766, val loss 2.365725517272949\n",
      "step 743: train loss 2.348754644393921, val loss 2.365313768386841\n",
      "step 744: train loss 2.3566243648529053, val loss 2.3748831748962402\n",
      "step 745: train loss 2.354112148284912, val loss 2.3828558921813965\n",
      "step 746: train loss 2.3560492992401123, val loss 2.3725740909576416\n",
      "step 747: train loss 2.3612024784088135, val loss 2.362488269805908\n",
      "step 748: train loss 2.3465421199798584, val loss 2.366401433944702\n",
      "step 749: train loss 2.349093437194824, val loss 2.3768868446350098\n",
      "step 750: train loss 2.359135627746582, val loss 2.385333776473999\n",
      "step 751: train loss 2.3472373485565186, val loss 2.3746721744537354\n",
      "step 752: train loss 2.3556432723999023, val loss 2.380394220352173\n",
      "step 753: train loss 2.3490021228790283, val loss 2.378877639770508\n",
      "step 754: train loss 2.357656478881836, val loss 2.3851513862609863\n",
      "step 755: train loss 2.3501076698303223, val loss 2.3790335655212402\n",
      "step 756: train loss 2.356346607208252, val loss 2.3745105266571045\n",
      "step 757: train loss 2.346003532409668, val loss 2.3717308044433594\n",
      "step 758: train loss 2.3530468940734863, val loss 2.369499444961548\n",
      "step 759: train loss 2.352435827255249, val loss 2.3750154972076416\n",
      "step 760: train loss 2.3478264808654785, val loss 2.3679158687591553\n",
      "step 761: train loss 2.3580148220062256, val loss 2.3671953678131104\n",
      "step 762: train loss 2.3466570377349854, val loss 2.371744394302368\n",
      "step 763: train loss 2.353080987930298, val loss 2.366398811340332\n",
      "step 764: train loss 2.346303939819336, val loss 2.3548383712768555\n",
      "step 765: train loss 2.3413608074188232, val loss 2.3642096519470215\n",
      "step 766: train loss 2.342524290084839, val loss 2.365363836288452\n",
      "step 767: train loss 2.3509011268615723, val loss 2.363424777984619\n",
      "step 768: train loss 2.342475652694702, val loss 2.3638527393341064\n",
      "step 769: train loss 2.3458352088928223, val loss 2.372880458831787\n",
      "step 770: train loss 2.354527711868286, val loss 2.369381904602051\n",
      "step 771: train loss 2.355419635772705, val loss 2.3626439571380615\n",
      "step 772: train loss 2.3452694416046143, val loss 2.3675997257232666\n",
      "step 773: train loss 2.345371961593628, val loss 2.3659815788269043\n",
      "step 774: train loss 2.3423616886138916, val loss 2.3628790378570557\n",
      "step 775: train loss 2.3436782360076904, val loss 2.3677241802215576\n",
      "step 776: train loss 2.349566698074341, val loss 2.3623995780944824\n",
      "step 777: train loss 2.3427512645721436, val loss 2.356640100479126\n",
      "step 778: train loss 2.340778112411499, val loss 2.3598804473876953\n",
      "step 779: train loss 2.345327615737915, val loss 2.3664984703063965\n",
      "step 780: train loss 2.347625494003296, val loss 2.369614839553833\n",
      "step 781: train loss 2.352895975112915, val loss 2.3655264377593994\n",
      "step 782: train loss 2.3478407859802246, val loss 2.3628902435302734\n",
      "step 783: train loss 2.352180004119873, val loss 2.3649473190307617\n",
      "step 784: train loss 2.3496954441070557, val loss 2.3625402450561523\n",
      "step 785: train loss 2.346205711364746, val loss 2.3586483001708984\n",
      "step 786: train loss 2.34206485748291, val loss 2.3527729511260986\n",
      "step 787: train loss 2.3390603065490723, val loss 2.352227210998535\n",
      "step 788: train loss 2.3442087173461914, val loss 2.360139846801758\n",
      "step 789: train loss 2.346695899963379, val loss 2.3603599071502686\n",
      "step 790: train loss 2.3509814739227295, val loss 2.357887029647827\n",
      "step 791: train loss 2.3440206050872803, val loss 2.3586559295654297\n",
      "step 792: train loss 2.3398165702819824, val loss 2.3558642864227295\n",
      "step 793: train loss 2.335831880569458, val loss 2.345374345779419\n",
      "step 794: train loss 2.342940092086792, val loss 2.348245143890381\n",
      "step 795: train loss 2.3502037525177, val loss 2.3552215099334717\n",
      "step 796: train loss 2.363506555557251, val loss 2.3609509468078613\n",
      "step 797: train loss 2.3629205226898193, val loss 2.36602520942688\n",
      "step 798: train loss 2.353729486465454, val loss 2.3533878326416016\n",
      "step 799: train loss 2.349073886871338, val loss 2.3506009578704834\n",
      "step 801: train loss 2.3378658294677734, val loss 2.3486857414245605\n",
      "step 802: train loss 2.3398187160491943, val loss 2.355556011199951\n",
      "step 803: train loss 2.3512840270996094, val loss 2.3621387481689453\n",
      "step 804: train loss 2.3542957305908203, val loss 2.354173183441162\n",
      "step 805: train loss 2.3474860191345215, val loss 2.3579277992248535\n",
      "step 806: train loss 2.3442440032958984, val loss 2.3553977012634277\n",
      "step 807: train loss 2.337925672531128, val loss 2.359445095062256\n",
      "step 808: train loss 2.3381283283233643, val loss 2.3519582748413086\n",
      "step 809: train loss 2.3306782245635986, val loss 2.3438446521759033\n",
      "step 810: train loss 2.3375232219696045, val loss 2.3489789962768555\n",
      "step 811: train loss 2.335831880569458, val loss 2.343284845352173\n",
      "step 812: train loss 2.3428096771240234, val loss 2.3524169921875\n",
      "step 813: train loss 2.3338279724121094, val loss 2.346238613128662\n",
      "step 814: train loss 2.3352103233337402, val loss 2.3523871898651123\n",
      "step 815: train loss 2.323154926300049, val loss 2.3445072174072266\n",
      "step 816: train loss 2.3349461555480957, val loss 2.3423240184783936\n",
      "step 817: train loss 2.333040475845337, val loss 2.3494699001312256\n",
      "step 818: train loss 2.3352625370025635, val loss 2.3552587032318115\n",
      "step 819: train loss 2.3356881141662598, val loss 2.3559446334838867\n",
      "step 820: train loss 2.3320472240448, val loss 2.352921485900879\n",
      "step 821: train loss 2.3407061100006104, val loss 2.349947214126587\n",
      "step 822: train loss 2.3280718326568604, val loss 2.3490817546844482\n",
      "step 823: train loss 2.3304104804992676, val loss 2.3483681678771973\n",
      "step 824: train loss 2.325467824935913, val loss 2.347041368484497\n",
      "step 825: train loss 2.3364219665527344, val loss 2.3491907119750977\n",
      "step 826: train loss 2.3307526111602783, val loss 2.3581252098083496\n",
      "step 827: train loss 2.332425832748413, val loss 2.3459525108337402\n",
      "step 828: train loss 2.3349127769470215, val loss 2.3549306392669678\n",
      "step 829: train loss 2.3411178588867188, val loss 2.356382131576538\n",
      "step 830: train loss 2.344433546066284, val loss 2.3506577014923096\n",
      "step 831: train loss 2.335165023803711, val loss 2.350806951522827\n",
      "step 832: train loss 2.3295159339904785, val loss 2.353677272796631\n",
      "step 833: train loss 2.322108507156372, val loss 2.3441410064697266\n",
      "step 834: train loss 2.323969841003418, val loss 2.348128318786621\n",
      "step 835: train loss 2.328927516937256, val loss 2.3476336002349854\n",
      "step 836: train loss 2.3314335346221924, val loss 2.3410189151763916\n",
      "step 837: train loss 2.332650661468506, val loss 2.3459458351135254\n",
      "step 838: train loss 2.328427791595459, val loss 2.3428187370300293\n",
      "step 839: train loss 2.334892749786377, val loss 2.346536636352539\n",
      "step 840: train loss 2.329533576965332, val loss 2.3537261486053467\n",
      "step 841: train loss 2.334127187728882, val loss 2.3478734493255615\n",
      "step 842: train loss 2.3298134803771973, val loss 2.3504581451416016\n",
      "step 843: train loss 2.3329617977142334, val loss 2.3495774269104004\n",
      "step 844: train loss 2.3274543285369873, val loss 2.3480701446533203\n",
      "step 845: train loss 2.3218719959259033, val loss 2.347073554992676\n",
      "step 846: train loss 2.3273587226867676, val loss 2.3545539379119873\n",
      "step 847: train loss 2.334406614303589, val loss 2.3537187576293945\n",
      "step 848: train loss 2.3337156772613525, val loss 2.3535561561584473\n",
      "step 849: train loss 2.3225111961364746, val loss 2.3573524951934814\n",
      "step 850: train loss 2.3286728858947754, val loss 2.3516016006469727\n",
      "step 851: train loss 2.328078508377075, val loss 2.349844455718994\n",
      "step 852: train loss 2.3380141258239746, val loss 2.351693630218506\n",
      "step 853: train loss 2.335395574569702, val loss 2.35457706451416\n",
      "step 854: train loss 2.3369743824005127, val loss 2.3576443195343018\n",
      "step 855: train loss 2.3288211822509766, val loss 2.3585681915283203\n",
      "step 856: train loss 2.3256406784057617, val loss 2.355198621749878\n",
      "step 857: train loss 2.335690975189209, val loss 2.359208345413208\n",
      "step 858: train loss 2.3361194133758545, val loss 2.3624818325042725\n",
      "step 859: train loss 2.3333120346069336, val loss 2.3517799377441406\n",
      "step 860: train loss 2.327223300933838, val loss 2.346094846725464\n",
      "step 861: train loss 2.327988624572754, val loss 2.3474009037017822\n",
      "step 862: train loss 2.3304176330566406, val loss 2.3458337783813477\n",
      "step 863: train loss 2.3293113708496094, val loss 2.3498384952545166\n",
      "step 864: train loss 2.338848352432251, val loss 2.3394229412078857\n",
      "step 865: train loss 2.3287901878356934, val loss 2.3409180641174316\n",
      "step 866: train loss 2.31960129737854, val loss 2.3393685817718506\n",
      "step 867: train loss 2.327505588531494, val loss 2.342503547668457\n",
      "step 868: train loss 2.331265449523926, val loss 2.346630573272705\n",
      "step 869: train loss 2.3275959491729736, val loss 2.340797185897827\n",
      "step 870: train loss 2.3246352672576904, val loss 2.343019962310791\n",
      "step 871: train loss 2.3269739151000977, val loss 2.34641695022583\n",
      "step 872: train loss 2.333310127258301, val loss 2.343482732772827\n",
      "step 873: train loss 2.325852870941162, val loss 2.34890079498291\n",
      "step 874: train loss 2.3244478702545166, val loss 2.338940143585205\n",
      "step 875: train loss 2.325221061706543, val loss 2.3481295108795166\n",
      "step 876: train loss 2.3229622840881348, val loss 2.348538398742676\n",
      "step 877: train loss 2.3206710815429688, val loss 2.3480963706970215\n",
      "step 878: train loss 2.323598861694336, val loss 2.3439748287200928\n",
      "step 879: train loss 2.322373628616333, val loss 2.343332290649414\n",
      "step 880: train loss 2.3213775157928467, val loss 2.332928419113159\n",
      "step 881: train loss 2.3206660747528076, val loss 2.3376684188842773\n",
      "step 882: train loss 2.316676139831543, val loss 2.3378870487213135\n",
      "step 883: train loss 2.3259334564208984, val loss 2.3314285278320312\n",
      "step 884: train loss 2.320727586746216, val loss 2.3289449214935303\n",
      "step 885: train loss 2.326871871948242, val loss 2.3364620208740234\n",
      "step 886: train loss 2.320786952972412, val loss 2.3352391719818115\n",
      "step 887: train loss 2.321434259414673, val loss 2.33781361579895\n",
      "step 888: train loss 2.318967342376709, val loss 2.3343358039855957\n",
      "step 889: train loss 2.324550151824951, val loss 2.3412606716156006\n",
      "step 890: train loss 2.318810224533081, val loss 2.331942319869995\n",
      "step 891: train loss 2.327951669692993, val loss 2.332725763320923\n",
      "step 892: train loss 2.328038215637207, val loss 2.343005895614624\n",
      "step 893: train loss 2.32702374458313, val loss 2.3450565338134766\n",
      "step 894: train loss 2.3249568939208984, val loss 2.3366963863372803\n",
      "step 895: train loss 2.3259878158569336, val loss 2.348771095275879\n",
      "step 896: train loss 2.3230011463165283, val loss 2.3260161876678467\n",
      "step 897: train loss 2.3126184940338135, val loss 2.3285863399505615\n",
      "step 898: train loss 2.3110597133636475, val loss 2.331934928894043\n",
      "step 899: train loss 2.3151912689208984, val loss 2.3208301067352295\n",
      "step 901: train loss 2.313253164291382, val loss 2.3266022205352783\n",
      "step 902: train loss 2.310900926589966, val loss 2.3308932781219482\n",
      "step 903: train loss 2.3167574405670166, val loss 2.3241336345672607\n",
      "step 904: train loss 2.3213067054748535, val loss 2.331190347671509\n",
      "step 905: train loss 2.3184642791748047, val loss 2.3338253498077393\n",
      "step 906: train loss 2.3154075145721436, val loss 2.3241050243377686\n",
      "step 907: train loss 2.3163161277770996, val loss 2.324899196624756\n",
      "step 908: train loss 2.309816837310791, val loss 2.325749397277832\n",
      "step 909: train loss 2.3142242431640625, val loss 2.336221218109131\n",
      "step 910: train loss 2.3228745460510254, val loss 2.3300228118896484\n",
      "step 911: train loss 2.3128857612609863, val loss 2.32405948638916\n",
      "step 912: train loss 2.308910846710205, val loss 2.3355448246002197\n",
      "step 913: train loss 2.3169336318969727, val loss 2.3275344371795654\n",
      "step 914: train loss 2.315793037414551, val loss 2.324141025543213\n",
      "step 915: train loss 2.3133325576782227, val loss 2.3184874057769775\n",
      "step 916: train loss 2.3159584999084473, val loss 2.314627170562744\n",
      "step 917: train loss 2.305328130722046, val loss 2.3245911598205566\n",
      "step 918: train loss 2.3119328022003174, val loss 2.3257880210876465\n",
      "step 919: train loss 2.314596652984619, val loss 2.3251290321350098\n",
      "step 920: train loss 2.3215973377227783, val loss 2.330683946609497\n",
      "step 921: train loss 2.321509599685669, val loss 2.3263909816741943\n",
      "step 922: train loss 2.3105850219726562, val loss 2.3206534385681152\n",
      "step 923: train loss 2.309323310852051, val loss 2.3207101821899414\n",
      "step 924: train loss 2.312239170074463, val loss 2.3269224166870117\n",
      "step 925: train loss 2.3144960403442383, val loss 2.331666946411133\n",
      "step 926: train loss 2.3208227157592773, val loss 2.3351972103118896\n",
      "step 927: train loss 2.326205253601074, val loss 2.3394529819488525\n",
      "step 928: train loss 2.320485830307007, val loss 2.3310136795043945\n",
      "step 929: train loss 2.3126425743103027, val loss 2.3264007568359375\n",
      "step 930: train loss 2.3133065700531006, val loss 2.3201100826263428\n",
      "step 931: train loss 2.3212921619415283, val loss 2.3295202255249023\n",
      "step 932: train loss 2.3271913528442383, val loss 2.331247329711914\n",
      "step 933: train loss 2.3241119384765625, val loss 2.3286521434783936\n",
      "step 934: train loss 2.3251190185546875, val loss 2.324688196182251\n",
      "step 935: train loss 2.3236374855041504, val loss 2.326009511947632\n",
      "step 936: train loss 2.320746421813965, val loss 2.3217923641204834\n",
      "step 937: train loss 2.31148099899292, val loss 2.3175923824310303\n",
      "step 938: train loss 2.3151302337646484, val loss 2.3229966163635254\n",
      "step 939: train loss 2.3052024841308594, val loss 2.327740430831909\n",
      "step 940: train loss 2.309887409210205, val loss 2.327099084854126\n",
      "step 941: train loss 2.307710886001587, val loss 2.3267738819122314\n",
      "step 942: train loss 2.309096097946167, val loss 2.324528694152832\n",
      "step 943: train loss 2.3053152561187744, val loss 2.3277859687805176\n",
      "step 944: train loss 2.3033230304718018, val loss 2.3217248916625977\n",
      "step 945: train loss 2.307429313659668, val loss 2.323178291320801\n",
      "step 946: train loss 2.3064560890197754, val loss 2.319424629211426\n",
      "step 947: train loss 2.320446729660034, val loss 2.3269777297973633\n",
      "step 948: train loss 2.3154196739196777, val loss 2.339679479598999\n",
      "step 949: train loss 2.317425489425659, val loss 2.336595058441162\n",
      "step 950: train loss 2.309539318084717, val loss 2.3326430320739746\n",
      "step 951: train loss 2.311936378479004, val loss 2.3250436782836914\n",
      "step 952: train loss 2.3052594661712646, val loss 2.325249195098877\n",
      "step 953: train loss 2.304213047027588, val loss 2.325010299682617\n",
      "step 954: train loss 2.314164161682129, val loss 2.3283698558807373\n",
      "step 955: train loss 2.311777114868164, val loss 2.3238823413848877\n",
      "step 956: train loss 2.3043761253356934, val loss 2.330289125442505\n",
      "step 957: train loss 2.3088576793670654, val loss 2.323488235473633\n",
      "step 958: train loss 2.297774076461792, val loss 2.3230109214782715\n",
      "step 959: train loss 2.3042430877685547, val loss 2.3140358924865723\n",
      "step 960: train loss 2.306281805038452, val loss 2.318129062652588\n",
      "step 961: train loss 2.295915365219116, val loss 2.3077120780944824\n",
      "step 962: train loss 2.304866313934326, val loss 2.316915273666382\n",
      "step 963: train loss 2.304490327835083, val loss 2.3206522464752197\n",
      "step 964: train loss 2.305626153945923, val loss 2.3201093673706055\n",
      "step 965: train loss 2.3168838024139404, val loss 2.3228867053985596\n",
      "step 966: train loss 2.3072924613952637, val loss 2.3269999027252197\n",
      "step 967: train loss 2.305281162261963, val loss 2.3155994415283203\n",
      "step 968: train loss 2.3063864707946777, val loss 2.3167879581451416\n",
      "step 969: train loss 2.2985122203826904, val loss 2.3118767738342285\n",
      "step 970: train loss 2.2982115745544434, val loss 2.311363697052002\n",
      "step 971: train loss 2.296823740005493, val loss 2.314767360687256\n",
      "step 972: train loss 2.300875186920166, val loss 2.327085256576538\n",
      "step 973: train loss 2.302090644836426, val loss 2.3168914318084717\n",
      "step 974: train loss 2.2986490726470947, val loss 2.3214316368103027\n",
      "step 975: train loss 2.3034818172454834, val loss 2.3218297958374023\n",
      "step 976: train loss 2.2988967895507812, val loss 2.325697898864746\n",
      "step 977: train loss 2.3097095489501953, val loss 2.3298707008361816\n",
      "step 978: train loss 2.303173542022705, val loss 2.3219106197357178\n",
      "step 979: train loss 2.3015329837799072, val loss 2.3279616832733154\n",
      "step 980: train loss 2.3019652366638184, val loss 2.3138937950134277\n",
      "step 981: train loss 2.292757034301758, val loss 2.327176094055176\n",
      "step 982: train loss 2.3024702072143555, val loss 2.3173978328704834\n",
      "step 983: train loss 2.3001115322113037, val loss 2.3298611640930176\n",
      "step 984: train loss 2.2987706661224365, val loss 2.319547414779663\n",
      "step 985: train loss 2.2973177433013916, val loss 2.3146660327911377\n",
      "step 986: train loss 2.2981345653533936, val loss 2.312805652618408\n",
      "step 987: train loss 2.302708864212036, val loss 2.310323715209961\n",
      "step 988: train loss 2.303035259246826, val loss 2.3193323612213135\n",
      "step 989: train loss 2.3007447719573975, val loss 2.3121814727783203\n",
      "step 990: train loss 2.3041043281555176, val loss 2.314363479614258\n",
      "step 991: train loss 2.3008153438568115, val loss 2.310276746749878\n",
      "step 992: train loss 2.299039602279663, val loss 2.315765619277954\n",
      "step 993: train loss 2.2899677753448486, val loss 2.3173727989196777\n",
      "step 994: train loss 2.295389175415039, val loss 2.311791181564331\n",
      "step 995: train loss 2.295104742050171, val loss 2.312037944793701\n",
      "step 996: train loss 2.286743402481079, val loss 2.313110589981079\n",
      "step 997: train loss 2.295494794845581, val loss 2.3113207817077637\n",
      "step 998: train loss 2.2988646030426025, val loss 2.3036537170410156\n",
      "step 999: train loss 2.2940566539764404, val loss 2.3164405822753906\n",
      "step 1001: train loss 2.295793056488037, val loss 2.300461769104004\n",
      "step 1002: train loss 2.294943332672119, val loss 2.307068109512329\n",
      "step 1003: train loss 2.2841076850891113, val loss 2.310654640197754\n",
      "step 1004: train loss 2.292985200881958, val loss 2.3092758655548096\n",
      "step 1005: train loss 2.2865118980407715, val loss 2.309891700744629\n",
      "step 1006: train loss 2.284954071044922, val loss 2.3166189193725586\n",
      "step 1007: train loss 2.300067901611328, val loss 2.3182921409606934\n",
      "step 1008: train loss 2.299318790435791, val loss 2.3256256580352783\n",
      "step 1009: train loss 2.2931413650512695, val loss 2.3100926876068115\n",
      "step 1010: train loss 2.289552688598633, val loss 2.310699701309204\n",
      "step 1011: train loss 2.299684762954712, val loss 2.304018020629883\n",
      "step 1012: train loss 2.2995405197143555, val loss 2.3125388622283936\n",
      "step 1013: train loss 2.297589063644409, val loss 2.323697566986084\n",
      "step 1014: train loss 2.2985482215881348, val loss 2.3175575733184814\n",
      "step 1015: train loss 2.2985236644744873, val loss 2.3149523735046387\n",
      "step 1016: train loss 2.297417163848877, val loss 2.3108327388763428\n",
      "step 1017: train loss 2.286217451095581, val loss 2.315143346786499\n",
      "step 1018: train loss 2.294984817504883, val loss 2.313323974609375\n",
      "step 1019: train loss 2.298755407333374, val loss 2.308868646621704\n",
      "step 1020: train loss 2.2995712757110596, val loss 2.3263866901397705\n",
      "step 1021: train loss 2.302267551422119, val loss 2.3229482173919678\n",
      "step 1022: train loss 2.300018787384033, val loss 2.318227767944336\n",
      "step 1023: train loss 2.3014588356018066, val loss 2.3075029850006104\n",
      "step 1024: train loss 2.289592742919922, val loss 2.3112406730651855\n",
      "step 1025: train loss 2.2844460010528564, val loss 2.3065457344055176\n",
      "step 1026: train loss 2.289273500442505, val loss 2.3027901649475098\n",
      "step 1027: train loss 2.2903945446014404, val loss 2.3073065280914307\n",
      "step 1028: train loss 2.2876949310302734, val loss 2.305896759033203\n",
      "step 1029: train loss 2.2877838611602783, val loss 2.3107831478118896\n",
      "step 1030: train loss 2.294773817062378, val loss 2.3199405670166016\n",
      "step 1031: train loss 2.3006105422973633, val loss 2.31180477142334\n",
      "step 1032: train loss 2.2979204654693604, val loss 2.323575735092163\n",
      "step 1033: train loss 2.3003804683685303, val loss 2.323611259460449\n",
      "step 1034: train loss 2.29415225982666, val loss 2.319688320159912\n",
      "step 1035: train loss 2.3029589653015137, val loss 2.319401741027832\n",
      "step 1036: train loss 2.2987983226776123, val loss 2.320814847946167\n",
      "step 1037: train loss 2.2932684421539307, val loss 2.3240292072296143\n",
      "step 1038: train loss 2.2852284908294678, val loss 2.310513496398926\n",
      "step 1039: train loss 2.2858755588531494, val loss 2.308877944946289\n",
      "step 1040: train loss 2.285968065261841, val loss 2.3070993423461914\n",
      "step 1041: train loss 2.297736644744873, val loss 2.3079450130462646\n",
      "step 1042: train loss 2.300842046737671, val loss 2.3109958171844482\n",
      "step 1043: train loss 2.2909698486328125, val loss 2.3152215480804443\n",
      "step 1044: train loss 2.2891085147857666, val loss 2.3092947006225586\n",
      "step 1045: train loss 2.2901487350463867, val loss 2.304105281829834\n",
      "step 1046: train loss 2.298818826675415, val loss 2.318876028060913\n",
      "step 1047: train loss 2.3013389110565186, val loss 2.3233823776245117\n",
      "step 1048: train loss 2.284837484359741, val loss 2.3122682571411133\n",
      "step 1049: train loss 2.295839548110962, val loss 2.307197332382202\n",
      "step 1050: train loss 2.2901194095611572, val loss 2.316035270690918\n",
      "step 1051: train loss 2.296663999557495, val loss 2.3128139972686768\n",
      "step 1052: train loss 2.288989543914795, val loss 2.314897298812866\n",
      "step 1053: train loss 2.2923431396484375, val loss 2.3160879611968994\n",
      "step 1054: train loss 2.2837743759155273, val loss 2.3120675086975098\n",
      "step 1055: train loss 2.292184829711914, val loss 2.3206522464752197\n",
      "step 1056: train loss 2.2910330295562744, val loss 2.3117988109588623\n",
      "step 1057: train loss 2.292724370956421, val loss 2.3188772201538086\n",
      "step 1058: train loss 2.2884247303009033, val loss 2.309682846069336\n",
      "step 1059: train loss 2.2892274856567383, val loss 2.3059234619140625\n",
      "step 1060: train loss 2.2921478748321533, val loss 2.3065671920776367\n",
      "step 1061: train loss 2.291818618774414, val loss 2.309508800506592\n",
      "step 1062: train loss 2.2853920459747314, val loss 2.2981607913970947\n",
      "step 1063: train loss 2.288048505783081, val loss 2.297625780105591\n",
      "step 1064: train loss 2.2905192375183105, val loss 2.304093360900879\n",
      "step 1065: train loss 2.2878432273864746, val loss 2.304954767227173\n",
      "step 1066: train loss 2.285191535949707, val loss 2.2945334911346436\n",
      "step 1067: train loss 2.283520460128784, val loss 2.307097911834717\n",
      "step 1068: train loss 2.2856385707855225, val loss 2.3127965927124023\n",
      "step 1069: train loss 2.2933433055877686, val loss 2.308866024017334\n",
      "step 1070: train loss 2.276995897293091, val loss 2.306014060974121\n",
      "step 1071: train loss 2.272597312927246, val loss 2.2958269119262695\n",
      "step 1072: train loss 2.277940034866333, val loss 2.302316665649414\n",
      "step 1073: train loss 2.2839975357055664, val loss 2.3023550510406494\n",
      "step 1074: train loss 2.28611159324646, val loss 2.3023149967193604\n",
      "step 1075: train loss 2.276930809020996, val loss 2.2954375743865967\n",
      "step 1076: train loss 2.279672384262085, val loss 2.294430732727051\n",
      "step 1077: train loss 2.2790329456329346, val loss 2.3029308319091797\n",
      "step 1078: train loss 2.2760813236236572, val loss 2.2927732467651367\n",
      "step 1079: train loss 2.280477285385132, val loss 2.2940948009490967\n",
      "step 1080: train loss 2.277985095977783, val loss 2.2985987663269043\n",
      "step 1081: train loss 2.2792208194732666, val loss 2.2967042922973633\n",
      "step 1082: train loss 2.285017490386963, val loss 2.29494571685791\n",
      "step 1083: train loss 2.2875213623046875, val loss 2.2907097339630127\n",
      "step 1084: train loss 2.281615972518921, val loss 2.299576759338379\n",
      "step 1085: train loss 2.2770137786865234, val loss 2.2941787242889404\n",
      "step 1086: train loss 2.2720959186553955, val loss 2.2895455360412598\n",
      "step 1087: train loss 2.275993585586548, val loss 2.2961838245391846\n",
      "step 1088: train loss 2.282783031463623, val loss 2.29144549369812\n",
      "step 1089: train loss 2.277878761291504, val loss 2.2946324348449707\n",
      "step 1090: train loss 2.278134822845459, val loss 2.2954533100128174\n",
      "step 1091: train loss 2.270975112915039, val loss 2.29695463180542\n",
      "step 1092: train loss 2.273862838745117, val loss 2.2901716232299805\n",
      "step 1093: train loss 2.275019645690918, val loss 2.295095920562744\n",
      "step 1094: train loss 2.2793188095092773, val loss 2.293435573577881\n",
      "step 1095: train loss 2.2782068252563477, val loss 2.2909674644470215\n",
      "step 1096: train loss 2.2782163619995117, val loss 2.2951416969299316\n",
      "step 1097: train loss 2.267765760421753, val loss 2.289794921875\n",
      "step 1098: train loss 2.269707679748535, val loss 2.288675546646118\n",
      "step 1099: train loss 2.2787787914276123, val loss 2.298027753829956\n",
      "step 1101: train loss 2.2737252712249756, val loss 2.2925667762756348\n",
      "step 1102: train loss 2.269385814666748, val loss 2.2841145992279053\n",
      "step 1103: train loss 2.2731618881225586, val loss 2.2901456356048584\n",
      "step 1104: train loss 2.267822504043579, val loss 2.2965123653411865\n",
      "step 1105: train loss 2.2816684246063232, val loss 2.2910568714141846\n",
      "step 1106: train loss 2.2801809310913086, val loss 2.291132688522339\n",
      "step 1107: train loss 2.2671709060668945, val loss 2.2914276123046875\n",
      "step 1108: train loss 2.2689719200134277, val loss 2.2907519340515137\n",
      "step 1109: train loss 2.273329973220825, val loss 2.2972333431243896\n",
      "step 1110: train loss 2.2759697437286377, val loss 2.3044650554656982\n",
      "step 1111: train loss 2.277747631072998, val loss 2.3029067516326904\n",
      "step 1112: train loss 2.2877445220947266, val loss 2.3076460361480713\n",
      "step 1113: train loss 2.2808942794799805, val loss 2.300243616104126\n",
      "step 1114: train loss 2.2719855308532715, val loss 2.302189350128174\n",
      "step 1115: train loss 2.2699592113494873, val loss 2.3003835678100586\n",
      "step 1116: train loss 2.268373489379883, val loss 2.2948272228240967\n",
      "step 1117: train loss 2.2801833152770996, val loss 2.300532102584839\n",
      "step 1118: train loss 2.276139736175537, val loss 2.3002262115478516\n",
      "step 1119: train loss 2.264497756958008, val loss 2.3027052879333496\n",
      "step 1120: train loss 2.270925998687744, val loss 2.2976086139678955\n",
      "step 1121: train loss 2.2755327224731445, val loss 2.300625801086426\n",
      "step 1122: train loss 2.277186393737793, val loss 2.2999820709228516\n",
      "step 1123: train loss 2.266376256942749, val loss 2.2937607765197754\n",
      "step 1124: train loss 2.2698867321014404, val loss 2.2922449111938477\n",
      "step 1125: train loss 2.2661080360412598, val loss 2.2854654788970947\n",
      "step 1126: train loss 2.258615255355835, val loss 2.2882399559020996\n",
      "step 1127: train loss 2.257298231124878, val loss 2.2799947261810303\n",
      "step 1128: train loss 2.263441801071167, val loss 2.2901084423065186\n",
      "step 1129: train loss 2.273000717163086, val loss 2.2895896434783936\n",
      "step 1130: train loss 2.270815372467041, val loss 2.292733907699585\n",
      "step 1131: train loss 2.272991180419922, val loss 2.2888870239257812\n",
      "step 1132: train loss 2.2720787525177, val loss 2.2899084091186523\n",
      "step 1133: train loss 2.2684807777404785, val loss 2.287276268005371\n",
      "step 1134: train loss 2.262458086013794, val loss 2.2836544513702393\n",
      "step 1135: train loss 2.263000726699829, val loss 2.2863030433654785\n",
      "step 1136: train loss 2.264758825302124, val loss 2.283965826034546\n",
      "step 1137: train loss 2.265300989151001, val loss 2.281928300857544\n",
      "step 1138: train loss 2.2639591693878174, val loss 2.280385971069336\n",
      "step 1139: train loss 2.2548177242279053, val loss 2.283165693283081\n",
      "step 1140: train loss 2.2556777000427246, val loss 2.2769927978515625\n",
      "step 1141: train loss 2.2624306678771973, val loss 2.278002977371216\n",
      "step 1142: train loss 2.2645721435546875, val loss 2.2752652168273926\n",
      "step 1143: train loss 2.260397434234619, val loss 2.2669456005096436\n",
      "step 1144: train loss 2.2567427158355713, val loss 2.286759614944458\n",
      "step 1145: train loss 2.2648606300354004, val loss 2.2882556915283203\n",
      "step 1146: train loss 2.268618106842041, val loss 2.2894933223724365\n",
      "step 1147: train loss 2.269097328186035, val loss 2.286822557449341\n",
      "step 1148: train loss 2.2653768062591553, val loss 2.287310838699341\n",
      "step 1149: train loss 2.2670202255249023, val loss 2.2990925312042236\n",
      "step 1150: train loss 2.2660815715789795, val loss 2.2921743392944336\n",
      "step 1151: train loss 2.2656900882720947, val loss 2.299494981765747\n",
      "step 1152: train loss 2.2734646797180176, val loss 2.286295175552368\n",
      "step 1153: train loss 2.265702724456787, val loss 2.282360315322876\n",
      "step 1154: train loss 2.2657086849212646, val loss 2.2949984073638916\n",
      "step 1155: train loss 2.274925947189331, val loss 2.296330213546753\n",
      "step 1156: train loss 2.2752649784088135, val loss 2.2928857803344727\n",
      "step 1157: train loss 2.2603542804718018, val loss 2.2890353202819824\n",
      "step 1158: train loss 2.259566307067871, val loss 2.281337022781372\n",
      "step 1159: train loss 2.2509779930114746, val loss 2.2898778915405273\n",
      "step 1160: train loss 2.2601218223571777, val loss 2.2816967964172363\n",
      "step 1161: train loss 2.2655727863311768, val loss 2.2935237884521484\n",
      "step 1162: train loss 2.259735584259033, val loss 2.2881486415863037\n",
      "step 1163: train loss 2.2571325302124023, val loss 2.2853705883026123\n",
      "step 1164: train loss 2.2549490928649902, val loss 2.294158458709717\n",
      "step 1165: train loss 2.26163387298584, val loss 2.2886393070220947\n",
      "step 1166: train loss 2.265892267227173, val loss 2.2907328605651855\n",
      "step 1167: train loss 2.262136936187744, val loss 2.28993821144104\n",
      "step 1168: train loss 2.270541191101074, val loss 2.2839853763580322\n",
      "step 1169: train loss 2.2621898651123047, val loss 2.2864456176757812\n",
      "step 1170: train loss 2.2579851150512695, val loss 2.279261589050293\n",
      "step 1171: train loss 2.2598717212677, val loss 2.284553289413452\n",
      "step 1172: train loss 2.267523765563965, val loss 2.2857232093811035\n",
      "step 1173: train loss 2.2627439498901367, val loss 2.281649351119995\n",
      "step 1174: train loss 2.2673864364624023, val loss 2.281384229660034\n",
      "step 1175: train loss 2.271047592163086, val loss 2.270214557647705\n",
      "step 1176: train loss 2.257768154144287, val loss 2.2818374633789062\n",
      "step 1177: train loss 2.256896734237671, val loss 2.277017593383789\n",
      "step 1178: train loss 2.2638099193573, val loss 2.283024311065674\n",
      "step 1179: train loss 2.258988618850708, val loss 2.2852072715759277\n",
      "step 1180: train loss 2.2671313285827637, val loss 2.2801034450531006\n",
      "step 1181: train loss 2.2637627124786377, val loss 2.2772676944732666\n",
      "step 1182: train loss 2.262144088745117, val loss 2.275028705596924\n",
      "step 1183: train loss 2.2681474685668945, val loss 2.279960870742798\n",
      "step 1184: train loss 2.256662368774414, val loss 2.2622241973876953\n",
      "step 1185: train loss 2.2550318241119385, val loss 2.2623510360717773\n",
      "step 1186: train loss 2.2510454654693604, val loss 2.273521900177002\n",
      "step 1187: train loss 2.2611119747161865, val loss 2.2720937728881836\n",
      "step 1188: train loss 2.2689380645751953, val loss 2.272639274597168\n",
      "step 1189: train loss 2.257875919342041, val loss 2.2705307006835938\n",
      "step 1190: train loss 2.260526418685913, val loss 2.2751474380493164\n",
      "step 1191: train loss 2.253692388534546, val loss 2.2714085578918457\n",
      "step 1192: train loss 2.257117509841919, val loss 2.271143913269043\n",
      "step 1193: train loss 2.2565271854400635, val loss 2.2748210430145264\n",
      "step 1194: train loss 2.2562341690063477, val loss 2.275871515274048\n",
      "step 1195: train loss 2.2593584060668945, val loss 2.2649991512298584\n",
      "step 1196: train loss 2.2627112865448, val loss 2.2692806720733643\n",
      "step 1197: train loss 2.2466928958892822, val loss 2.2726786136627197\n",
      "step 1198: train loss 2.251638889312744, val loss 2.269383192062378\n",
      "step 1199: train loss 2.2476792335510254, val loss 2.2692902088165283\n",
      "step 1201: train loss 2.2539610862731934, val loss 2.273256301879883\n",
      "step 1202: train loss 2.2431952953338623, val loss 2.2728986740112305\n",
      "step 1203: train loss 2.2463197708129883, val loss 2.272930860519409\n",
      "step 1204: train loss 2.2503273487091064, val loss 2.2719855308532715\n",
      "step 1205: train loss 2.2561919689178467, val loss 2.2759687900543213\n",
      "step 1206: train loss 2.2639036178588867, val loss 2.2757277488708496\n",
      "step 1207: train loss 2.25929856300354, val loss 2.2866270542144775\n",
      "step 1208: train loss 2.2560956478118896, val loss 2.278311252593994\n",
      "step 1209: train loss 2.257347822189331, val loss 2.277019500732422\n",
      "step 1210: train loss 2.2492568492889404, val loss 2.266014337539673\n",
      "step 1211: train loss 2.244213819503784, val loss 2.268062114715576\n",
      "step 1212: train loss 2.2516262531280518, val loss 2.268596887588501\n",
      "step 1213: train loss 2.2464513778686523, val loss 2.272172451019287\n",
      "step 1214: train loss 2.254204034805298, val loss 2.2640209197998047\n",
      "step 1215: train loss 2.252807378768921, val loss 2.274374008178711\n",
      "step 1216: train loss 2.251344680786133, val loss 2.2680394649505615\n",
      "step 1217: train loss 2.2568936347961426, val loss 2.2692935466766357\n",
      "step 1218: train loss 2.247673749923706, val loss 2.2716987133026123\n",
      "step 1219: train loss 2.2563984394073486, val loss 2.2674202919006348\n",
      "step 1220: train loss 2.258260726928711, val loss 2.2684133052825928\n",
      "step 1221: train loss 2.262815475463867, val loss 2.2757198810577393\n",
      "step 1222: train loss 2.259089469909668, val loss 2.2712790966033936\n",
      "step 1223: train loss 2.2460525035858154, val loss 2.2723512649536133\n",
      "step 1224: train loss 2.2594733238220215, val loss 2.265594720840454\n",
      "step 1225: train loss 2.2570202350616455, val loss 2.2759547233581543\n",
      "step 1226: train loss 2.2458791732788086, val loss 2.2633936405181885\n",
      "step 1227: train loss 2.246898651123047, val loss 2.2705488204956055\n",
      "step 1228: train loss 2.2542574405670166, val loss 2.269615411758423\n",
      "step 1229: train loss 2.2421414852142334, val loss 2.2624013423919678\n",
      "step 1230: train loss 2.250742197036743, val loss 2.268578290939331\n",
      "step 1231: train loss 2.2554690837860107, val loss 2.2621030807495117\n",
      "step 1232: train loss 2.2511813640594482, val loss 2.264815330505371\n",
      "step 1233: train loss 2.2464208602905273, val loss 2.2623226642608643\n",
      "step 1234: train loss 2.2570245265960693, val loss 2.2708146572113037\n",
      "step 1235: train loss 2.234269618988037, val loss 2.265946626663208\n",
      "step 1236: train loss 2.2472803592681885, val loss 2.2590856552124023\n",
      "step 1237: train loss 2.245065689086914, val loss 2.2609128952026367\n",
      "step 1238: train loss 2.23846435546875, val loss 2.2565622329711914\n",
      "step 1239: train loss 2.241454601287842, val loss 2.2663369178771973\n",
      "step 1240: train loss 2.2523648738861084, val loss 2.266831398010254\n",
      "step 1241: train loss 2.246727705001831, val loss 2.276308536529541\n",
      "step 1242: train loss 2.254587411880493, val loss 2.267573356628418\n",
      "step 1243: train loss 2.25270414352417, val loss 2.2787630558013916\n",
      "step 1244: train loss 2.2546088695526123, val loss 2.2616939544677734\n",
      "step 1245: train loss 2.2530088424682617, val loss 2.2640888690948486\n",
      "step 1246: train loss 2.258296251296997, val loss 2.2728805541992188\n",
      "step 1247: train loss 2.2604129314422607, val loss 2.281730890274048\n",
      "step 1248: train loss 2.2635233402252197, val loss 2.2804110050201416\n",
      "step 1249: train loss 2.2557485103607178, val loss 2.2705342769622803\n",
      "step 1250: train loss 2.2423887252807617, val loss 2.2648842334747314\n",
      "step 1251: train loss 2.2449257373809814, val loss 2.2525603771209717\n",
      "step 1252: train loss 2.2413573265075684, val loss 2.268389940261841\n",
      "step 1253: train loss 2.2403483390808105, val loss 2.2638111114501953\n",
      "step 1254: train loss 2.244171380996704, val loss 2.2569875717163086\n",
      "step 1255: train loss 2.245664596557617, val loss 2.2706139087677\n",
      "step 1256: train loss 2.248809814453125, val loss 2.2701964378356934\n",
      "step 1257: train loss 2.2495968341827393, val loss 2.2694308757781982\n",
      "step 1258: train loss 2.24595046043396, val loss 2.2704813480377197\n",
      "step 1259: train loss 2.243561029434204, val loss 2.2637813091278076\n",
      "step 1260: train loss 2.246884822845459, val loss 2.2697665691375732\n",
      "step 1261: train loss 2.25148868560791, val loss 2.2665741443634033\n",
      "step 1262: train loss 2.2444536685943604, val loss 2.270217180252075\n",
      "step 1263: train loss 2.2433974742889404, val loss 2.2732231616973877\n",
      "step 1264: train loss 2.2462427616119385, val loss 2.2636637687683105\n",
      "step 1265: train loss 2.2442870140075684, val loss 2.256570816040039\n",
      "step 1266: train loss 2.2402029037475586, val loss 2.250288724899292\n",
      "step 1267: train loss 2.241452693939209, val loss 2.251004695892334\n",
      "step 1268: train loss 2.2366037368774414, val loss 2.2617311477661133\n",
      "step 1269: train loss 2.2399532794952393, val loss 2.2602505683898926\n",
      "step 1270: train loss 2.238694429397583, val loss 2.260928153991699\n",
      "step 1271: train loss 2.2429702281951904, val loss 2.249216318130493\n",
      "step 1272: train loss 2.233341932296753, val loss 2.2579615116119385\n",
      "step 1273: train loss 2.2423837184906006, val loss 2.257577419281006\n",
      "step 1274: train loss 2.2438204288482666, val loss 2.2637805938720703\n",
      "step 1275: train loss 2.23626708984375, val loss 2.26033616065979\n",
      "step 1276: train loss 2.2386209964752197, val loss 2.252284049987793\n",
      "step 1277: train loss 2.2387685775756836, val loss 2.256199836730957\n",
      "step 1278: train loss 2.2417664527893066, val loss 2.254723072052002\n",
      "step 1279: train loss 2.248702049255371, val loss 2.257800817489624\n",
      "step 1280: train loss 2.239783763885498, val loss 2.2579290866851807\n",
      "step 1281: train loss 2.2340381145477295, val loss 2.2582080364227295\n",
      "step 1282: train loss 2.2360823154449463, val loss 2.259232997894287\n",
      "step 1283: train loss 2.2325279712677, val loss 2.265610933303833\n",
      "step 1284: train loss 2.239834785461426, val loss 2.2568421363830566\n",
      "step 1285: train loss 2.2358744144439697, val loss 2.2574217319488525\n",
      "step 1286: train loss 2.240840196609497, val loss 2.2640397548675537\n",
      "step 1287: train loss 2.236114501953125, val loss 2.2643229961395264\n",
      "step 1288: train loss 2.233853340148926, val loss 2.265998601913452\n",
      "step 1289: train loss 2.232595205307007, val loss 2.2557740211486816\n",
      "step 1290: train loss 2.244372606277466, val loss 2.2616193294525146\n",
      "step 1291: train loss 2.228731155395508, val loss 2.2632551193237305\n",
      "step 1292: train loss 2.231937885284424, val loss 2.2604382038116455\n",
      "step 1293: train loss 2.231952667236328, val loss 2.253425121307373\n",
      "step 1294: train loss 2.237236738204956, val loss 2.2568843364715576\n",
      "step 1295: train loss 2.230182647705078, val loss 2.260438919067383\n",
      "step 1296: train loss 2.238246440887451, val loss 2.2515037059783936\n",
      "step 1297: train loss 2.238375186920166, val loss 2.258570671081543\n",
      "step 1298: train loss 2.2426228523254395, val loss 2.264937400817871\n",
      "step 1299: train loss 2.237821578979492, val loss 2.2590208053588867\n",
      "step 1301: train loss 2.221867799758911, val loss 2.2524702548980713\n",
      "step 1302: train loss 2.236564874649048, val loss 2.254836082458496\n",
      "step 1303: train loss 2.222297191619873, val loss 2.2582597732543945\n",
      "step 1304: train loss 2.2283551692962646, val loss 2.2481043338775635\n",
      "step 1305: train loss 2.223817825317383, val loss 2.2499420642852783\n",
      "step 1306: train loss 2.2303342819213867, val loss 2.2557637691497803\n",
      "step 1307: train loss 2.228309392929077, val loss 2.2510554790496826\n",
      "step 1308: train loss 2.2233574390411377, val loss 2.254413366317749\n",
      "step 1309: train loss 2.2273902893066406, val loss 2.2468903064727783\n",
      "step 1310: train loss 2.235340118408203, val loss 2.2469913959503174\n",
      "step 1311: train loss 2.242349147796631, val loss 2.248380661010742\n",
      "step 1312: train loss 2.240095615386963, val loss 2.254784107208252\n",
      "step 1313: train loss 2.239877223968506, val loss 2.2441489696502686\n",
      "step 1314: train loss 2.2392327785491943, val loss 2.2520229816436768\n",
      "step 1315: train loss 2.2303690910339355, val loss 2.247396945953369\n",
      "step 1316: train loss 2.227247714996338, val loss 2.250673294067383\n",
      "step 1317: train loss 2.2368764877319336, val loss 2.2564826011657715\n",
      "step 1318: train loss 2.226048469543457, val loss 2.255815029144287\n",
      "step 1319: train loss 2.225165605545044, val loss 2.2543318271636963\n",
      "step 1320: train loss 2.2247469425201416, val loss 2.2502591609954834\n",
      "step 1321: train loss 2.2234346866607666, val loss 2.252854824066162\n",
      "step 1322: train loss 2.2293131351470947, val loss 2.2577884197235107\n",
      "step 1323: train loss 2.2298529148101807, val loss 2.2490475177764893\n",
      "step 1324: train loss 2.2207446098327637, val loss 2.2430074214935303\n",
      "step 1325: train loss 2.2266833782196045, val loss 2.24348521232605\n",
      "step 1326: train loss 2.2234442234039307, val loss 2.239623785018921\n",
      "step 1327: train loss 2.2258694171905518, val loss 2.2431962490081787\n",
      "step 1328: train loss 2.2220194339752197, val loss 2.2458155155181885\n",
      "step 1329: train loss 2.2276406288146973, val loss 2.2344160079956055\n",
      "step 1330: train loss 2.225889205932617, val loss 2.2421913146972656\n",
      "step 1331: train loss 2.2306294441223145, val loss 2.242997169494629\n",
      "step 1332: train loss 2.229036808013916, val loss 2.238441228866577\n",
      "step 1333: train loss 2.229820966720581, val loss 2.2378597259521484\n",
      "step 1334: train loss 2.2370805740356445, val loss 2.2390151023864746\n",
      "step 1335: train loss 2.233140707015991, val loss 2.2589316368103027\n",
      "step 1336: train loss 2.2264609336853027, val loss 2.240838050842285\n",
      "step 1337: train loss 2.2299442291259766, val loss 2.2371633052825928\n",
      "step 1338: train loss 2.2283828258514404, val loss 2.254418134689331\n",
      "step 1339: train loss 2.238119602203369, val loss 2.2679808139801025\n",
      "step 1340: train loss 2.2404768466949463, val loss 2.2610764503479004\n",
      "step 1341: train loss 2.237290859222412, val loss 2.2461719512939453\n",
      "step 1342: train loss 2.2359695434570312, val loss 2.2526204586029053\n",
      "step 1343: train loss 2.21868634223938, val loss 2.2408392429351807\n",
      "step 1344: train loss 2.2159318923950195, val loss 2.241520881652832\n",
      "step 1345: train loss 2.229396104812622, val loss 2.237333059310913\n",
      "step 1346: train loss 2.235185146331787, val loss 2.2494397163391113\n",
      "step 1347: train loss 2.226928949356079, val loss 2.2491190433502197\n",
      "step 1348: train loss 2.2190604209899902, val loss 2.2405877113342285\n",
      "step 1349: train loss 2.2199714183807373, val loss 2.2426657676696777\n",
      "step 1350: train loss 2.217407464981079, val loss 2.239197015762329\n",
      "step 1351: train loss 2.2187163829803467, val loss 2.2415034770965576\n",
      "step 1352: train loss 2.216668128967285, val loss 2.2393908500671387\n",
      "step 1353: train loss 2.224280834197998, val loss 2.2418053150177\n",
      "step 1354: train loss 2.2349698543548584, val loss 2.2518439292907715\n",
      "step 1355: train loss 2.2230474948883057, val loss 2.2494335174560547\n",
      "step 1356: train loss 2.226682662963867, val loss 2.2528200149536133\n",
      "step 1357: train loss 2.2247655391693115, val loss 2.2463181018829346\n",
      "step 1358: train loss 2.222832441329956, val loss 2.2417585849761963\n",
      "step 1359: train loss 2.220099687576294, val loss 2.240459442138672\n",
      "step 1360: train loss 2.2248926162719727, val loss 2.2399768829345703\n",
      "step 1361: train loss 2.2280149459838867, val loss 2.242478847503662\n",
      "step 1362: train loss 2.228539228439331, val loss 2.2443687915802\n",
      "step 1363: train loss 2.227867603302002, val loss 2.2473583221435547\n",
      "step 1364: train loss 2.216557264328003, val loss 2.2440149784088135\n",
      "step 1365: train loss 2.225771903991699, val loss 2.2510764598846436\n",
      "step 1366: train loss 2.2189738750457764, val loss 2.241924285888672\n",
      "step 1367: train loss 2.211216688156128, val loss 2.245586633682251\n",
      "step 1368: train loss 2.212273359298706, val loss 2.238607406616211\n",
      "step 1369: train loss 2.212235689163208, val loss 2.234147787094116\n",
      "step 1370: train loss 2.215834617614746, val loss 2.2427163124084473\n",
      "step 1371: train loss 2.227353096008301, val loss 2.2499470710754395\n",
      "step 1372: train loss 2.22629714012146, val loss 2.2483086585998535\n",
      "step 1373: train loss 2.2182815074920654, val loss 2.244997262954712\n",
      "step 1374: train loss 2.221924066543579, val loss 2.2452003955841064\n",
      "step 1375: train loss 2.2189037799835205, val loss 2.2453043460845947\n",
      "step 1376: train loss 2.224102258682251, val loss 2.234199285507202\n",
      "step 1377: train loss 2.2098453044891357, val loss 2.2414188385009766\n",
      "step 1378: train loss 2.2181499004364014, val loss 2.2359158992767334\n",
      "step 1379: train loss 2.2275028228759766, val loss 2.232504367828369\n",
      "step 1380: train loss 2.214857339859009, val loss 2.2480721473693848\n",
      "step 1381: train loss 2.209779739379883, val loss 2.247239351272583\n",
      "step 1382: train loss 2.217245578765869, val loss 2.2394423484802246\n",
      "step 1383: train loss 2.2219643592834473, val loss 2.228459119796753\n",
      "step 1384: train loss 2.2283952236175537, val loss 2.2460813522338867\n",
      "step 1385: train loss 2.2126669883728027, val loss 2.239781379699707\n",
      "step 1386: train loss 2.221859931945801, val loss 2.242558479309082\n",
      "step 1387: train loss 2.2156713008880615, val loss 2.237684488296509\n",
      "step 1388: train loss 2.213416814804077, val loss 2.2271032333374023\n",
      "step 1389: train loss 2.2304646968841553, val loss 2.2412285804748535\n",
      "step 1390: train loss 2.218740224838257, val loss 2.235471725463867\n",
      "step 1391: train loss 2.2122690677642822, val loss 2.237438678741455\n",
      "step 1392: train loss 2.2199385166168213, val loss 2.235349178314209\n",
      "step 1393: train loss 2.20822811126709, val loss 2.23333477973938\n",
      "step 1394: train loss 2.2192516326904297, val loss 2.2226336002349854\n",
      "step 1395: train loss 2.215015172958374, val loss 2.2328827381134033\n",
      "step 1396: train loss 2.206312656402588, val loss 2.2386858463287354\n",
      "step 1397: train loss 2.2080109119415283, val loss 2.2285940647125244\n",
      "step 1398: train loss 2.20819354057312, val loss 2.2285797595977783\n",
      "step 1399: train loss 2.2106549739837646, val loss 2.237316131591797\n",
      "step 1401: train loss 2.203514814376831, val loss 2.231510877609253\n",
      "step 1402: train loss 2.215634822845459, val loss 2.2324013710021973\n",
      "step 1403: train loss 2.2109131813049316, val loss 2.2326433658599854\n",
      "step 1404: train loss 2.2122766971588135, val loss 2.240614652633667\n",
      "step 1405: train loss 2.216172695159912, val loss 2.2578208446502686\n",
      "step 1406: train loss 2.217665433883667, val loss 2.2413971424102783\n",
      "step 1407: train loss 2.2194433212280273, val loss 2.245652914047241\n",
      "step 1408: train loss 2.2079062461853027, val loss 2.2366795539855957\n",
      "step 1409: train loss 2.2113640308380127, val loss 2.22765851020813\n",
      "step 1410: train loss 2.2118053436279297, val loss 2.233128309249878\n",
      "step 1411: train loss 2.2209551334381104, val loss 2.2427878379821777\n",
      "step 1412: train loss 2.2163262367248535, val loss 2.243215560913086\n",
      "step 1413: train loss 2.2177820205688477, val loss 2.2372756004333496\n",
      "step 1414: train loss 2.221461057662964, val loss 2.249892234802246\n",
      "step 1415: train loss 2.2128124237060547, val loss 2.2315621376037598\n",
      "step 1416: train loss 2.204885244369507, val loss 2.2256007194519043\n",
      "step 1417: train loss 2.2073721885681152, val loss 2.2252678871154785\n",
      "step 1418: train loss 2.210690498352051, val loss 2.2322189807891846\n",
      "step 1419: train loss 2.210212230682373, val loss 2.236154794692993\n",
      "step 1420: train loss 2.208793878555298, val loss 2.2382614612579346\n",
      "step 1421: train loss 2.217616319656372, val loss 2.22813081741333\n",
      "step 1422: train loss 2.2051496505737305, val loss 2.2306175231933594\n",
      "step 1423: train loss 2.2089498043060303, val loss 2.2414608001708984\n",
      "step 1424: train loss 2.2203774452209473, val loss 2.241384983062744\n",
      "step 1425: train loss 2.223635196685791, val loss 2.2458527088165283\n",
      "step 1426: train loss 2.221187114715576, val loss 2.233368158340454\n",
      "step 1427: train loss 2.204177141189575, val loss 2.2316031455993652\n",
      "step 1428: train loss 2.203644275665283, val loss 2.232337713241577\n",
      "step 1429: train loss 2.207448959350586, val loss 2.2249088287353516\n",
      "step 1430: train loss 2.2026498317718506, val loss 2.225104331970215\n",
      "step 1431: train loss 2.2117972373962402, val loss 2.231891632080078\n",
      "step 1432: train loss 2.2116916179656982, val loss 2.23128604888916\n",
      "step 1433: train loss 2.2077715396881104, val loss 2.2366111278533936\n",
      "step 1434: train loss 2.214768409729004, val loss 2.2323315143585205\n",
      "step 1435: train loss 2.203068733215332, val loss 2.2290430068969727\n",
      "step 1436: train loss 2.193793296813965, val loss 2.2276532649993896\n",
      "step 1437: train loss 2.2041077613830566, val loss 2.2198903560638428\n",
      "step 1438: train loss 2.2003777027130127, val loss 2.219592809677124\n",
      "step 1439: train loss 2.2096450328826904, val loss 2.2313551902770996\n",
      "step 1440: train loss 2.208292245864868, val loss 2.2218167781829834\n",
      "step 1441: train loss 2.2175722122192383, val loss 2.2403688430786133\n",
      "step 1442: train loss 2.207150936126709, val loss 2.2328028678894043\n",
      "step 1443: train loss 2.2020320892333984, val loss 2.2337539196014404\n",
      "step 1444: train loss 2.1977481842041016, val loss 2.2294445037841797\n",
      "step 1445: train loss 2.2042646408081055, val loss 2.2263975143432617\n",
      "step 1446: train loss 2.210357427597046, val loss 2.2279412746429443\n",
      "step 1447: train loss 2.2022504806518555, val loss 2.2224831581115723\n",
      "step 1448: train loss 2.206895112991333, val loss 2.233213186264038\n",
      "step 1449: train loss 2.2063148021698, val loss 2.223442316055298\n",
      "step 1450: train loss 2.1977715492248535, val loss 2.2171120643615723\n",
      "step 1451: train loss 2.1952011585235596, val loss 2.2179486751556396\n",
      "step 1452: train loss 2.199082374572754, val loss 2.207620859146118\n",
      "step 1453: train loss 2.194308042526245, val loss 2.2205164432525635\n",
      "step 1454: train loss 2.2050859928131104, val loss 2.2154946327209473\n",
      "step 1455: train loss 2.209468364715576, val loss 2.2242276668548584\n",
      "step 1456: train loss 2.202587366104126, val loss 2.214608669281006\n",
      "step 1457: train loss 2.2050018310546875, val loss 2.232729196548462\n",
      "step 1458: train loss 2.198634147644043, val loss 2.2182693481445312\n",
      "step 1459: train loss 2.198815107345581, val loss 2.224409818649292\n",
      "step 1460: train loss 2.1947102546691895, val loss 2.218461036682129\n",
      "step 1461: train loss 2.2001421451568604, val loss 2.226454257965088\n",
      "step 1462: train loss 2.2041313648223877, val loss 2.2173194885253906\n",
      "step 1463: train loss 2.2053017616271973, val loss 2.2303383350372314\n",
      "step 1464: train loss 2.202579975128174, val loss 2.2224247455596924\n",
      "step 1465: train loss 2.2015199661254883, val loss 2.2208547592163086\n",
      "step 1466: train loss 2.1944518089294434, val loss 2.222313642501831\n",
      "step 1467: train loss 2.2010960578918457, val loss 2.218451976776123\n",
      "step 1468: train loss 2.1965248584747314, val loss 2.220308303833008\n",
      "step 1469: train loss 2.2028143405914307, val loss 2.225898504257202\n",
      "step 1470: train loss 2.196776866912842, val loss 2.224877119064331\n",
      "step 1471: train loss 2.1976547241210938, val loss 2.2212653160095215\n",
      "step 1472: train loss 2.2002151012420654, val loss 2.2169177532196045\n",
      "step 1473: train loss 2.1888723373413086, val loss 2.210984706878662\n",
      "step 1474: train loss 2.200498104095459, val loss 2.2166292667388916\n",
      "step 1475: train loss 2.198512077331543, val loss 2.2171542644500732\n",
      "step 1476: train loss 2.1941699981689453, val loss 2.223548412322998\n",
      "step 1477: train loss 2.1932806968688965, val loss 2.230358839035034\n",
      "step 1478: train loss 2.1992223262786865, val loss 2.2260494232177734\n",
      "step 1479: train loss 2.2027056217193604, val loss 2.2310643196105957\n",
      "step 1480: train loss 2.2049384117126465, val loss 2.2273142337799072\n",
      "step 1481: train loss 2.20406436920166, val loss 2.227254629135132\n",
      "step 1482: train loss 2.1991655826568604, val loss 2.216069221496582\n",
      "step 1483: train loss 2.189767599105835, val loss 2.2091825008392334\n",
      "step 1484: train loss 2.1944167613983154, val loss 2.2207727432250977\n",
      "step 1485: train loss 2.2020368576049805, val loss 2.2221457958221436\n",
      "step 1486: train loss 2.1946206092834473, val loss 2.215160608291626\n",
      "step 1487: train loss 2.1897857189178467, val loss 2.221522331237793\n",
      "step 1488: train loss 2.1885764598846436, val loss 2.201831579208374\n",
      "step 1489: train loss 2.182194948196411, val loss 2.2089643478393555\n",
      "step 1490: train loss 2.1837220191955566, val loss 2.208404302597046\n",
      "step 1491: train loss 2.202186346054077, val loss 2.213028907775879\n",
      "step 1492: train loss 2.1971161365509033, val loss 2.225365400314331\n",
      "step 1493: train loss 2.198776960372925, val loss 2.222247362136841\n",
      "step 1494: train loss 2.1907129287719727, val loss 2.2074337005615234\n",
      "step 1495: train loss 2.194899320602417, val loss 2.2146248817443848\n",
      "step 1496: train loss 2.2055482864379883, val loss 2.219575881958008\n",
      "step 1497: train loss 2.206218957901001, val loss 2.2244560718536377\n",
      "step 1498: train loss 2.210475206375122, val loss 2.230120897293091\n",
      "step 1499: train loss 2.2054784297943115, val loss 2.218465566635132\n",
      "step 1501: train loss 2.198500394821167, val loss 2.2229909896850586\n",
      "step 1502: train loss 2.192002534866333, val loss 2.2211689949035645\n",
      "step 1503: train loss 2.2066080570220947, val loss 2.2260122299194336\n",
      "step 1504: train loss 2.19586181640625, val loss 2.217348575592041\n",
      "step 1505: train loss 2.196268081665039, val loss 2.2177700996398926\n",
      "step 1506: train loss 2.191279411315918, val loss 2.2188544273376465\n",
      "step 1507: train loss 2.194546699523926, val loss 2.2172017097473145\n",
      "step 1508: train loss 2.1941542625427246, val loss 2.227304458618164\n",
      "step 1509: train loss 2.1922383308410645, val loss 2.2122962474823\n",
      "step 1510: train loss 2.1976301670074463, val loss 2.2178103923797607\n",
      "step 1511: train loss 2.1907622814178467, val loss 2.214660406112671\n",
      "step 1512: train loss 2.1852779388427734, val loss 2.224766731262207\n",
      "step 1513: train loss 2.1912319660186768, val loss 2.2091169357299805\n",
      "step 1514: train loss 2.190502405166626, val loss 2.2146952152252197\n",
      "step 1515: train loss 2.193740129470825, val loss 2.2161757946014404\n",
      "step 1516: train loss 2.1874136924743652, val loss 2.210360050201416\n",
      "step 1517: train loss 2.1904916763305664, val loss 2.2157809734344482\n",
      "step 1518: train loss 2.1885111331939697, val loss 2.2170565128326416\n",
      "step 1519: train loss 2.191157579421997, val loss 2.2119815349578857\n",
      "step 1520: train loss 2.1961357593536377, val loss 2.2216367721557617\n",
      "step 1521: train loss 2.194171667098999, val loss 2.221445083618164\n",
      "step 1522: train loss 2.186129093170166, val loss 2.222867488861084\n",
      "step 1523: train loss 2.201024293899536, val loss 2.2183072566986084\n",
      "step 1524: train loss 2.18744158744812, val loss 2.225280284881592\n",
      "step 1525: train loss 2.1912124156951904, val loss 2.207061290740967\n",
      "step 1526: train loss 2.171171188354492, val loss 2.2160425186157227\n",
      "step 1527: train loss 2.1924962997436523, val loss 2.2101492881774902\n",
      "step 1528: train loss 2.196014165878296, val loss 2.218017816543579\n",
      "step 1529: train loss 2.1977248191833496, val loss 2.210298776626587\n",
      "step 1530: train loss 2.192559242248535, val loss 2.214616298675537\n",
      "step 1531: train loss 2.184511423110962, val loss 2.2093560695648193\n",
      "step 1532: train loss 2.1865882873535156, val loss 2.2052488327026367\n",
      "step 1533: train loss 2.1940135955810547, val loss 2.212860345840454\n",
      "step 1534: train loss 2.1942026615142822, val loss 2.2179479598999023\n",
      "step 1535: train loss 2.206012010574341, val loss 2.2243709564208984\n",
      "step 1536: train loss 2.18910551071167, val loss 2.229893684387207\n",
      "step 1537: train loss 2.1888086795806885, val loss 2.2106974124908447\n",
      "step 1538: train loss 2.197437047958374, val loss 2.2221381664276123\n",
      "step 1539: train loss 2.1949965953826904, val loss 2.2175488471984863\n",
      "step 1540: train loss 2.196208953857422, val loss 2.2185933589935303\n",
      "step 1541: train loss 2.192351818084717, val loss 2.214686632156372\n",
      "step 1542: train loss 2.190575361251831, val loss 2.212618112564087\n",
      "step 1543: train loss 2.195403814315796, val loss 2.2063093185424805\n",
      "step 1544: train loss 2.1862566471099854, val loss 2.220498561859131\n",
      "step 1545: train loss 2.199110984802246, val loss 2.2124745845794678\n",
      "step 1546: train loss 2.1901302337646484, val loss 2.2137582302093506\n",
      "step 1547: train loss 2.184748888015747, val loss 2.21218204498291\n",
      "step 1548: train loss 2.177703619003296, val loss 2.206704616546631\n",
      "step 1549: train loss 2.1861298084259033, val loss 2.2119240760803223\n",
      "step 1550: train loss 2.1826705932617188, val loss 2.2058777809143066\n",
      "step 1551: train loss 2.181326389312744, val loss 2.2087416648864746\n",
      "step 1552: train loss 2.198072671890259, val loss 2.2174956798553467\n",
      "step 1553: train loss 2.1909077167510986, val loss 2.2179372310638428\n",
      "step 1554: train loss 2.191052198410034, val loss 2.2211623191833496\n",
      "step 1555: train loss 2.186290979385376, val loss 2.2125203609466553\n",
      "step 1556: train loss 2.183494806289673, val loss 2.2094485759735107\n",
      "step 1557: train loss 2.1856462955474854, val loss 2.2057816982269287\n",
      "step 1558: train loss 2.185363531112671, val loss 2.218409776687622\n",
      "step 1559: train loss 2.1891674995422363, val loss 2.211927652359009\n",
      "step 1560: train loss 2.180349111557007, val loss 2.203073024749756\n",
      "step 1561: train loss 2.1793429851531982, val loss 2.207875967025757\n",
      "step 1562: train loss 2.183194160461426, val loss 2.21952486038208\n",
      "step 1563: train loss 2.177643060684204, val loss 2.2133874893188477\n",
      "step 1564: train loss 2.182297468185425, val loss 2.2153072357177734\n",
      "step 1565: train loss 2.174121379852295, val loss 2.207702159881592\n",
      "step 1566: train loss 2.1778147220611572, val loss 2.2004172801971436\n",
      "step 1567: train loss 2.180360794067383, val loss 2.2131543159484863\n",
      "step 1568: train loss 2.1758785247802734, val loss 2.2034528255462646\n",
      "step 1569: train loss 2.1874032020568848, val loss 2.210115909576416\n",
      "step 1570: train loss 2.1736481189727783, val loss 2.2095558643341064\n",
      "step 1571: train loss 2.1777875423431396, val loss 2.2073774337768555\n",
      "step 1572: train loss 2.1762197017669678, val loss 2.2016310691833496\n",
      "step 1573: train loss 2.1750690937042236, val loss 2.2059290409088135\n",
      "step 1574: train loss 2.183067798614502, val loss 2.209153175354004\n",
      "step 1575: train loss 2.175328016281128, val loss 2.201669931411743\n",
      "step 1576: train loss 2.1779139041900635, val loss 2.208127737045288\n",
      "step 1577: train loss 2.1677932739257812, val loss 2.2047204971313477\n",
      "step 1578: train loss 2.181603193283081, val loss 2.191840887069702\n",
      "step 1579: train loss 2.1793243885040283, val loss 2.2055060863494873\n",
      "step 1580: train loss 2.1836118698120117, val loss 2.209219217300415\n",
      "step 1581: train loss 2.1781809329986572, val loss 2.2010302543640137\n",
      "step 1582: train loss 2.1852493286132812, val loss 2.20454740524292\n",
      "step 1583: train loss 2.173887252807617, val loss 2.2032346725463867\n",
      "step 1584: train loss 2.185149669647217, val loss 2.214970588684082\n",
      "step 1585: train loss 2.18097186088562, val loss 2.206099271774292\n",
      "step 1586: train loss 2.183919906616211, val loss 2.202467918395996\n",
      "step 1587: train loss 2.1758031845092773, val loss 2.2036736011505127\n",
      "step 1588: train loss 2.175294876098633, val loss 2.2014248371124268\n",
      "step 1589: train loss 2.182708978652954, val loss 2.2125439643859863\n",
      "step 1590: train loss 2.181102991104126, val loss 2.2042784690856934\n",
      "step 1591: train loss 2.1748619079589844, val loss 2.2016167640686035\n",
      "step 1592: train loss 2.179724931716919, val loss 2.2051515579223633\n",
      "step 1593: train loss 2.1804897785186768, val loss 2.1978049278259277\n",
      "step 1594: train loss 2.1861014366149902, val loss 2.2034261226654053\n",
      "step 1595: train loss 2.178154945373535, val loss 2.209617853164673\n",
      "step 1596: train loss 2.178861379623413, val loss 2.191383123397827\n",
      "step 1597: train loss 2.181217670440674, val loss 2.196176052093506\n",
      "step 1598: train loss 2.1740403175354004, val loss 2.1945598125457764\n",
      "step 1599: train loss 2.185624599456787, val loss 2.208197593688965\n",
      "step 1601: train loss 2.186779737472534, val loss 2.2049825191497803\n",
      "step 1602: train loss 2.176955461502075, val loss 2.197901725769043\n",
      "step 1603: train loss 2.176262140274048, val loss 2.1996910572052\n",
      "step 1604: train loss 2.1728992462158203, val loss 2.213916301727295\n",
      "step 1605: train loss 2.1762359142303467, val loss 2.1950759887695312\n",
      "step 1606: train loss 2.1787185668945312, val loss 2.2032668590545654\n",
      "step 1607: train loss 2.1759848594665527, val loss 2.2017903327941895\n",
      "step 1608: train loss 2.166435718536377, val loss 2.1956441402435303\n",
      "step 1609: train loss 2.166499376296997, val loss 2.203984260559082\n",
      "step 1610: train loss 2.1759281158447266, val loss 2.204960346221924\n",
      "step 1611: train loss 2.1774098873138428, val loss 2.1979565620422363\n",
      "step 1612: train loss 2.1728579998016357, val loss 2.204268455505371\n",
      "step 1613: train loss 2.183440923690796, val loss 2.2153687477111816\n",
      "step 1614: train loss 2.1808435916900635, val loss 2.2095932960510254\n",
      "step 1615: train loss 2.1808786392211914, val loss 2.2122952938079834\n",
      "step 1616: train loss 2.180217981338501, val loss 2.2077419757843018\n",
      "step 1617: train loss 2.1805307865142822, val loss 2.214707612991333\n",
      "step 1618: train loss 2.1733477115631104, val loss 2.206505537033081\n",
      "step 1619: train loss 2.1743476390838623, val loss 2.1999282836914062\n",
      "step 1620: train loss 2.1701457500457764, val loss 2.191779136657715\n",
      "step 1621: train loss 2.181046485900879, val loss 2.202085018157959\n",
      "step 1622: train loss 2.1867117881774902, val loss 2.203117609024048\n",
      "step 1623: train loss 2.1741225719451904, val loss 2.2098731994628906\n",
      "step 1624: train loss 2.1813457012176514, val loss 2.2032999992370605\n",
      "step 1625: train loss 2.1709179878234863, val loss 2.1978418827056885\n",
      "step 1626: train loss 2.1787030696868896, val loss 2.2005326747894287\n",
      "step 1627: train loss 2.167301654815674, val loss 2.204223871231079\n",
      "step 1628: train loss 2.1680660247802734, val loss 2.2037172317504883\n",
      "step 1629: train loss 2.1713554859161377, val loss 2.2030534744262695\n",
      "step 1630: train loss 2.1755313873291016, val loss 2.1998414993286133\n",
      "step 1631: train loss 2.166563034057617, val loss 2.2039573192596436\n",
      "step 1632: train loss 2.178288459777832, val loss 2.2083137035369873\n",
      "step 1633: train loss 2.1878955364227295, val loss 2.214447259902954\n",
      "step 1634: train loss 2.1795613765716553, val loss 2.2104556560516357\n",
      "step 1635: train loss 2.1769275665283203, val loss 2.2096967697143555\n",
      "step 1636: train loss 2.1774611473083496, val loss 2.2113757133483887\n",
      "step 1637: train loss 2.171114206314087, val loss 2.201510190963745\n",
      "step 1638: train loss 2.1671228408813477, val loss 2.2049930095672607\n",
      "step 1639: train loss 2.1705830097198486, val loss 2.19399356842041\n",
      "step 1640: train loss 2.1676549911499023, val loss 2.1937034130096436\n",
      "step 1641: train loss 2.1720125675201416, val loss 2.201788902282715\n",
      "step 1642: train loss 2.178814649581909, val loss 2.205909490585327\n",
      "step 1643: train loss 2.172544002532959, val loss 2.196265459060669\n",
      "step 1644: train loss 2.1767635345458984, val loss 2.199376106262207\n",
      "step 1645: train loss 2.1807897090911865, val loss 2.203693389892578\n",
      "step 1646: train loss 2.175316095352173, val loss 2.1902012825012207\n",
      "step 1647: train loss 2.1707489490509033, val loss 2.2014408111572266\n",
      "step 1648: train loss 2.168923854827881, val loss 2.1961023807525635\n",
      "step 1649: train loss 2.1596927642822266, val loss 2.1999502182006836\n",
      "step 1650: train loss 2.1658971309661865, val loss 2.1966941356658936\n",
      "step 1651: train loss 2.1582140922546387, val loss 2.19608211517334\n",
      "step 1652: train loss 2.1620798110961914, val loss 2.1944591999053955\n",
      "step 1653: train loss 2.1581857204437256, val loss 2.19869327545166\n",
      "step 1654: train loss 2.1613409519195557, val loss 2.2016868591308594\n",
      "step 1655: train loss 2.176959753036499, val loss 2.200206995010376\n",
      "step 1656: train loss 2.1669962406158447, val loss 2.190171718597412\n",
      "step 1657: train loss 2.1673505306243896, val loss 2.195465326309204\n",
      "step 1658: train loss 2.1736671924591064, val loss 2.1862614154815674\n",
      "step 1659: train loss 2.1604602336883545, val loss 2.1866631507873535\n",
      "step 1660: train loss 2.16109299659729, val loss 2.190431594848633\n",
      "step 1661: train loss 2.161924123764038, val loss 2.1886696815490723\n",
      "step 1662: train loss 2.173218250274658, val loss 2.1913230419158936\n",
      "step 1663: train loss 2.1634039878845215, val loss 2.2008142471313477\n",
      "step 1664: train loss 2.1577389240264893, val loss 2.197504758834839\n",
      "step 1665: train loss 2.1669583320617676, val loss 2.197413444519043\n",
      "step 1666: train loss 2.1659598350524902, val loss 2.197599411010742\n",
      "step 1667: train loss 2.167173385620117, val loss 2.1971092224121094\n",
      "step 1668: train loss 2.1532745361328125, val loss 2.200761556625366\n",
      "step 1669: train loss 2.150761127471924, val loss 2.1976702213287354\n",
      "step 1670: train loss 2.150615930557251, val loss 2.200881242752075\n",
      "step 1671: train loss 2.1573500633239746, val loss 2.193389415740967\n",
      "step 1672: train loss 2.1604740619659424, val loss 2.192544460296631\n",
      "step 1673: train loss 2.1643991470336914, val loss 2.202218532562256\n",
      "step 1674: train loss 2.1632091999053955, val loss 2.195193290710449\n",
      "step 1675: train loss 2.162853956222534, val loss 2.1953284740448\n",
      "step 1676: train loss 2.161227226257324, val loss 2.193077802658081\n",
      "step 1677: train loss 2.168203115463257, val loss 2.2001919746398926\n",
      "step 1678: train loss 2.166203737258911, val loss 2.2021255493164062\n",
      "step 1679: train loss 2.153674364089966, val loss 2.201404094696045\n",
      "step 1680: train loss 2.150777816772461, val loss 2.2099592685699463\n",
      "step 1681: train loss 2.165524482727051, val loss 2.1975693702697754\n",
      "step 1682: train loss 2.1567904949188232, val loss 2.200836420059204\n",
      "step 1683: train loss 2.15525221824646, val loss 2.1886422634124756\n",
      "step 1684: train loss 2.156132459640503, val loss 2.1872451305389404\n",
      "step 1685: train loss 2.158538341522217, val loss 2.188474416732788\n",
      "step 1686: train loss 2.1542606353759766, val loss 2.1867928504943848\n",
      "step 1687: train loss 2.148672580718994, val loss 2.1956286430358887\n",
      "step 1688: train loss 2.149111270904541, val loss 2.185112476348877\n",
      "step 1689: train loss 2.156646966934204, val loss 2.180197238922119\n",
      "step 1690: train loss 2.149420738220215, val loss 2.1932199001312256\n",
      "step 1691: train loss 2.1547913551330566, val loss 2.190622329711914\n",
      "step 1692: train loss 2.1538586616516113, val loss 2.1847565174102783\n",
      "step 1693: train loss 2.1602957248687744, val loss 2.2017946243286133\n",
      "step 1694: train loss 2.1628804206848145, val loss 2.1995420455932617\n",
      "step 1695: train loss 2.160748243331909, val loss 2.1913628578186035\n",
      "step 1696: train loss 2.172253370285034, val loss 2.1948885917663574\n",
      "step 1697: train loss 2.158080816268921, val loss 2.1980197429656982\n",
      "step 1698: train loss 2.167600154876709, val loss 2.197540521621704\n",
      "step 1699: train loss 2.1686837673187256, val loss 2.1926891803741455\n",
      "step 1701: train loss 2.1710190773010254, val loss 2.1928961277008057\n",
      "step 1702: train loss 2.1653664112091064, val loss 2.191499710083008\n",
      "step 1703: train loss 2.172523260116577, val loss 2.1912484169006348\n",
      "step 1704: train loss 2.162015438079834, val loss 2.1975810527801514\n",
      "step 1705: train loss 2.1644132137298584, val loss 2.1828770637512207\n",
      "step 1706: train loss 2.1488687992095947, val loss 2.1929664611816406\n",
      "step 1707: train loss 2.1581673622131348, val loss 2.1911256313323975\n",
      "step 1708: train loss 2.159052848815918, val loss 2.1899421215057373\n",
      "step 1709: train loss 2.1469669342041016, val loss 2.186694383621216\n",
      "step 1710: train loss 2.156769275665283, val loss 2.1901657581329346\n",
      "step 1711: train loss 2.1520798206329346, val loss 2.197638750076294\n",
      "step 1712: train loss 2.155963659286499, val loss 2.1943957805633545\n",
      "step 1713: train loss 2.158456802368164, val loss 2.1990654468536377\n",
      "step 1714: train loss 2.1486668586730957, val loss 2.1895413398742676\n",
      "step 1715: train loss 2.1515560150146484, val loss 2.1873743534088135\n",
      "step 1716: train loss 2.153554916381836, val loss 2.182861566543579\n",
      "step 1717: train loss 2.1562392711639404, val loss 2.185861825942993\n",
      "step 1718: train loss 2.1489462852478027, val loss 2.1971590518951416\n",
      "step 1719: train loss 2.1516432762145996, val loss 2.1916091442108154\n",
      "step 1720: train loss 2.1521425247192383, val loss 2.1931281089782715\n",
      "step 1721: train loss 2.151949405670166, val loss 2.187849760055542\n",
      "step 1722: train loss 2.1552491188049316, val loss 2.2014882564544678\n",
      "step 1723: train loss 2.1623220443725586, val loss 2.1932621002197266\n",
      "step 1724: train loss 2.1605100631713867, val loss 2.1948087215423584\n",
      "step 1725: train loss 2.1540005207061768, val loss 2.1910836696624756\n",
      "step 1726: train loss 2.147984266281128, val loss 2.177668809890747\n",
      "step 1727: train loss 2.141803503036499, val loss 2.171750068664551\n",
      "step 1728: train loss 2.146566390991211, val loss 2.1809346675872803\n",
      "step 1729: train loss 2.147742748260498, val loss 2.1754753589630127\n",
      "step 1730: train loss 2.154575824737549, val loss 2.182783603668213\n",
      "step 1731: train loss 2.143458843231201, val loss 2.183802604675293\n",
      "step 1732: train loss 2.138399362564087, val loss 2.182415008544922\n",
      "step 1733: train loss 2.146104335784912, val loss 2.187138080596924\n",
      "step 1734: train loss 2.1503078937530518, val loss 2.1877801418304443\n",
      "step 1735: train loss 2.1539647579193115, val loss 2.1761765480041504\n",
      "step 1736: train loss 2.1537394523620605, val loss 2.1870620250701904\n",
      "step 1737: train loss 2.1507089138031006, val loss 2.1887166500091553\n",
      "step 1738: train loss 2.146210193634033, val loss 2.190310001373291\n",
      "step 1739: train loss 2.1388182640075684, val loss 2.1755597591400146\n",
      "step 1740: train loss 2.1419670581817627, val loss 2.1880218982696533\n",
      "step 1741: train loss 2.1518001556396484, val loss 2.1910736560821533\n",
      "step 1742: train loss 2.1528749465942383, val loss 2.1895012855529785\n",
      "step 1743: train loss 2.154768705368042, val loss 2.186319351196289\n",
      "step 1744: train loss 2.1562483310699463, val loss 2.187302589416504\n",
      "step 1745: train loss 2.1484904289245605, val loss 2.177645206451416\n",
      "step 1746: train loss 2.1471352577209473, val loss 2.1769590377807617\n",
      "step 1747: train loss 2.158196210861206, val loss 2.193798542022705\n",
      "step 1748: train loss 2.1634390354156494, val loss 2.196716547012329\n",
      "step 1749: train loss 2.1512722969055176, val loss 2.1847503185272217\n",
      "step 1750: train loss 2.147507667541504, val loss 2.1911470890045166\n",
      "step 1751: train loss 2.152083158493042, val loss 2.1884169578552246\n",
      "step 1752: train loss 2.1537744998931885, val loss 2.1893177032470703\n",
      "step 1753: train loss 2.1547205448150635, val loss 2.1848015785217285\n",
      "step 1754: train loss 2.153343915939331, val loss 2.192842960357666\n",
      "step 1755: train loss 2.150531530380249, val loss 2.1919658184051514\n",
      "step 1756: train loss 2.143573522567749, val loss 2.185075283050537\n",
      "step 1757: train loss 2.1373186111450195, val loss 2.1763412952423096\n",
      "step 1758: train loss 2.1303319931030273, val loss 2.1767325401306152\n",
      "step 1759: train loss 2.134902238845825, val loss 2.176102638244629\n",
      "step 1760: train loss 2.145623207092285, val loss 2.175529718399048\n",
      "step 1761: train loss 2.142087936401367, val loss 2.1799068450927734\n",
      "step 1762: train loss 2.140214443206787, val loss 2.1807515621185303\n",
      "step 1763: train loss 2.150679349899292, val loss 2.1789281368255615\n",
      "step 1764: train loss 2.1469812393188477, val loss 2.187596559524536\n",
      "step 1765: train loss 2.136971950531006, val loss 2.1746304035186768\n",
      "step 1766: train loss 2.13972544670105, val loss 2.178809404373169\n",
      "step 1767: train loss 2.1440796852111816, val loss 2.1817924976348877\n",
      "step 1768: train loss 2.137249231338501, val loss 2.198748826980591\n",
      "step 1769: train loss 2.144559383392334, val loss 2.1733789443969727\n",
      "step 1770: train loss 2.1460893154144287, val loss 2.172757625579834\n",
      "step 1771: train loss 2.137399435043335, val loss 2.1745874881744385\n",
      "step 1772: train loss 2.144585371017456, val loss 2.1725330352783203\n",
      "step 1773: train loss 2.1412718296051025, val loss 2.1654272079467773\n",
      "step 1774: train loss 2.136040449142456, val loss 2.164111614227295\n",
      "step 1775: train loss 2.130048990249634, val loss 2.166351795196533\n",
      "step 1776: train loss 2.136939764022827, val loss 2.175171375274658\n",
      "step 1777: train loss 2.135169506072998, val loss 2.166973114013672\n",
      "step 1778: train loss 2.1335041522979736, val loss 2.1781375408172607\n",
      "step 1779: train loss 2.1379919052124023, val loss 2.1742770671844482\n",
      "step 1780: train loss 2.1408660411834717, val loss 2.168729543685913\n",
      "step 1781: train loss 2.13154673576355, val loss 2.171858310699463\n",
      "step 1782: train loss 2.137202739715576, val loss 2.164255142211914\n",
      "step 1783: train loss 2.1390438079833984, val loss 2.177511692047119\n",
      "step 1784: train loss 2.1403791904449463, val loss 2.1699609756469727\n",
      "step 1785: train loss 2.139214515686035, val loss 2.1686038970947266\n",
      "step 1786: train loss 2.1447153091430664, val loss 2.169275999069214\n",
      "step 1787: train loss 2.13643741607666, val loss 2.183525562286377\n",
      "step 1788: train loss 2.1300618648529053, val loss 2.1661648750305176\n",
      "step 1789: train loss 2.1370458602905273, val loss 2.1685776710510254\n",
      "step 1790: train loss 2.1412110328674316, val loss 2.1731512546539307\n",
      "step 1791: train loss 2.147486686706543, val loss 2.1751627922058105\n",
      "step 1792: train loss 2.1338319778442383, val loss 2.1746819019317627\n",
      "step 1793: train loss 2.138429880142212, val loss 2.1665711402893066\n",
      "step 1794: train loss 2.1323740482330322, val loss 2.171260118484497\n",
      "step 1795: train loss 2.1333112716674805, val loss 2.16987943649292\n",
      "step 1796: train loss 2.130528450012207, val loss 2.1787006855010986\n",
      "step 1797: train loss 2.1274709701538086, val loss 2.169119119644165\n",
      "step 1798: train loss 2.13181471824646, val loss 2.179837703704834\n",
      "step 1799: train loss 2.1258156299591064, val loss 2.1799049377441406\n",
      "step 1801: train loss 2.1374671459198, val loss 2.179590940475464\n",
      "step 1802: train loss 2.1351096630096436, val loss 2.186230421066284\n",
      "step 1803: train loss 2.129697799682617, val loss 2.177501678466797\n",
      "step 1804: train loss 2.127243995666504, val loss 2.172043800354004\n",
      "step 1805: train loss 2.135697603225708, val loss 2.1723549365997314\n",
      "step 1806: train loss 2.1314516067504883, val loss 2.1786201000213623\n",
      "step 1807: train loss 2.128432512283325, val loss 2.176682233810425\n",
      "step 1808: train loss 2.129695415496826, val loss 2.173567533493042\n",
      "step 1809: train loss 2.1341745853424072, val loss 2.170182704925537\n",
      "step 1810: train loss 2.1328470706939697, val loss 2.1750359535217285\n",
      "step 1811: train loss 2.1260406970977783, val loss 2.1799564361572266\n",
      "step 1812: train loss 2.1337265968322754, val loss 2.171677350997925\n",
      "step 1813: train loss 2.1414694786071777, val loss 2.1686623096466064\n",
      "step 1814: train loss 2.1355199813842773, val loss 2.1606552600860596\n",
      "step 1815: train loss 2.1288208961486816, val loss 2.1705880165100098\n",
      "step 1816: train loss 2.1265480518341064, val loss 2.156312942504883\n",
      "step 1817: train loss 2.1215991973876953, val loss 2.163076877593994\n",
      "step 1818: train loss 2.1196601390838623, val loss 2.1724748611450195\n",
      "step 1819: train loss 2.136509418487549, val loss 2.167325019836426\n",
      "step 1820: train loss 2.1348345279693604, val loss 2.1651406288146973\n",
      "step 1821: train loss 2.1326236724853516, val loss 2.170208215713501\n",
      "step 1822: train loss 2.133305549621582, val loss 2.169931411743164\n",
      "step 1823: train loss 2.1307787895202637, val loss 2.1646018028259277\n",
      "step 1824: train loss 2.129946231842041, val loss 2.170581579208374\n",
      "step 1825: train loss 2.1307485103607178, val loss 2.1744697093963623\n",
      "step 1826: train loss 2.131279945373535, val loss 2.169218063354492\n",
      "step 1827: train loss 2.135343313217163, val loss 2.1782240867614746\n",
      "step 1828: train loss 2.140329360961914, val loss 2.175269842147827\n",
      "step 1829: train loss 2.126192092895508, val loss 2.169127941131592\n",
      "step 1830: train loss 2.1262011528015137, val loss 2.1620845794677734\n",
      "step 1831: train loss 2.126227855682373, val loss 2.1672379970550537\n",
      "step 1832: train loss 2.1285288333892822, val loss 2.170884847640991\n",
      "step 1833: train loss 2.1241796016693115, val loss 2.1722824573516846\n",
      "step 1834: train loss 2.1331262588500977, val loss 2.1681792736053467\n",
      "step 1835: train loss 2.131967306137085, val loss 2.177380323410034\n",
      "step 1836: train loss 2.121310234069824, val loss 2.167548894882202\n",
      "step 1837: train loss 2.12302565574646, val loss 2.166627883911133\n",
      "step 1838: train loss 2.119645118713379, val loss 2.1767172813415527\n",
      "step 1839: train loss 2.1252527236938477, val loss 2.1661019325256348\n",
      "step 1840: train loss 2.128887414932251, val loss 2.1672394275665283\n",
      "step 1841: train loss 2.132291793823242, val loss 2.177248477935791\n",
      "step 1842: train loss 2.1270275115966797, val loss 2.179800271987915\n",
      "step 1843: train loss 2.115527629852295, val loss 2.1694648265838623\n",
      "step 1844: train loss 2.1246891021728516, val loss 2.16335391998291\n",
      "step 1845: train loss 2.121816873550415, val loss 2.1799635887145996\n",
      "step 1846: train loss 2.12465238571167, val loss 2.1723902225494385\n",
      "step 1847: train loss 2.1307458877563477, val loss 2.1758017539978027\n",
      "step 1848: train loss 2.1216931343078613, val loss 2.1789205074310303\n",
      "step 1849: train loss 2.127937078475952, val loss 2.1702311038970947\n",
      "step 1850: train loss 2.131885290145874, val loss 2.180906057357788\n",
      "step 1851: train loss 2.1282951831817627, val loss 2.176156520843506\n",
      "step 1852: train loss 2.1305031776428223, val loss 2.1772263050079346\n",
      "step 1853: train loss 2.1282424926757812, val loss 2.182521104812622\n",
      "step 1854: train loss 2.1271584033966064, val loss 2.178494691848755\n",
      "step 1855: train loss 2.1307926177978516, val loss 2.1734519004821777\n",
      "step 1856: train loss 2.1293764114379883, val loss 2.1750333309173584\n",
      "step 1857: train loss 2.127039670944214, val loss 2.1724958419799805\n",
      "step 1858: train loss 2.122987747192383, val loss 2.172450065612793\n",
      "step 1859: train loss 2.128652334213257, val loss 2.1750881671905518\n",
      "step 1860: train loss 2.1285433769226074, val loss 2.163323163986206\n",
      "step 1861: train loss 2.125044584274292, val loss 2.1659138202667236\n",
      "step 1862: train loss 2.1113455295562744, val loss 2.1570072174072266\n",
      "step 1863: train loss 2.123441219329834, val loss 2.1585652828216553\n",
      "step 1864: train loss 2.1216847896575928, val loss 2.1627135276794434\n",
      "step 1865: train loss 2.1269423961639404, val loss 2.168088912963867\n",
      "step 1866: train loss 2.1294946670532227, val loss 2.1653754711151123\n",
      "step 1867: train loss 2.1278672218322754, val loss 2.1680939197540283\n",
      "step 1868: train loss 2.122039556503296, val loss 2.167081594467163\n",
      "step 1869: train loss 2.1233139038085938, val loss 2.159189224243164\n",
      "step 1870: train loss 2.1121997833251953, val loss 2.1591079235076904\n",
      "step 1871: train loss 2.1294305324554443, val loss 2.163120985031128\n",
      "step 1872: train loss 2.1153602600097656, val loss 2.1557118892669678\n",
      "step 1873: train loss 2.1128313541412354, val loss 2.162785291671753\n",
      "step 1874: train loss 2.11429500579834, val loss 2.1542022228240967\n",
      "step 1875: train loss 2.1221604347229004, val loss 2.1561408042907715\n",
      "step 1876: train loss 2.115013360977173, val loss 2.162992238998413\n",
      "step 1877: train loss 2.123847723007202, val loss 2.1583328247070312\n",
      "step 1878: train loss 2.1231961250305176, val loss 2.1646170616149902\n",
      "step 1879: train loss 2.1292853355407715, val loss 2.1532039642333984\n",
      "step 1880: train loss 2.117802143096924, val loss 2.1505115032196045\n",
      "step 1881: train loss 2.1205010414123535, val loss 2.159547805786133\n",
      "step 1882: train loss 2.1061487197875977, val loss 2.1558139324188232\n",
      "step 1883: train loss 2.113454818725586, val loss 2.152712345123291\n",
      "step 1884: train loss 2.116950035095215, val loss 2.1600496768951416\n",
      "step 1885: train loss 2.1253674030303955, val loss 2.1591503620147705\n",
      "step 1886: train loss 2.119158983230591, val loss 2.1505203247070312\n",
      "step 1887: train loss 2.120873212814331, val loss 2.159621477127075\n",
      "step 1888: train loss 2.1271793842315674, val loss 2.1574978828430176\n",
      "step 1889: train loss 2.1300172805786133, val loss 2.168006181716919\n",
      "step 1890: train loss 2.1272783279418945, val loss 2.1668386459350586\n",
      "step 1891: train loss 2.1243317127227783, val loss 2.1654515266418457\n",
      "step 1892: train loss 2.1246144771575928, val loss 2.163802146911621\n",
      "step 1893: train loss 2.119067430496216, val loss 2.161590099334717\n",
      "step 1894: train loss 2.121722936630249, val loss 2.1600844860076904\n",
      "step 1895: train loss 2.1104958057403564, val loss 2.1482439041137695\n",
      "step 1896: train loss 2.118929386138916, val loss 2.1478703022003174\n",
      "step 1897: train loss 2.1086316108703613, val loss 2.1547069549560547\n",
      "step 1898: train loss 2.111997127532959, val loss 2.1470181941986084\n",
      "step 1899: train loss 2.114851713180542, val loss 2.1589114665985107\n",
      "step 1901: train loss 2.120070695877075, val loss 2.1590023040771484\n",
      "step 1902: train loss 2.107234239578247, val loss 2.150347948074341\n",
      "step 1903: train loss 2.1186859607696533, val loss 2.149538516998291\n",
      "step 1904: train loss 2.108535051345825, val loss 2.1514861583709717\n",
      "step 1905: train loss 2.116150140762329, val loss 2.1544883251190186\n",
      "step 1906: train loss 2.1161770820617676, val loss 2.155632972717285\n",
      "step 1907: train loss 2.1193463802337646, val loss 2.152900218963623\n",
      "step 1908: train loss 2.1145403385162354, val loss 2.160019874572754\n",
      "step 1909: train loss 2.1153204441070557, val loss 2.156965732574463\n",
      "step 1910: train loss 2.1184799671173096, val loss 2.160431146621704\n",
      "step 1911: train loss 2.1175711154937744, val loss 2.1553215980529785\n",
      "step 1912: train loss 2.1036972999572754, val loss 2.1672208309173584\n",
      "step 1913: train loss 2.11663818359375, val loss 2.1644110679626465\n",
      "step 1914: train loss 2.1142609119415283, val loss 2.1613292694091797\n",
      "step 1915: train loss 2.110679864883423, val loss 2.1574857234954834\n",
      "step 1916: train loss 2.1188509464263916, val loss 2.152819871902466\n",
      "step 1917: train loss 2.106813669204712, val loss 2.157301425933838\n",
      "step 1918: train loss 2.1094541549682617, val loss 2.148556709289551\n",
      "step 1919: train loss 2.1059234142303467, val loss 2.151970386505127\n",
      "step 1920: train loss 2.1138107776641846, val loss 2.15383243560791\n",
      "step 1921: train loss 2.1083357334136963, val loss 2.160830020904541\n",
      "step 1922: train loss 2.111037254333496, val loss 2.1625757217407227\n",
      "step 1923: train loss 2.122025966644287, val loss 2.1742889881134033\n",
      "step 1924: train loss 2.114835023880005, val loss 2.164836883544922\n",
      "step 1925: train loss 2.1078896522521973, val loss 2.1427414417266846\n",
      "step 1926: train loss 2.1152803897857666, val loss 2.151993751525879\n",
      "step 1927: train loss 2.11259388923645, val loss 2.161123275756836\n",
      "step 1928: train loss 2.1121392250061035, val loss 2.1585934162139893\n",
      "step 1929: train loss 2.122014045715332, val loss 2.162245988845825\n",
      "step 1930: train loss 2.1084794998168945, val loss 2.1559693813323975\n",
      "step 1931: train loss 2.113222360610962, val loss 2.152838706970215\n",
      "step 1932: train loss 2.1176671981811523, val loss 2.1489224433898926\n",
      "step 1933: train loss 2.116875171661377, val loss 2.1600615978240967\n",
      "step 1934: train loss 2.1157379150390625, val loss 2.164799928665161\n",
      "step 1935: train loss 2.1227378845214844, val loss 2.1609809398651123\n",
      "step 1936: train loss 2.1160104274749756, val loss 2.160820722579956\n",
      "step 1937: train loss 2.118701219558716, val loss 2.152641773223877\n",
      "step 1938: train loss 2.124509572982788, val loss 2.1605687141418457\n",
      "step 1939: train loss 2.1131691932678223, val loss 2.1674790382385254\n",
      "step 1940: train loss 2.126389741897583, val loss 2.170645236968994\n",
      "step 1941: train loss 2.122084856033325, val loss 2.159144639968872\n",
      "step 1942: train loss 2.1093759536743164, val loss 2.1580591201782227\n",
      "step 1943: train loss 2.1073615550994873, val loss 2.1593284606933594\n",
      "step 1944: train loss 2.1146116256713867, val loss 2.144998073577881\n",
      "step 1945: train loss 2.1195976734161377, val loss 2.1527533531188965\n",
      "step 1946: train loss 2.0963714122772217, val loss 2.142819881439209\n",
      "step 1947: train loss 2.1072232723236084, val loss 2.1414177417755127\n",
      "step 1948: train loss 2.1077184677124023, val loss 2.1455206871032715\n",
      "step 1949: train loss 2.1062090396881104, val loss 2.153576135635376\n",
      "step 1950: train loss 2.1191985607147217, val loss 2.1637282371520996\n",
      "step 1951: train loss 2.120861768722534, val loss 2.1591808795928955\n",
      "step 1952: train loss 2.114976644515991, val loss 2.146791934967041\n",
      "step 1953: train loss 2.115830421447754, val loss 2.1541049480438232\n",
      "step 1954: train loss 2.1092820167541504, val loss 2.161886215209961\n",
      "step 1955: train loss 2.114215135574341, val loss 2.1549336910247803\n",
      "step 1956: train loss 2.1031060218811035, val loss 2.159320592880249\n",
      "step 1957: train loss 2.1122307777404785, val loss 2.1528232097625732\n",
      "step 1958: train loss 2.113168239593506, val loss 2.1560661792755127\n",
      "step 1959: train loss 2.1097359657287598, val loss 2.160895586013794\n",
      "step 1960: train loss 2.105147361755371, val loss 2.1493117809295654\n",
      "step 1961: train loss 2.111185312271118, val loss 2.1605935096740723\n",
      "step 1962: train loss 2.1139063835144043, val loss 2.152571201324463\n",
      "step 1963: train loss 2.1022982597351074, val loss 2.1457102298736572\n",
      "step 1964: train loss 2.1013410091400146, val loss 2.140738010406494\n",
      "step 1965: train loss 2.107006549835205, val loss 2.1434316635131836\n",
      "step 1966: train loss 2.0962026119232178, val loss 2.138998508453369\n",
      "step 1967: train loss 2.0976171493530273, val loss 2.138914108276367\n",
      "step 1968: train loss 2.098398208618164, val loss 2.135270118713379\n",
      "step 1969: train loss 2.0963385105133057, val loss 2.133882999420166\n",
      "step 1970: train loss 2.1040494441986084, val loss 2.149245500564575\n",
      "step 1971: train loss 2.105844497680664, val loss 2.142425060272217\n",
      "step 1972: train loss 2.1123852729797363, val loss 2.142652988433838\n",
      "step 1973: train loss 2.1099724769592285, val loss 2.1502327919006348\n",
      "step 1974: train loss 2.111046075820923, val loss 2.147707462310791\n",
      "step 1975: train loss 2.102877378463745, val loss 2.1425070762634277\n",
      "step 1976: train loss 2.1078574657440186, val loss 2.1448867321014404\n",
      "step 1977: train loss 2.101341485977173, val loss 2.1364850997924805\n",
      "step 1978: train loss 2.1071929931640625, val loss 2.1478047370910645\n",
      "step 1979: train loss 2.11055064201355, val loss 2.140601396560669\n",
      "step 1980: train loss 2.102959156036377, val loss 2.132908821105957\n",
      "step 1981: train loss 2.1027016639709473, val loss 2.1513864994049072\n",
      "step 1982: train loss 2.1119120121002197, val loss 2.1540005207061768\n",
      "step 1983: train loss 2.1053061485290527, val loss 2.1409599781036377\n",
      "step 1984: train loss 2.0952513217926025, val loss 2.142101287841797\n",
      "step 1985: train loss 2.098773241043091, val loss 2.143195152282715\n",
      "step 1986: train loss 2.102290391921997, val loss 2.1491146087646484\n",
      "step 1987: train loss 2.0982227325439453, val loss 2.15171480178833\n",
      "step 1988: train loss 2.1042678356170654, val loss 2.1520094871520996\n",
      "step 1989: train loss 2.104926824569702, val loss 2.143352746963501\n",
      "step 1990: train loss 2.108177900314331, val loss 2.149413824081421\n",
      "step 1991: train loss 2.10809063911438, val loss 2.148284673690796\n",
      "step 1992: train loss 2.106442451477051, val loss 2.141383171081543\n",
      "step 1993: train loss 2.10620379447937, val loss 2.1507275104522705\n",
      "step 1994: train loss 2.105695962905884, val loss 2.1431527137756348\n",
      "step 1995: train loss 2.101461887359619, val loss 2.153012752532959\n",
      "step 1996: train loss 2.111758232116699, val loss 2.152059555053711\n",
      "step 1997: train loss 2.106438159942627, val loss 2.1373000144958496\n",
      "step 1998: train loss 2.0976145267486572, val loss 2.1461477279663086\n",
      "step 1999: train loss 2.1050922870635986, val loss 2.1532318592071533\n",
      "step 2001: train loss 2.0969176292419434, val loss 2.135103702545166\n",
      "step 2002: train loss 2.0911800861358643, val loss 2.1376378536224365\n",
      "step 2003: train loss 2.1015026569366455, val loss 2.138756275177002\n",
      "step 2004: train loss 2.10016131401062, val loss 2.1482601165771484\n",
      "step 2005: train loss 2.0994269847869873, val loss 2.1330738067626953\n",
      "step 2006: train loss 2.106534481048584, val loss 2.147022008895874\n",
      "step 2007: train loss 2.097567319869995, val loss 2.137284278869629\n",
      "step 2008: train loss 2.08768367767334, val loss 2.130199432373047\n",
      "step 2009: train loss 2.094007968902588, val loss 2.1392345428466797\n",
      "step 2010: train loss 2.0962021350860596, val loss 2.1361243724823\n",
      "step 2011: train loss 2.100756883621216, val loss 2.146705150604248\n",
      "step 2012: train loss 2.0943610668182373, val loss 2.139157295227051\n",
      "step 2013: train loss 2.092543601989746, val loss 2.1328227519989014\n",
      "step 2014: train loss 2.0937201976776123, val loss 2.127103567123413\n",
      "step 2015: train loss 2.0882861614227295, val loss 2.1402764320373535\n",
      "step 2016: train loss 2.101454496383667, val loss 2.1357131004333496\n",
      "step 2017: train loss 2.1062734127044678, val loss 2.1398303508758545\n",
      "step 2018: train loss 2.0971336364746094, val loss 2.1391849517822266\n",
      "step 2019: train loss 2.084383964538574, val loss 2.1263558864593506\n",
      "step 2020: train loss 2.099156379699707, val loss 2.1274890899658203\n",
      "step 2021: train loss 2.088020086288452, val loss 2.1312437057495117\n",
      "step 2022: train loss 2.089315414428711, val loss 2.126985549926758\n",
      "step 2023: train loss 2.092170476913452, val loss 2.1341335773468018\n",
      "step 2024: train loss 2.0978517532348633, val loss 2.1302602291107178\n",
      "step 2025: train loss 2.0973355770111084, val loss 2.139958143234253\n",
      "step 2026: train loss 2.0962517261505127, val loss 2.139158248901367\n",
      "step 2027: train loss 2.0881927013397217, val loss 2.121325969696045\n",
      "step 2028: train loss 2.0925960540771484, val loss 2.129415988922119\n",
      "step 2029: train loss 2.093735694885254, val loss 2.140947103500366\n",
      "step 2030: train loss 2.0974106788635254, val loss 2.1335060596466064\n",
      "step 2031: train loss 2.0875110626220703, val loss 2.1388018131256104\n",
      "step 2032: train loss 2.086606502532959, val loss 2.1455459594726562\n",
      "step 2033: train loss 2.0983502864837646, val loss 2.139099359512329\n",
      "step 2034: train loss 2.101074457168579, val loss 2.148026466369629\n",
      "step 2035: train loss 2.091947317123413, val loss 2.1465210914611816\n",
      "step 2036: train loss 2.098672389984131, val loss 2.143718957901001\n",
      "step 2037: train loss 2.0993895530700684, val loss 2.14774227142334\n",
      "step 2038: train loss 2.1008338928222656, val loss 2.13273549079895\n",
      "step 2039: train loss 2.0968713760375977, val loss 2.1353893280029297\n",
      "step 2040: train loss 2.101541042327881, val loss 2.158553123474121\n",
      "step 2041: train loss 2.1036651134490967, val loss 2.1487724781036377\n",
      "step 2042: train loss 2.0972607135772705, val loss 2.145473003387451\n",
      "step 2043: train loss 2.091106653213501, val loss 2.136592388153076\n",
      "step 2044: train loss 2.097670793533325, val loss 2.1394567489624023\n",
      "step 2045: train loss 2.095785617828369, val loss 2.138563632965088\n",
      "step 2046: train loss 2.097557783126831, val loss 2.1510677337646484\n",
      "step 2047: train loss 2.091421604156494, val loss 2.1385252475738525\n",
      "step 2048: train loss 2.1100289821624756, val loss 2.1507692337036133\n",
      "step 2049: train loss 2.098273515701294, val loss 2.137878894805908\n",
      "step 2050: train loss 2.095134735107422, val loss 2.1316635608673096\n",
      "step 2051: train loss 2.087540864944458, val loss 2.1292428970336914\n",
      "step 2052: train loss 2.084629774093628, val loss 2.136326789855957\n",
      "step 2053: train loss 2.0898022651672363, val loss 2.1342782974243164\n",
      "step 2054: train loss 2.091376781463623, val loss 2.138019561767578\n",
      "step 2055: train loss 2.08790922164917, val loss 2.130307197570801\n",
      "step 2056: train loss 2.0842490196228027, val loss 2.1368956565856934\n",
      "step 2057: train loss 2.0947110652923584, val loss 2.130389928817749\n",
      "step 2058: train loss 2.0798609256744385, val loss 2.1483635902404785\n",
      "step 2059: train loss 2.0905933380126953, val loss 2.1444244384765625\n",
      "step 2060: train loss 2.0949618816375732, val loss 2.141500473022461\n",
      "step 2061: train loss 2.091759443283081, val loss 2.144331932067871\n",
      "step 2062: train loss 2.092073678970337, val loss 2.1343438625335693\n",
      "step 2063: train loss 2.0905492305755615, val loss 2.142305374145508\n",
      "step 2064: train loss 2.099025011062622, val loss 2.1487066745758057\n",
      "step 2065: train loss 2.095357894897461, val loss 2.142123222351074\n",
      "step 2066: train loss 2.0942206382751465, val loss 2.1389453411102295\n",
      "step 2067: train loss 2.096465826034546, val loss 2.1362857818603516\n",
      "step 2068: train loss 2.0998809337615967, val loss 2.136841297149658\n",
      "step 2069: train loss 2.0923821926116943, val loss 2.1393675804138184\n",
      "step 2070: train loss 2.1003684997558594, val loss 2.1420986652374268\n",
      "step 2071: train loss 2.085055351257324, val loss 2.1337783336639404\n",
      "step 2072: train loss 2.092683792114258, val loss 2.1435546875\n",
      "step 2073: train loss 2.091702938079834, val loss 2.1437439918518066\n",
      "step 2074: train loss 2.100250482559204, val loss 2.1435799598693848\n",
      "step 2075: train loss 2.094649314880371, val loss 2.147240400314331\n",
      "step 2076: train loss 2.089430093765259, val loss 2.146728277206421\n",
      "step 2077: train loss 2.0818734169006348, val loss 2.1339664459228516\n",
      "step 2078: train loss 2.097836971282959, val loss 2.1312010288238525\n",
      "step 2079: train loss 2.0893006324768066, val loss 2.1351871490478516\n",
      "step 2080: train loss 2.0813755989074707, val loss 2.133718967437744\n",
      "step 2081: train loss 2.0883119106292725, val loss 2.1254122257232666\n",
      "step 2082: train loss 2.080862522125244, val loss 2.135977268218994\n",
      "step 2083: train loss 2.0850625038146973, val loss 2.1315155029296875\n",
      "step 2084: train loss 2.0866827964782715, val loss 2.1265296936035156\n",
      "step 2085: train loss 2.0896241664886475, val loss 2.1278209686279297\n",
      "step 2086: train loss 2.0730855464935303, val loss 2.1261370182037354\n",
      "step 2087: train loss 2.0859169960021973, val loss 2.1350595951080322\n",
      "step 2088: train loss 2.0843677520751953, val loss 2.1237692832946777\n",
      "step 2089: train loss 2.087578058242798, val loss 2.1336662769317627\n",
      "step 2090: train loss 2.091280460357666, val loss 2.129153251647949\n",
      "step 2091: train loss 2.093416690826416, val loss 2.125997543334961\n",
      "step 2092: train loss 2.0819191932678223, val loss 2.1274709701538086\n",
      "step 2093: train loss 2.0844757556915283, val loss 2.1227211952209473\n",
      "step 2094: train loss 2.0784056186676025, val loss 2.1132326126098633\n",
      "step 2095: train loss 2.0746517181396484, val loss 2.1184237003326416\n",
      "step 2096: train loss 2.078216791152954, val loss 2.1323728561401367\n",
      "step 2097: train loss 2.0789177417755127, val loss 2.1172125339508057\n",
      "step 2098: train loss 2.0825836658477783, val loss 2.1273937225341797\n",
      "step 2099: train loss 2.0865561962127686, val loss 2.1382393836975098\n",
      "step 2101: train loss 2.087662696838379, val loss 2.116947889328003\n",
      "step 2102: train loss 2.080174207687378, val loss 2.1333038806915283\n",
      "step 2103: train loss 2.082045316696167, val loss 2.1252758502960205\n",
      "step 2104: train loss 2.0815529823303223, val loss 2.121145248413086\n",
      "step 2105: train loss 2.073528528213501, val loss 2.121372699737549\n",
      "step 2106: train loss 2.0744283199310303, val loss 2.1312694549560547\n",
      "step 2107: train loss 2.0815954208374023, val loss 2.1314823627471924\n",
      "step 2108: train loss 2.076993703842163, val loss 2.1246936321258545\n",
      "step 2109: train loss 2.0792324542999268, val loss 2.1332476139068604\n",
      "step 2110: train loss 2.079920530319214, val loss 2.1231157779693604\n",
      "step 2111: train loss 2.0723776817321777, val loss 2.125650405883789\n",
      "step 2112: train loss 2.0767765045166016, val loss 2.1281418800354004\n",
      "step 2113: train loss 2.0703628063201904, val loss 2.128669023513794\n",
      "step 2114: train loss 2.0980024337768555, val loss 2.128024101257324\n",
      "step 2115: train loss 2.0810418128967285, val loss 2.126542329788208\n",
      "step 2116: train loss 2.0877387523651123, val loss 2.131908893585205\n",
      "step 2117: train loss 2.079342842102051, val loss 2.1232974529266357\n",
      "step 2118: train loss 2.0805118083953857, val loss 2.1331915855407715\n",
      "step 2119: train loss 2.0826473236083984, val loss 2.133044481277466\n",
      "step 2120: train loss 2.098855972290039, val loss 2.143371820449829\n",
      "step 2121: train loss 2.0908429622650146, val loss 2.138737916946411\n",
      "step 2122: train loss 2.0853989124298096, val loss 2.122340679168701\n",
      "step 2123: train loss 2.075239419937134, val loss 2.1208901405334473\n",
      "step 2124: train loss 2.0868401527404785, val loss 2.129178047180176\n",
      "step 2125: train loss 2.076706886291504, val loss 2.120454788208008\n",
      "step 2126: train loss 2.078549861907959, val loss 2.1157076358795166\n",
      "step 2127: train loss 2.075941801071167, val loss 2.12028431892395\n",
      "step 2128: train loss 2.064849376678467, val loss 2.123898506164551\n",
      "step 2129: train loss 2.0659780502319336, val loss 2.1182916164398193\n",
      "step 2130: train loss 2.0664727687835693, val loss 2.1207211017608643\n",
      "step 2131: train loss 2.0668795108795166, val loss 2.1176769733428955\n",
      "step 2132: train loss 2.069070339202881, val loss 2.1201813220977783\n",
      "step 2133: train loss 2.069261074066162, val loss 2.1178441047668457\n",
      "step 2134: train loss 2.0730817317962646, val loss 2.115828514099121\n",
      "step 2135: train loss 2.077226161956787, val loss 2.120821952819824\n",
      "step 2136: train loss 2.0788493156433105, val loss 2.117452621459961\n",
      "step 2137: train loss 2.0766167640686035, val loss 2.1277670860290527\n",
      "step 2138: train loss 2.0821588039398193, val loss 2.132199287414551\n",
      "step 2139: train loss 2.0792577266693115, val loss 2.133108377456665\n",
      "step 2140: train loss 2.0889382362365723, val loss 2.1404662132263184\n",
      "step 2141: train loss 2.0833818912506104, val loss 2.136152505874634\n",
      "step 2142: train loss 2.0744433403015137, val loss 2.127455949783325\n",
      "step 2143: train loss 2.0732696056365967, val loss 2.1105446815490723\n",
      "step 2144: train loss 2.0745153427124023, val loss 2.1259748935699463\n",
      "step 2145: train loss 2.0785508155822754, val loss 2.1257050037384033\n",
      "step 2146: train loss 2.075131893157959, val loss 2.119724988937378\n",
      "step 2147: train loss 2.0763497352600098, val loss 2.120598077774048\n",
      "step 2148: train loss 2.0719308853149414, val loss 2.124891996383667\n",
      "step 2149: train loss 2.0745222568511963, val loss 2.120786666870117\n",
      "step 2150: train loss 2.0713727474212646, val loss 2.1184494495391846\n",
      "step 2151: train loss 2.0746512413024902, val loss 2.1180601119995117\n",
      "step 2152: train loss 2.0728490352630615, val loss 2.121872901916504\n",
      "step 2153: train loss 2.069744825363159, val loss 2.113223075866699\n",
      "step 2154: train loss 2.0737292766571045, val loss 2.1140732765197754\n",
      "step 2155: train loss 2.074371814727783, val loss 2.1187632083892822\n",
      "step 2156: train loss 2.084162950515747, val loss 2.125699996948242\n",
      "step 2157: train loss 2.0796432495117188, val loss 2.117899179458618\n",
      "step 2158: train loss 2.0752785205841064, val loss 2.1201648712158203\n",
      "step 2159: train loss 2.074331283569336, val loss 2.116514205932617\n",
      "step 2160: train loss 2.070195198059082, val loss 2.1125986576080322\n",
      "step 2161: train loss 2.063843011856079, val loss 2.0984482765197754\n",
      "step 2162: train loss 2.071521520614624, val loss 2.1212143898010254\n",
      "step 2163: train loss 2.0751123428344727, val loss 2.115382432937622\n",
      "step 2164: train loss 2.081643581390381, val loss 2.112060785293579\n",
      "step 2165: train loss 2.0685384273529053, val loss 2.1254444122314453\n",
      "step 2166: train loss 2.0663442611694336, val loss 2.117367744445801\n",
      "step 2167: train loss 2.07403564453125, val loss 2.1229639053344727\n",
      "step 2168: train loss 2.0740389823913574, val loss 2.125509023666382\n",
      "step 2169: train loss 2.0768141746520996, val loss 2.1332898139953613\n",
      "step 2170: train loss 2.0664031505584717, val loss 2.1198084354400635\n",
      "step 2171: train loss 2.068002223968506, val loss 2.1205077171325684\n",
      "step 2172: train loss 2.075045347213745, val loss 2.1237103939056396\n",
      "step 2173: train loss 2.0633490085601807, val loss 2.1262645721435547\n",
      "step 2174: train loss 2.069481611251831, val loss 2.1231610774993896\n",
      "step 2175: train loss 2.0679855346679688, val loss 2.1101980209350586\n",
      "step 2176: train loss 2.066715955734253, val loss 2.1231446266174316\n",
      "step 2177: train loss 2.0701143741607666, val loss 2.122744083404541\n",
      "step 2178: train loss 2.0703160762786865, val loss 2.1150074005126953\n",
      "step 2179: train loss 2.0657761096954346, val loss 2.11787486076355\n",
      "step 2180: train loss 2.0607492923736572, val loss 2.118509292602539\n",
      "step 2181: train loss 2.0619046688079834, val loss 2.1103034019470215\n",
      "step 2182: train loss 2.060202121734619, val loss 2.1271703243255615\n",
      "step 2183: train loss 2.0684046745300293, val loss 2.106201648712158\n",
      "step 2184: train loss 2.0638599395751953, val loss 2.1085426807403564\n",
      "step 2185: train loss 2.0592586994171143, val loss 2.113210439682007\n",
      "step 2186: train loss 2.0603508949279785, val loss 2.1148719787597656\n",
      "step 2187: train loss 2.0726733207702637, val loss 2.112795829772949\n",
      "step 2188: train loss 2.0637400150299072, val loss 2.116962194442749\n",
      "step 2189: train loss 2.0629618167877197, val loss 2.114330530166626\n",
      "step 2190: train loss 2.0728423595428467, val loss 2.1191701889038086\n",
      "step 2191: train loss 2.0626137256622314, val loss 2.1057024002075195\n",
      "step 2192: train loss 2.062497138977051, val loss 2.1098434925079346\n",
      "step 2193: train loss 2.0572566986083984, val loss 2.111015796661377\n",
      "step 2194: train loss 2.064805269241333, val loss 2.1146903038024902\n",
      "step 2195: train loss 2.0585038661956787, val loss 2.1105215549468994\n",
      "step 2196: train loss 2.065023899078369, val loss 2.107107639312744\n",
      "step 2197: train loss 2.062779188156128, val loss 2.1082961559295654\n",
      "step 2198: train loss 2.0578067302703857, val loss 2.1061861515045166\n",
      "step 2199: train loss 2.0735278129577637, val loss 2.117358922958374\n",
      "step 2201: train loss 2.0697174072265625, val loss 2.1059322357177734\n",
      "step 2202: train loss 2.063760995864868, val loss 2.105630874633789\n",
      "step 2203: train loss 2.065797805786133, val loss 2.1083364486694336\n",
      "step 2204: train loss 2.063748836517334, val loss 2.105926513671875\n",
      "step 2205: train loss 2.0640571117401123, val loss 2.109800338745117\n",
      "step 2206: train loss 2.062673807144165, val loss 2.105534553527832\n",
      "step 2207: train loss 2.070711135864258, val loss 2.1128265857696533\n",
      "step 2208: train loss 2.0675084590911865, val loss 2.1064934730529785\n",
      "step 2209: train loss 2.058598518371582, val loss 2.104998826980591\n",
      "step 2210: train loss 2.058171033859253, val loss 2.107518196105957\n",
      "step 2211: train loss 2.060957431793213, val loss 2.10896372795105\n",
      "step 2212: train loss 2.06567120552063, val loss 2.117645025253296\n",
      "step 2213: train loss 2.070584774017334, val loss 2.1163742542266846\n",
      "step 2214: train loss 2.072194814682007, val loss 2.116344690322876\n",
      "step 2215: train loss 2.0657427310943604, val loss 2.123086929321289\n",
      "step 2216: train loss 2.072779893875122, val loss 2.1226236820220947\n",
      "step 2217: train loss 2.0679683685302734, val loss 2.1220645904541016\n",
      "step 2218: train loss 2.0669548511505127, val loss 2.120985984802246\n",
      "step 2219: train loss 2.066493511199951, val loss 2.104573965072632\n",
      "step 2220: train loss 2.060987949371338, val loss 2.1200449466705322\n",
      "step 2221: train loss 2.0681405067443848, val loss 2.1129250526428223\n",
      "step 2222: train loss 2.0605812072753906, val loss 2.10774302482605\n",
      "step 2223: train loss 2.0746400356292725, val loss 2.122316360473633\n",
      "step 2224: train loss 2.0607874393463135, val loss 2.105379581451416\n",
      "step 2225: train loss 2.056433916091919, val loss 2.1057546138763428\n",
      "step 2226: train loss 2.0687100887298584, val loss 2.101313829421997\n",
      "step 2227: train loss 2.0562801361083984, val loss 2.103905200958252\n",
      "step 2228: train loss 2.060969114303589, val loss 2.1047792434692383\n",
      "step 2229: train loss 2.064042568206787, val loss 2.112583637237549\n",
      "step 2230: train loss 2.0515079498291016, val loss 2.1078972816467285\n",
      "step 2231: train loss 2.056975841522217, val loss 2.1092031002044678\n",
      "step 2232: train loss 2.0656158924102783, val loss 2.106431722640991\n",
      "step 2233: train loss 2.064614772796631, val loss 2.105056047439575\n",
      "step 2234: train loss 2.065483808517456, val loss 2.1188132762908936\n",
      "step 2235: train loss 2.0609259605407715, val loss 2.110398054122925\n",
      "step 2236: train loss 2.0575995445251465, val loss 2.104043960571289\n",
      "step 2237: train loss 2.0642356872558594, val loss 2.1116466522216797\n",
      "step 2238: train loss 2.0608696937561035, val loss 2.1167614459991455\n",
      "step 2239: train loss 2.062437057495117, val loss 2.109983444213867\n",
      "step 2240: train loss 2.059674024581909, val loss 2.116800308227539\n",
      "step 2241: train loss 2.0661776065826416, val loss 2.1113295555114746\n",
      "step 2242: train loss 2.057130813598633, val loss 2.121466875076294\n",
      "step 2243: train loss 2.063988447189331, val loss 2.1176750659942627\n",
      "step 2244: train loss 2.0611519813537598, val loss 2.110285758972168\n",
      "step 2245: train loss 2.0581514835357666, val loss 2.1169538497924805\n",
      "step 2246: train loss 2.058366298675537, val loss 2.100620985031128\n",
      "step 2247: train loss 2.053016424179077, val loss 2.1050515174865723\n",
      "step 2248: train loss 2.0544252395629883, val loss 2.1001780033111572\n",
      "step 2249: train loss 2.0522513389587402, val loss 2.106637477874756\n",
      "step 2250: train loss 2.0492377281188965, val loss 2.1141676902770996\n",
      "step 2251: train loss 2.0645864009857178, val loss 2.113706111907959\n",
      "step 2252: train loss 2.0718042850494385, val loss 2.109315872192383\n",
      "step 2253: train loss 2.06302547454834, val loss 2.1055800914764404\n",
      "step 2254: train loss 2.07185697555542, val loss 2.1183347702026367\n",
      "step 2255: train loss 2.0567708015441895, val loss 2.1169281005859375\n",
      "step 2256: train loss 2.0655839443206787, val loss 2.1107053756713867\n",
      "step 2257: train loss 2.0695810317993164, val loss 2.1260011196136475\n",
      "step 2258: train loss 2.0707459449768066, val loss 2.118924617767334\n",
      "step 2259: train loss 2.069899320602417, val loss 2.1189844608306885\n",
      "step 2260: train loss 2.0620477199554443, val loss 2.1211655139923096\n",
      "step 2261: train loss 2.0626800060272217, val loss 2.107370615005493\n",
      "step 2262: train loss 2.0619819164276123, val loss 2.1096677780151367\n",
      "step 2263: train loss 2.0573318004608154, val loss 2.112088203430176\n",
      "step 2264: train loss 2.054568290710449, val loss 2.1077325344085693\n",
      "step 2265: train loss 2.0596747398376465, val loss 2.1050071716308594\n",
      "step 2266: train loss 2.0622973442077637, val loss 2.118981122970581\n",
      "step 2267: train loss 2.05312180519104, val loss 2.110767126083374\n",
      "step 2268: train loss 2.0573601722717285, val loss 2.1087305545806885\n",
      "step 2269: train loss 2.0563881397247314, val loss 2.097001552581787\n",
      "step 2270: train loss 2.054259777069092, val loss 2.1086883544921875\n",
      "step 2271: train loss 2.0557329654693604, val loss 2.106837034225464\n",
      "step 2272: train loss 2.057147979736328, val loss 2.1051278114318848\n",
      "step 2273: train loss 2.047668695449829, val loss 2.1096227169036865\n",
      "step 2274: train loss 2.0695321559906006, val loss 2.117614507675171\n",
      "step 2275: train loss 2.052929162979126, val loss 2.1125943660736084\n",
      "step 2276: train loss 2.05844783782959, val loss 2.108566999435425\n",
      "step 2277: train loss 2.0563013553619385, val loss 2.1007027626037598\n",
      "step 2278: train loss 2.05985164642334, val loss 2.107801914215088\n",
      "step 2279: train loss 2.0550312995910645, val loss 2.1005895137786865\n",
      "step 2280: train loss 2.056135654449463, val loss 2.1060242652893066\n",
      "step 2281: train loss 2.050274133682251, val loss 2.109994411468506\n",
      "step 2282: train loss 2.052619695663452, val loss 2.1045923233032227\n",
      "step 2283: train loss 2.0426876544952393, val loss 2.104921579360962\n",
      "step 2284: train loss 2.0471348762512207, val loss 2.094137191772461\n",
      "step 2285: train loss 2.0541305541992188, val loss 2.100745439529419\n",
      "step 2286: train loss 2.05410099029541, val loss 2.108767509460449\n",
      "step 2287: train loss 2.057312488555908, val loss 2.108818292617798\n",
      "step 2288: train loss 2.052260160446167, val loss 2.111250638961792\n",
      "step 2289: train loss 2.046421527862549, val loss 2.102174758911133\n",
      "step 2290: train loss 2.043426275253296, val loss 2.105790853500366\n",
      "step 2291: train loss 2.0526230335235596, val loss 2.0969574451446533\n",
      "step 2292: train loss 2.047755002975464, val loss 2.1068115234375\n",
      "step 2293: train loss 2.0610690116882324, val loss 2.102118968963623\n",
      "step 2294: train loss 2.051560163497925, val loss 2.1021697521209717\n",
      "step 2295: train loss 2.052781105041504, val loss 2.104482412338257\n",
      "step 2296: train loss 2.053164482116699, val loss 2.0999703407287598\n",
      "step 2297: train loss 2.0603675842285156, val loss 2.1052536964416504\n",
      "step 2298: train loss 2.057847261428833, val loss 2.1024367809295654\n",
      "step 2299: train loss 2.0558905601501465, val loss 2.0992467403411865\n",
      "step 2301: train loss 2.0497372150421143, val loss 2.097994089126587\n",
      "step 2302: train loss 2.0507352352142334, val loss 2.1062817573547363\n",
      "step 2303: train loss 2.041696548461914, val loss 2.1004068851470947\n",
      "step 2304: train loss 2.0519728660583496, val loss 2.095350980758667\n",
      "step 2305: train loss 2.0453526973724365, val loss 2.0949344635009766\n",
      "step 2306: train loss 2.053586959838867, val loss 2.090618848800659\n",
      "step 2307: train loss 2.0553526878356934, val loss 2.0899107456207275\n",
      "step 2308: train loss 2.04339599609375, val loss 2.100466012954712\n",
      "step 2309: train loss 2.052098274230957, val loss 2.1016740798950195\n",
      "step 2310: train loss 2.056555986404419, val loss 2.103461503982544\n",
      "step 2311: train loss 2.0577211380004883, val loss 2.0984723567962646\n",
      "step 2312: train loss 2.0677835941314697, val loss 2.1053972244262695\n",
      "step 2313: train loss 2.0606751441955566, val loss 2.099733829498291\n",
      "step 2314: train loss 2.048234701156616, val loss 2.094912052154541\n",
      "step 2315: train loss 2.042511224746704, val loss 2.0971174240112305\n",
      "step 2316: train loss 2.050473213195801, val loss 2.0958945751190186\n",
      "step 2317: train loss 2.0407321453094482, val loss 2.1034746170043945\n",
      "step 2318: train loss 2.061028480529785, val loss 2.1038451194763184\n",
      "step 2319: train loss 2.0547547340393066, val loss 2.108924150466919\n",
      "step 2320: train loss 2.0617480278015137, val loss 2.1008591651916504\n",
      "step 2321: train loss 2.042677879333496, val loss 2.0957508087158203\n",
      "step 2322: train loss 2.0457253456115723, val loss 2.0916872024536133\n",
      "step 2323: train loss 2.0409722328186035, val loss 2.101879358291626\n",
      "step 2324: train loss 2.045722007751465, val loss 2.0987398624420166\n",
      "step 2325: train loss 2.049751043319702, val loss 2.0944106578826904\n",
      "step 2326: train loss 2.0468194484710693, val loss 2.1019411087036133\n",
      "step 2327: train loss 2.045454740524292, val loss 2.107473373413086\n",
      "step 2328: train loss 2.0446743965148926, val loss 2.098856210708618\n",
      "step 2329: train loss 2.0537445545196533, val loss 2.105844020843506\n",
      "step 2330: train loss 2.0491161346435547, val loss 2.1094300746917725\n",
      "step 2331: train loss 2.060540199279785, val loss 2.1163628101348877\n",
      "step 2332: train loss 2.050717353820801, val loss 2.1163477897644043\n",
      "step 2333: train loss 2.0501952171325684, val loss 2.110398769378662\n",
      "step 2334: train loss 2.0388948917388916, val loss 2.098055362701416\n",
      "step 2335: train loss 2.043219566345215, val loss 2.1002869606018066\n",
      "step 2336: train loss 2.050593376159668, val loss 2.1028525829315186\n",
      "step 2337: train loss 2.0441384315490723, val loss 2.112187147140503\n",
      "step 2338: train loss 2.048682928085327, val loss 2.0994443893432617\n",
      "step 2339: train loss 2.0335092544555664, val loss 2.104288101196289\n",
      "step 2340: train loss 2.0380301475524902, val loss 2.1001458168029785\n",
      "step 2341: train loss 2.054872989654541, val loss 2.1003963947296143\n",
      "step 2342: train loss 2.054999828338623, val loss 2.109567165374756\n",
      "step 2343: train loss 2.043181896209717, val loss 2.113057851791382\n",
      "step 2344: train loss 2.040710210800171, val loss 2.095449447631836\n",
      "step 2345: train loss 2.0356833934783936, val loss 2.0887246131896973\n",
      "step 2346: train loss 2.045215606689453, val loss 2.1024389266967773\n",
      "step 2347: train loss 2.0360584259033203, val loss 2.098707675933838\n",
      "step 2348: train loss 2.040438652038574, val loss 2.1019771099090576\n",
      "step 2349: train loss 2.0388295650482178, val loss 2.0957908630371094\n",
      "step 2350: train loss 2.0391297340393066, val loss 2.1030993461608887\n",
      "step 2351: train loss 2.0422909259796143, val loss 2.0983495712280273\n",
      "step 2352: train loss 2.0426156520843506, val loss 2.0935399532318115\n",
      "step 2353: train loss 2.044299364089966, val loss 2.0958430767059326\n",
      "step 2354: train loss 2.039741277694702, val loss 2.0843210220336914\n",
      "step 2355: train loss 2.0416297912597656, val loss 2.0952842235565186\n",
      "step 2356: train loss 2.0464937686920166, val loss 2.106334686279297\n",
      "step 2357: train loss 2.047351837158203, val loss 2.0958945751190186\n",
      "step 2358: train loss 2.0332727432250977, val loss 2.0954177379608154\n",
      "step 2359: train loss 2.0416531562805176, val loss 2.090233564376831\n",
      "step 2360: train loss 2.034217596054077, val loss 2.089545965194702\n",
      "step 2361: train loss 2.0461881160736084, val loss 2.0990395545959473\n",
      "step 2362: train loss 2.052131414413452, val loss 2.09342360496521\n",
      "step 2363: train loss 2.050112247467041, val loss 2.0940423011779785\n",
      "step 2364: train loss 2.044332504272461, val loss 2.104631185531616\n",
      "step 2365: train loss 2.0379910469055176, val loss 2.095767021179199\n",
      "step 2366: train loss 2.0480470657348633, val loss 2.099731206893921\n",
      "step 2367: train loss 2.0443503856658936, val loss 2.101564884185791\n",
      "step 2368: train loss 2.048104763031006, val loss 2.1056907176971436\n",
      "step 2369: train loss 2.0396692752838135, val loss 2.096127986907959\n",
      "step 2370: train loss 2.048325777053833, val loss 2.110078811645508\n",
      "step 2371: train loss 2.0448379516601562, val loss 2.101215124130249\n",
      "step 2372: train loss 2.0509955883026123, val loss 2.097032070159912\n",
      "step 2373: train loss 2.0469987392425537, val loss 2.110703229904175\n",
      "step 2374: train loss 2.047024965286255, val loss 2.103877305984497\n",
      "step 2375: train loss 2.045020341873169, val loss 2.0951027870178223\n",
      "step 2376: train loss 2.045297622680664, val loss 2.1028003692626953\n",
      "step 2377: train loss 2.039158582687378, val loss 2.0978546142578125\n",
      "step 2378: train loss 2.0378191471099854, val loss 2.1041197776794434\n",
      "step 2379: train loss 2.037644386291504, val loss 2.096776247024536\n",
      "step 2380: train loss 2.0399906635284424, val loss 2.098940134048462\n",
      "step 2381: train loss 2.0385851860046387, val loss 2.106555461883545\n",
      "step 2382: train loss 2.043086290359497, val loss 2.101336717605591\n",
      "step 2383: train loss 2.0404577255249023, val loss 2.1056265830993652\n",
      "step 2384: train loss 2.050687789916992, val loss 2.0999295711517334\n",
      "step 2385: train loss 2.0464956760406494, val loss 2.1026053428649902\n",
      "step 2386: train loss 2.046642780303955, val loss 2.094238042831421\n",
      "step 2387: train loss 2.036475896835327, val loss 2.104977607727051\n",
      "step 2388: train loss 2.0383691787719727, val loss 2.0944466590881348\n",
      "step 2389: train loss 2.0326054096221924, val loss 2.093963861465454\n",
      "step 2390: train loss 2.03951358795166, val loss 2.092963457107544\n",
      "step 2391: train loss 2.034560203552246, val loss 2.0950238704681396\n",
      "step 2392: train loss 2.0376029014587402, val loss 2.0961170196533203\n",
      "step 2393: train loss 2.031771183013916, val loss 2.087052583694458\n",
      "step 2394: train loss 2.031517267227173, val loss 2.0926032066345215\n",
      "step 2395: train loss 2.033602714538574, val loss 2.0854101181030273\n",
      "step 2396: train loss 2.0315239429473877, val loss 2.087942600250244\n",
      "step 2397: train loss 2.0400781631469727, val loss 2.0944576263427734\n",
      "step 2398: train loss 2.0434372425079346, val loss 2.09846830368042\n",
      "step 2399: train loss 2.050269603729248, val loss 2.092466354370117\n",
      "step 2401: train loss 2.0324900150299072, val loss 2.081037759780884\n",
      "step 2402: train loss 2.0251235961914062, val loss 2.091095447540283\n",
      "step 2403: train loss 2.026895523071289, val loss 2.084496259689331\n",
      "step 2404: train loss 2.0374181270599365, val loss 2.090130090713501\n",
      "step 2405: train loss 2.036118268966675, val loss 2.085134983062744\n",
      "step 2406: train loss 2.029865264892578, val loss 2.089467763900757\n",
      "step 2407: train loss 2.029664993286133, val loss 2.0817601680755615\n",
      "step 2408: train loss 2.0325767993927, val loss 2.087411403656006\n",
      "step 2409: train loss 2.019418716430664, val loss 2.083315372467041\n",
      "step 2410: train loss 2.0249273777008057, val loss 2.079385995864868\n",
      "step 2411: train loss 2.022895574569702, val loss 2.084899425506592\n",
      "step 2412: train loss 2.0289864540100098, val loss 2.083725929260254\n",
      "step 2413: train loss 2.041390895843506, val loss 2.089200258255005\n",
      "step 2414: train loss 2.0311012268066406, val loss 2.0806591510772705\n",
      "step 2415: train loss 2.0265932083129883, val loss 2.0761005878448486\n",
      "step 2416: train loss 2.037052631378174, val loss 2.084935188293457\n",
      "step 2417: train loss 2.032700300216675, val loss 2.090466022491455\n",
      "step 2418: train loss 2.034127712249756, val loss 2.0834004878997803\n",
      "step 2419: train loss 2.040837287902832, val loss 2.097898244857788\n",
      "step 2420: train loss 2.038088083267212, val loss 2.089963912963867\n",
      "step 2421: train loss 2.0398120880126953, val loss 2.0939764976501465\n",
      "step 2422: train loss 2.042917251586914, val loss 2.0940582752227783\n",
      "step 2423: train loss 2.0315520763397217, val loss 2.091588020324707\n",
      "step 2424: train loss 2.0296621322631836, val loss 2.0860233306884766\n",
      "step 2425: train loss 2.0332329273223877, val loss 2.097489356994629\n",
      "step 2426: train loss 2.0381195545196533, val loss 2.0938940048217773\n",
      "step 2427: train loss 2.039853096008301, val loss 2.101961612701416\n",
      "step 2428: train loss 2.036825656890869, val loss 2.0943334102630615\n",
      "step 2429: train loss 2.038397789001465, val loss 2.0892958641052246\n",
      "step 2430: train loss 2.03008770942688, val loss 2.094646692276001\n",
      "step 2431: train loss 2.0370497703552246, val loss 2.082974433898926\n",
      "step 2432: train loss 2.0485684871673584, val loss 2.0926880836486816\n",
      "step 2433: train loss 2.0550310611724854, val loss 2.09592342376709\n",
      "step 2434: train loss 2.0420279502868652, val loss 2.091817855834961\n",
      "step 2435: train loss 2.0294697284698486, val loss 2.094403028488159\n",
      "step 2436: train loss 2.0397839546203613, val loss 2.086841344833374\n",
      "step 2437: train loss 2.038172960281372, val loss 2.0933289527893066\n",
      "step 2438: train loss 2.049410343170166, val loss 2.0914385318756104\n",
      "step 2439: train loss 2.0374889373779297, val loss 2.0924172401428223\n",
      "step 2440: train loss 2.0388832092285156, val loss 2.092318058013916\n",
      "step 2441: train loss 2.034621477127075, val loss 2.0957822799682617\n",
      "step 2442: train loss 2.026625633239746, val loss 2.077695846557617\n",
      "step 2443: train loss 2.034811496734619, val loss 2.0831148624420166\n",
      "step 2444: train loss 2.0488553047180176, val loss 2.091733694076538\n",
      "step 2445: train loss 2.0441155433654785, val loss 2.0982983112335205\n",
      "step 2446: train loss 2.0443451404571533, val loss 2.086798667907715\n",
      "step 2447: train loss 2.030007839202881, val loss 2.0888357162475586\n",
      "step 2448: train loss 2.0377140045166016, val loss 2.0828850269317627\n",
      "step 2449: train loss 2.0404129028320312, val loss 2.085231065750122\n",
      "step 2450: train loss 2.0278708934783936, val loss 2.0837202072143555\n",
      "step 2451: train loss 2.035407781600952, val loss 2.0901994705200195\n",
      "step 2452: train loss 2.0324738025665283, val loss 2.0961947441101074\n",
      "step 2453: train loss 2.0272669792175293, val loss 2.095214605331421\n",
      "step 2454: train loss 2.0232932567596436, val loss 2.0919196605682373\n",
      "step 2455: train loss 2.0269484519958496, val loss 2.0860767364501953\n",
      "step 2456: train loss 2.0281851291656494, val loss 2.0843443870544434\n",
      "step 2457: train loss 2.033831834793091, val loss 2.0818445682525635\n",
      "step 2458: train loss 2.0320205688476562, val loss 2.097250461578369\n",
      "step 2459: train loss 2.0325112342834473, val loss 2.0918869972229004\n",
      "step 2460: train loss 2.036499261856079, val loss 2.0990943908691406\n",
      "step 2461: train loss 2.0302376747131348, val loss 2.0904288291931152\n",
      "step 2462: train loss 2.0286192893981934, val loss 2.088716506958008\n",
      "step 2463: train loss 2.0291876792907715, val loss 2.0814688205718994\n",
      "step 2464: train loss 2.0412251949310303, val loss 2.0982046127319336\n",
      "step 2465: train loss 2.0393319129943848, val loss 2.082339286804199\n",
      "step 2466: train loss 2.027808904647827, val loss 2.08693265914917\n",
      "step 2467: train loss 2.026538133621216, val loss 2.0756940841674805\n",
      "step 2468: train loss 2.036674976348877, val loss 2.0891671180725098\n",
      "step 2469: train loss 2.0344464778900146, val loss 2.0900163650512695\n",
      "step 2470: train loss 2.033324956893921, val loss 2.0907676219940186\n",
      "step 2471: train loss 2.033600330352783, val loss 2.0891761779785156\n",
      "step 2472: train loss 2.043835163116455, val loss 2.0988683700561523\n",
      "step 2473: train loss 2.029550552368164, val loss 2.0990238189697266\n",
      "step 2474: train loss 2.028549909591675, val loss 2.095768690109253\n",
      "step 2475: train loss 2.027894973754883, val loss 2.0931971073150635\n",
      "step 2476: train loss 2.0218279361724854, val loss 2.0858213901519775\n",
      "step 2477: train loss 2.026071786880493, val loss 2.095463514328003\n",
      "step 2478: train loss 2.0255813598632812, val loss 2.093261957168579\n",
      "step 2479: train loss 2.02052640914917, val loss 2.0880517959594727\n",
      "step 2480: train loss 2.0240983963012695, val loss 2.091353416442871\n",
      "step 2481: train loss 2.0218122005462646, val loss 2.085123300552368\n",
      "step 2482: train loss 2.0278401374816895, val loss 2.0939464569091797\n",
      "step 2483: train loss 2.0300583839416504, val loss 2.0832903385162354\n",
      "step 2484: train loss 2.0166730880737305, val loss 2.0815582275390625\n",
      "step 2485: train loss 2.023399591445923, val loss 2.0848748683929443\n",
      "step 2486: train loss 2.018153429031372, val loss 2.0865328311920166\n",
      "step 2487: train loss 2.028278112411499, val loss 2.0890417098999023\n",
      "step 2488: train loss 2.028822898864746, val loss 2.0897843837738037\n",
      "step 2489: train loss 2.0240566730499268, val loss 2.0864462852478027\n",
      "step 2490: train loss 2.0236501693725586, val loss 2.085909128189087\n",
      "step 2491: train loss 2.0305261611938477, val loss 2.0871994495391846\n",
      "step 2492: train loss 2.026749610900879, val loss 2.083982229232788\n",
      "step 2493: train loss 2.016319513320923, val loss 2.0849227905273438\n",
      "step 2494: train loss 2.0230376720428467, val loss 2.0839779376983643\n",
      "step 2495: train loss 2.022832155227661, val loss 2.0892467498779297\n",
      "step 2496: train loss 2.0282254219055176, val loss 2.075263500213623\n",
      "step 2497: train loss 2.0196945667266846, val loss 2.0763206481933594\n",
      "step 2498: train loss 2.0215635299682617, val loss 2.078573703765869\n",
      "step 2499: train loss 2.0237534046173096, val loss 2.0839226245880127\n",
      "step 2501: train loss 2.025376796722412, val loss 2.079411745071411\n",
      "step 2502: train loss 2.027514696121216, val loss 2.0777204036712646\n",
      "step 2503: train loss 2.0204079151153564, val loss 2.0788352489471436\n",
      "step 2504: train loss 2.022153377532959, val loss 2.0823302268981934\n",
      "step 2505: train loss 2.0224733352661133, val loss 2.069518804550171\n",
      "step 2506: train loss 2.024181604385376, val loss 2.0848946571350098\n",
      "step 2507: train loss 2.0199828147888184, val loss 2.0780837535858154\n",
      "step 2508: train loss 2.031726598739624, val loss 2.0914371013641357\n",
      "step 2509: train loss 2.022225856781006, val loss 2.081902027130127\n",
      "step 2510: train loss 2.021364212036133, val loss 2.0941410064697266\n",
      "step 2511: train loss 2.020024299621582, val loss 2.0889971256256104\n",
      "step 2512: train loss 2.0245237350463867, val loss 2.0909688472747803\n",
      "step 2513: train loss 2.0213735103607178, val loss 2.0838799476623535\n",
      "step 2514: train loss 2.0174412727355957, val loss 2.084601402282715\n",
      "step 2515: train loss 2.0187413692474365, val loss 2.0826711654663086\n",
      "step 2516: train loss 2.021233320236206, val loss 2.0890657901763916\n",
      "step 2517: train loss 2.017427682876587, val loss 2.0944039821624756\n",
      "step 2518: train loss 2.0230302810668945, val loss 2.0952653884887695\n",
      "step 2519: train loss 2.024134874343872, val loss 2.092043876647949\n",
      "step 2520: train loss 2.0230143070220947, val loss 2.0780153274536133\n",
      "step 2521: train loss 2.0200629234313965, val loss 2.096381187438965\n",
      "step 2522: train loss 2.0277974605560303, val loss 2.0787529945373535\n",
      "step 2523: train loss 2.019948720932007, val loss 2.080672264099121\n",
      "step 2524: train loss 2.0073843002319336, val loss 2.076988697052002\n",
      "step 2525: train loss 2.0132570266723633, val loss 2.069154977798462\n",
      "step 2526: train loss 2.031982898712158, val loss 2.075777053833008\n",
      "step 2527: train loss 2.033958673477173, val loss 2.094162702560425\n",
      "step 2528: train loss 2.027097225189209, val loss 2.0807225704193115\n",
      "step 2529: train loss 2.014873743057251, val loss 2.0764877796173096\n",
      "step 2530: train loss 2.0194649696350098, val loss 2.0783767700195312\n",
      "step 2531: train loss 2.020348310470581, val loss 2.072000026702881\n",
      "step 2532: train loss 2.0213637351989746, val loss 2.0703487396240234\n",
      "step 2533: train loss 2.0240097045898438, val loss 2.08088755607605\n",
      "step 2534: train loss 2.021151065826416, val loss 2.076010227203369\n",
      "step 2535: train loss 2.0206596851348877, val loss 2.077481746673584\n",
      "step 2536: train loss 2.0245463848114014, val loss 2.082860231399536\n",
      "step 2537: train loss 2.030571222305298, val loss 2.076219081878662\n",
      "step 2538: train loss 2.023733139038086, val loss 2.0738043785095215\n",
      "step 2539: train loss 2.0145103931427, val loss 2.069314956665039\n",
      "step 2540: train loss 2.0232465267181396, val loss 2.0741512775421143\n",
      "step 2541: train loss 2.0215401649475098, val loss 2.0861783027648926\n",
      "step 2542: train loss 2.0293750762939453, val loss 2.094956636428833\n",
      "step 2543: train loss 2.035494089126587, val loss 2.098003625869751\n",
      "step 2544: train loss 2.0404622554779053, val loss 2.0957131385803223\n",
      "step 2545: train loss 2.018991470336914, val loss 2.079005002975464\n",
      "step 2546: train loss 2.0188019275665283, val loss 2.079730749130249\n",
      "step 2547: train loss 2.0239837169647217, val loss 2.0729758739471436\n",
      "step 2548: train loss 2.026654005050659, val loss 2.076687812805176\n",
      "step 2549: train loss 2.0149779319763184, val loss 2.0739598274230957\n",
      "step 2550: train loss 2.0259368419647217, val loss 2.068767547607422\n",
      "step 2551: train loss 2.0312814712524414, val loss 2.0821170806884766\n",
      "step 2552: train loss 2.0206193923950195, val loss 2.074570417404175\n",
      "step 2553: train loss 2.0291073322296143, val loss 2.0699689388275146\n",
      "step 2554: train loss 2.023594617843628, val loss 2.0709714889526367\n",
      "step 2555: train loss 2.017641067504883, val loss 2.0761098861694336\n",
      "step 2556: train loss 2.023256540298462, val loss 2.073662519454956\n",
      "step 2557: train loss 2.01937198638916, val loss 2.070082902908325\n",
      "step 2558: train loss 2.0213632583618164, val loss 2.077385663986206\n",
      "step 2559: train loss 2.0216238498687744, val loss 2.072387933731079\n",
      "step 2560: train loss 2.0199265480041504, val loss 2.0710909366607666\n",
      "step 2561: train loss 2.0132863521575928, val loss 2.0721569061279297\n",
      "step 2562: train loss 2.0124142169952393, val loss 2.067415237426758\n",
      "step 2563: train loss 2.0123376846313477, val loss 2.076592445373535\n",
      "step 2564: train loss 2.0214056968688965, val loss 2.0847983360290527\n",
      "step 2565: train loss 2.023620367050171, val loss 2.0838189125061035\n",
      "step 2566: train loss 2.016620397567749, val loss 2.0834884643554688\n",
      "step 2567: train loss 2.016660690307617, val loss 2.0790703296661377\n",
      "step 2568: train loss 2.011195182800293, val loss 2.0707590579986572\n",
      "step 2569: train loss 2.0140933990478516, val loss 2.078855514526367\n",
      "step 2570: train loss 2.0097203254699707, val loss 2.0669469833374023\n",
      "step 2571: train loss 2.0156402587890625, val loss 2.0779285430908203\n",
      "step 2572: train loss 2.01615571975708, val loss 2.0676381587982178\n",
      "step 2573: train loss 2.0148885250091553, val loss 2.068749189376831\n",
      "step 2574: train loss 2.014533758163452, val loss 2.0714516639709473\n",
      "step 2575: train loss 2.012336015701294, val loss 2.081887722015381\n",
      "step 2576: train loss 2.0055973529815674, val loss 2.0785505771636963\n",
      "step 2577: train loss 2.009984016418457, val loss 2.0706777572631836\n",
      "step 2578: train loss 2.010336399078369, val loss 2.078826904296875\n",
      "step 2579: train loss 2.0089755058288574, val loss 2.079630136489868\n",
      "step 2580: train loss 2.0171539783477783, val loss 2.074557065963745\n",
      "step 2581: train loss 2.0177462100982666, val loss 2.082841396331787\n",
      "step 2582: train loss 2.0283684730529785, val loss 2.084604263305664\n",
      "step 2583: train loss 2.0122580528259277, val loss 2.083242654800415\n",
      "step 2584: train loss 2.0185179710388184, val loss 2.0838348865509033\n",
      "step 2585: train loss 2.006518602371216, val loss 2.0742881298065186\n",
      "step 2586: train loss 2.0175862312316895, val loss 2.073390245437622\n",
      "step 2587: train loss 2.024894952774048, val loss 2.083498001098633\n",
      "step 2588: train loss 2.0319669246673584, val loss 2.0806782245635986\n",
      "step 2589: train loss 2.0182082653045654, val loss 2.077873468399048\n",
      "step 2590: train loss 2.010145664215088, val loss 2.0689618587493896\n",
      "step 2591: train loss 2.0055298805236816, val loss 2.0524325370788574\n",
      "step 2592: train loss 2.0128700733184814, val loss 2.0609188079833984\n",
      "step 2593: train loss 2.0310449600219727, val loss 2.0778961181640625\n",
      "step 2594: train loss 2.0149431228637695, val loss 2.0797665119171143\n",
      "step 2595: train loss 2.016383409500122, val loss 2.0819451808929443\n",
      "step 2596: train loss 2.0134615898132324, val loss 2.0712389945983887\n",
      "step 2597: train loss 2.0229151248931885, val loss 2.076538324356079\n",
      "step 2598: train loss 2.0212347507476807, val loss 2.0744779109954834\n",
      "step 2599: train loss 2.012147903442383, val loss 2.0699405670166016\n",
      "step 2601: train loss 2.008477210998535, val loss 2.0808358192443848\n",
      "step 2602: train loss 2.0098047256469727, val loss 2.0718390941619873\n",
      "step 2603: train loss 2.006962537765503, val loss 2.0739314556121826\n",
      "step 2604: train loss 2.0016679763793945, val loss 2.062272310256958\n",
      "step 2605: train loss 1.999434232711792, val loss 2.0648255348205566\n",
      "step 2606: train loss 1.9989607334136963, val loss 2.0595152378082275\n",
      "step 2607: train loss 2.002328634262085, val loss 2.06618595123291\n",
      "step 2608: train loss 2.0175375938415527, val loss 2.0744729042053223\n",
      "step 2609: train loss 2.0180158615112305, val loss 2.0784518718719482\n",
      "step 2610: train loss 2.0204079151153564, val loss 2.070589065551758\n",
      "step 2611: train loss 2.011798143386841, val loss 2.078187942504883\n",
      "step 2612: train loss 2.0069375038146973, val loss 2.0749173164367676\n",
      "step 2613: train loss 2.004432439804077, val loss 2.0716171264648438\n",
      "step 2614: train loss 2.014831304550171, val loss 2.081019401550293\n",
      "step 2615: train loss 2.005462646484375, val loss 2.0821855068206787\n",
      "step 2616: train loss 2.008608818054199, val loss 2.07452654838562\n",
      "step 2617: train loss 2.0071403980255127, val loss 2.065253973007202\n",
      "step 2618: train loss 2.0040132999420166, val loss 2.062114953994751\n",
      "step 2619: train loss 1.999692678451538, val loss 2.0725388526916504\n",
      "step 2620: train loss 1.99967360496521, val loss 2.074517250061035\n",
      "step 2621: train loss 2.0038251876831055, val loss 2.0750114917755127\n",
      "step 2622: train loss 2.0017144680023193, val loss 2.0787513256073\n",
      "step 2623: train loss 2.0067825317382812, val loss 2.0710291862487793\n",
      "step 2624: train loss 2.0216448307037354, val loss 2.0700113773345947\n",
      "step 2625: train loss 2.0135233402252197, val loss 2.0688533782958984\n",
      "step 2626: train loss 2.0079121589660645, val loss 2.0630459785461426\n",
      "step 2627: train loss 2.0030314922332764, val loss 2.068927049636841\n",
      "step 2628: train loss 1.9993717670440674, val loss 2.0655641555786133\n",
      "step 2629: train loss 2.0007710456848145, val loss 2.0707879066467285\n",
      "step 2630: train loss 2.0169730186462402, val loss 2.0707590579986572\n",
      "step 2631: train loss 2.011293411254883, val loss 2.0820059776306152\n",
      "step 2632: train loss 2.0104339122772217, val loss 2.0698494911193848\n",
      "step 2633: train loss 2.017625570297241, val loss 2.0697760581970215\n",
      "step 2634: train loss 2.0013954639434814, val loss 2.066737651824951\n",
      "step 2635: train loss 2.0073468685150146, val loss 2.0682928562164307\n",
      "step 2636: train loss 2.0079963207244873, val loss 2.073788642883301\n",
      "step 2637: train loss 2.0101237297058105, val loss 2.0646774768829346\n",
      "step 2638: train loss 2.0100626945495605, val loss 2.0770490169525146\n",
      "step 2639: train loss 2.0064585208892822, val loss 2.0648937225341797\n",
      "step 2640: train loss 1.999688982963562, val loss 2.068671941757202\n",
      "step 2641: train loss 2.0115303993225098, val loss 2.0753469467163086\n",
      "step 2642: train loss 2.0076346397399902, val loss 2.066054344177246\n",
      "step 2643: train loss 2.0047261714935303, val loss 2.064774513244629\n",
      "step 2644: train loss 2.014481782913208, val loss 2.063096523284912\n",
      "step 2645: train loss 2.0007734298706055, val loss 2.0669479370117188\n",
      "step 2646: train loss 2.0081822872161865, val loss 2.072262763977051\n",
      "step 2647: train loss 2.0062320232391357, val loss 2.0597352981567383\n",
      "step 2648: train loss 2.010219097137451, val loss 2.069185733795166\n",
      "step 2649: train loss 2.008992910385132, val loss 2.0704243183135986\n",
      "step 2650: train loss 2.010078191757202, val loss 2.0570549964904785\n",
      "step 2651: train loss 2.004363775253296, val loss 2.0644326210021973\n",
      "step 2652: train loss 2.012310743331909, val loss 2.062021255493164\n",
      "step 2653: train loss 2.0091474056243896, val loss 2.069429397583008\n",
      "step 2654: train loss 2.0007004737854004, val loss 2.062130928039551\n",
      "step 2655: train loss 2.0041091442108154, val loss 2.064509391784668\n",
      "step 2656: train loss 2.0088393688201904, val loss 2.0616610050201416\n",
      "step 2657: train loss 2.007798194885254, val loss 2.0703277587890625\n",
      "step 2658: train loss 1.9964770078659058, val loss 2.0605885982513428\n",
      "step 2659: train loss 2.009077787399292, val loss 2.0586190223693848\n",
      "step 2660: train loss 2.0032596588134766, val loss 2.063091516494751\n",
      "step 2661: train loss 1.993552803993225, val loss 2.0590760707855225\n",
      "step 2662: train loss 2.0074589252471924, val loss 2.060856819152832\n",
      "step 2663: train loss 2.0065884590148926, val loss 2.0651590824127197\n",
      "step 2664: train loss 2.0035483837127686, val loss 2.063466787338257\n",
      "step 2665: train loss 2.0044121742248535, val loss 2.0674521923065186\n",
      "step 2666: train loss 1.9982162714004517, val loss 2.064023971557617\n",
      "step 2667: train loss 2.0018668174743652, val loss 2.0515127182006836\n",
      "step 2668: train loss 1.9985488653182983, val loss 2.062894105911255\n",
      "step 2669: train loss 2.003019332885742, val loss 2.0562098026275635\n",
      "step 2670: train loss 1.9973028898239136, val loss 2.063326835632324\n",
      "step 2671: train loss 2.015159845352173, val loss 2.0618624687194824\n",
      "step 2672: train loss 1.9968928098678589, val loss 2.0677034854888916\n",
      "step 2673: train loss 2.0086019039154053, val loss 2.0635082721710205\n",
      "step 2674: train loss 2.00821590423584, val loss 2.0674898624420166\n",
      "step 2675: train loss 2.0027313232421875, val loss 2.0655269622802734\n",
      "step 2676: train loss 2.0005459785461426, val loss 2.0635924339294434\n",
      "step 2677: train loss 1.993605136871338, val loss 2.069079637527466\n",
      "step 2678: train loss 2.0024759769439697, val loss 2.0580289363861084\n",
      "step 2679: train loss 1.999741792678833, val loss 2.0730226039886475\n",
      "step 2680: train loss 1.998689889907837, val loss 2.061112403869629\n",
      "step 2681: train loss 2.0000901222229004, val loss 2.0661680698394775\n",
      "step 2682: train loss 2.0031769275665283, val loss 2.0661516189575195\n",
      "step 2683: train loss 2.0002574920654297, val loss 2.0728182792663574\n",
      "step 2684: train loss 2.007857322692871, val loss 2.0785083770751953\n",
      "step 2685: train loss 2.0078134536743164, val loss 2.062617301940918\n",
      "step 2686: train loss 2.003561019897461, val loss 2.068192481994629\n",
      "step 2687: train loss 2.0080840587615967, val loss 2.0688672065734863\n",
      "step 2688: train loss 2.000641107559204, val loss 2.0727386474609375\n",
      "step 2689: train loss 1.9959050416946411, val loss 2.0623080730438232\n",
      "step 2690: train loss 2.0056509971618652, val loss 2.064645767211914\n",
      "step 2691: train loss 2.0046017169952393, val loss 2.0599918365478516\n",
      "step 2692: train loss 1.9898873567581177, val loss 2.0660300254821777\n",
      "step 2693: train loss 2.001006841659546, val loss 2.0603830814361572\n",
      "step 2694: train loss 1.9988479614257812, val loss 2.063322067260742\n",
      "step 2695: train loss 2.0016350746154785, val loss 2.0587317943573\n",
      "step 2696: train loss 2.004065752029419, val loss 2.074700117111206\n",
      "step 2697: train loss 2.0003912448883057, val loss 2.0646324157714844\n",
      "step 2698: train loss 2.0069093704223633, val loss 2.06488299369812\n",
      "step 2699: train loss 2.0046586990356445, val loss 2.060058355331421\n",
      "step 2701: train loss 2.004448175430298, val loss 2.0631604194641113\n",
      "step 2702: train loss 1.9999902248382568, val loss 2.0700411796569824\n",
      "step 2703: train loss 2.007143974304199, val loss 2.068272352218628\n",
      "step 2704: train loss 2.0046660900115967, val loss 2.0715255737304688\n",
      "step 2705: train loss 2.00791072845459, val loss 2.0733771324157715\n",
      "step 2706: train loss 1.9951543807983398, val loss 2.073885202407837\n",
      "step 2707: train loss 1.9997448921203613, val loss 2.066422939300537\n",
      "step 2708: train loss 2.002258539199829, val loss 2.060518980026245\n",
      "step 2709: train loss 2.0086281299591064, val loss 2.0578339099884033\n",
      "step 2710: train loss 2.0074338912963867, val loss 2.0676724910736084\n",
      "step 2711: train loss 2.0041332244873047, val loss 2.068953037261963\n",
      "step 2712: train loss 2.009622573852539, val loss 2.0668697357177734\n",
      "step 2713: train loss 2.0113348960876465, val loss 2.0659468173980713\n",
      "step 2714: train loss 1.993873953819275, val loss 2.071627140045166\n",
      "step 2715: train loss 2.001791000366211, val loss 2.0616438388824463\n",
      "step 2716: train loss 1.996976375579834, val loss 2.059715986251831\n",
      "step 2717: train loss 1.991418480873108, val loss 2.0516197681427\n",
      "step 2718: train loss 1.9930163621902466, val loss 2.0485754013061523\n",
      "step 2719: train loss 2.0021893978118896, val loss 2.0627384185791016\n",
      "step 2720: train loss 2.0004894733428955, val loss 2.071324586868286\n",
      "step 2721: train loss 2.003295660018921, val loss 2.0673933029174805\n",
      "step 2722: train loss 1.9985238313674927, val loss 2.0691399574279785\n",
      "step 2723: train loss 2.001145601272583, val loss 2.0658328533172607\n",
      "step 2724: train loss 2.0038533210754395, val loss 2.07877254486084\n",
      "step 2725: train loss 2.0054073333740234, val loss 2.068589210510254\n",
      "step 2726: train loss 2.0045905113220215, val loss 2.0660641193389893\n",
      "step 2727: train loss 1.9997456073760986, val loss 2.061617612838745\n",
      "step 2728: train loss 1.9942671060562134, val loss 2.0744259357452393\n",
      "step 2729: train loss 1.994504690170288, val loss 2.0587990283966064\n",
      "step 2730: train loss 1.997889518737793, val loss 2.0613346099853516\n",
      "step 2731: train loss 1.9935892820358276, val loss 2.064136266708374\n",
      "step 2732: train loss 1.986171841621399, val loss 2.0588974952697754\n",
      "step 2733: train loss 1.9859460592269897, val loss 2.058807373046875\n",
      "step 2734: train loss 1.9834263324737549, val loss 2.062643051147461\n",
      "step 2735: train loss 1.9915333986282349, val loss 2.0702104568481445\n",
      "step 2736: train loss 1.9981179237365723, val loss 2.055727005004883\n",
      "step 2737: train loss 1.9948676824569702, val loss 2.071901559829712\n",
      "step 2738: train loss 2.0075480937957764, val loss 2.068112850189209\n",
      "step 2739: train loss 2.0070688724517822, val loss 2.0651497840881348\n",
      "step 2740: train loss 2.003939628601074, val loss 2.0648391246795654\n",
      "step 2741: train loss 1.988945722579956, val loss 2.066124200820923\n",
      "step 2742: train loss 1.9940224885940552, val loss 2.053823947906494\n",
      "step 2743: train loss 1.9861661195755005, val loss 2.065371036529541\n",
      "step 2744: train loss 1.9954332113265991, val loss 2.0585193634033203\n",
      "step 2745: train loss 1.9959168434143066, val loss 2.0527477264404297\n",
      "step 2746: train loss 2.0011720657348633, val loss 2.0617337226867676\n",
      "step 2747: train loss 1.9962579011917114, val loss 2.051527500152588\n",
      "step 2748: train loss 1.995639681816101, val loss 2.063048839569092\n",
      "step 2749: train loss 1.9958492517471313, val loss 2.066936492919922\n",
      "step 2750: train loss 1.9896808862686157, val loss 2.0483016967773438\n",
      "step 2751: train loss 1.9968843460083008, val loss 2.067314386367798\n",
      "step 2752: train loss 1.989926815032959, val loss 2.060818672180176\n",
      "step 2753: train loss 1.9835994243621826, val loss 2.0566887855529785\n",
      "step 2754: train loss 1.9959373474121094, val loss 2.05387806892395\n",
      "step 2755: train loss 1.986204981803894, val loss 2.0599520206451416\n",
      "step 2756: train loss 1.9911555051803589, val loss 2.0546066761016846\n",
      "step 2757: train loss 1.9835374355316162, val loss 2.056774377822876\n",
      "step 2758: train loss 1.9927364587783813, val loss 2.0604915618896484\n",
      "step 2759: train loss 1.997029423713684, val loss 2.061985492706299\n",
      "step 2760: train loss 1.9967010021209717, val loss 2.061445951461792\n",
      "step 2761: train loss 1.9928767681121826, val loss 2.0606305599212646\n",
      "step 2762: train loss 1.9964762926101685, val loss 2.0610435009002686\n",
      "step 2763: train loss 1.99053955078125, val loss 2.046632766723633\n",
      "step 2764: train loss 1.992266058921814, val loss 2.0462751388549805\n",
      "step 2765: train loss 1.9983729124069214, val loss 2.049604892730713\n",
      "step 2766: train loss 1.9881078004837036, val loss 2.0565383434295654\n",
      "step 2767: train loss 1.9983242750167847, val loss 2.0580835342407227\n",
      "step 2768: train loss 1.9923208951950073, val loss 2.0610413551330566\n",
      "step 2769: train loss 2.0033624172210693, val loss 2.0618698596954346\n",
      "step 2770: train loss 1.9908251762390137, val loss 2.0679209232330322\n",
      "step 2771: train loss 1.991159200668335, val loss 2.053678512573242\n",
      "step 2772: train loss 1.9788928031921387, val loss 2.043703556060791\n",
      "step 2773: train loss 1.9951046705245972, val loss 2.0597188472747803\n",
      "step 2774: train loss 1.9866958856582642, val loss 2.0488803386688232\n",
      "step 2775: train loss 1.9880061149597168, val loss 2.0519461631774902\n",
      "step 2776: train loss 2.0008716583251953, val loss 2.0524423122406006\n",
      "step 2777: train loss 1.9805779457092285, val loss 2.058535099029541\n",
      "step 2778: train loss 1.9892401695251465, val loss 2.0559208393096924\n",
      "step 2779: train loss 1.9925765991210938, val loss 2.0528922080993652\n",
      "step 2780: train loss 1.986199975013733, val loss 2.052480936050415\n",
      "step 2781: train loss 1.986243486404419, val loss 2.0535178184509277\n",
      "step 2782: train loss 1.9820088148117065, val loss 2.057028293609619\n",
      "step 2783: train loss 1.9834691286087036, val loss 2.046785831451416\n",
      "step 2784: train loss 1.9859040975570679, val loss 2.059387445449829\n",
      "step 2785: train loss 1.993135690689087, val loss 2.0518722534179688\n",
      "step 2786: train loss 1.9888432025909424, val loss 2.046766757965088\n",
      "step 2787: train loss 1.9860436916351318, val loss 2.0576064586639404\n",
      "step 2788: train loss 1.983915090560913, val loss 2.0528502464294434\n",
      "step 2789: train loss 1.9801642894744873, val loss 2.055950403213501\n",
      "step 2790: train loss 1.9859510660171509, val loss 2.0577497482299805\n",
      "step 2791: train loss 1.9805721044540405, val loss 2.0628738403320312\n",
      "step 2792: train loss 1.9905014038085938, val loss 2.0548126697540283\n",
      "step 2793: train loss 1.989845633506775, val loss 2.0551953315734863\n",
      "step 2794: train loss 1.987094759941101, val loss 2.0662124156951904\n",
      "step 2795: train loss 1.9918802976608276, val loss 2.065523386001587\n",
      "step 2796: train loss 1.9945263862609863, val loss 2.0622663497924805\n",
      "step 2797: train loss 1.988088846206665, val loss 2.0622451305389404\n",
      "step 2798: train loss 1.9871795177459717, val loss 2.048743486404419\n",
      "step 2799: train loss 1.9830126762390137, val loss 2.0571815967559814\n",
      "step 2801: train loss 1.9805161952972412, val loss 2.0478460788726807\n",
      "step 2802: train loss 1.9798378944396973, val loss 2.0610976219177246\n",
      "step 2803: train loss 1.9994491338729858, val loss 2.0560433864593506\n",
      "step 2804: train loss 1.9851402044296265, val loss 2.062999963760376\n",
      "step 2805: train loss 1.991705060005188, val loss 2.0585098266601562\n",
      "step 2806: train loss 1.9946153163909912, val loss 2.0580451488494873\n",
      "step 2807: train loss 1.9872578382492065, val loss 2.060281276702881\n",
      "step 2808: train loss 1.9799789190292358, val loss 2.0526907444000244\n",
      "step 2809: train loss 1.9731643199920654, val loss 2.0710999965667725\n",
      "step 2810: train loss 1.9767286777496338, val loss 2.052255630493164\n",
      "step 2811: train loss 1.987281322479248, val loss 2.0628905296325684\n",
      "step 2812: train loss 1.9885835647583008, val loss 2.059946298599243\n",
      "step 2813: train loss 1.9788155555725098, val loss 2.0484533309936523\n",
      "step 2814: train loss 1.9861657619476318, val loss 2.051361322402954\n",
      "step 2815: train loss 1.9804877042770386, val loss 2.0572845935821533\n",
      "step 2816: train loss 1.9931141138076782, val loss 2.0542688369750977\n",
      "step 2817: train loss 1.9864716529846191, val loss 2.051846504211426\n",
      "step 2818: train loss 1.9953159093856812, val loss 2.0504703521728516\n",
      "step 2819: train loss 1.9989813566207886, val loss 2.0500333309173584\n",
      "step 2820: train loss 1.9923971891403198, val loss 2.058161735534668\n",
      "step 2821: train loss 1.9920026063919067, val loss 2.060908079147339\n",
      "step 2822: train loss 1.9903494119644165, val loss 2.042215585708618\n",
      "step 2823: train loss 1.986077904701233, val loss 2.0438392162323\n",
      "step 2824: train loss 1.9881103038787842, val loss 2.054837226867676\n",
      "step 2825: train loss 2.000323534011841, val loss 2.056624412536621\n",
      "step 2826: train loss 2.0027642250061035, val loss 2.052436590194702\n",
      "step 2827: train loss 1.9909069538116455, val loss 2.0643210411071777\n",
      "step 2828: train loss 1.9921015501022339, val loss 2.056896924972534\n",
      "step 2829: train loss 1.9887175559997559, val loss 2.0540413856506348\n",
      "step 2830: train loss 1.9906312227249146, val loss 2.050903558731079\n",
      "step 2831: train loss 1.9814704656600952, val loss 2.0538480281829834\n",
      "step 2832: train loss 1.9906463623046875, val loss 2.055971384048462\n",
      "step 2833: train loss 1.987828254699707, val loss 2.0562596321105957\n",
      "step 2834: train loss 1.9933701753616333, val loss 2.066676378250122\n",
      "step 2835: train loss 1.9906655550003052, val loss 2.0528478622436523\n",
      "step 2836: train loss 1.9940533638000488, val loss 2.0564124584198\n",
      "step 2837: train loss 1.9861429929733276, val loss 2.0574119091033936\n",
      "step 2838: train loss 1.985816478729248, val loss 2.0510060787200928\n",
      "step 2839: train loss 1.9930156469345093, val loss 2.0639302730560303\n",
      "step 2840: train loss 1.9897369146347046, val loss 2.055391311645508\n",
      "step 2841: train loss 1.991823673248291, val loss 2.047001838684082\n",
      "step 2842: train loss 1.9849557876586914, val loss 2.053119421005249\n",
      "step 2843: train loss 1.9749510288238525, val loss 2.056781053543091\n",
      "step 2844: train loss 1.9814538955688477, val loss 2.0525901317596436\n",
      "step 2845: train loss 1.9897116422653198, val loss 2.055783748626709\n",
      "step 2846: train loss 1.9842764139175415, val loss 2.057112693786621\n",
      "step 2847: train loss 1.9808850288391113, val loss 2.05820894241333\n",
      "step 2848: train loss 1.984124779701233, val loss 2.052116632461548\n",
      "step 2849: train loss 1.9787209033966064, val loss 2.0508646965026855\n",
      "step 2850: train loss 1.9668399095535278, val loss 2.0446078777313232\n",
      "step 2851: train loss 1.9781394004821777, val loss 2.0444955825805664\n",
      "step 2852: train loss 1.977095365524292, val loss 2.0519514083862305\n",
      "step 2853: train loss 1.9718395471572876, val loss 2.052804470062256\n",
      "step 2854: train loss 1.9763118028640747, val loss 2.0516414642333984\n",
      "step 2855: train loss 1.9750733375549316, val loss 2.0544192790985107\n",
      "step 2856: train loss 1.9788568019866943, val loss 2.0482518672943115\n",
      "step 2857: train loss 1.9844257831573486, val loss 2.050175189971924\n",
      "step 2858: train loss 1.979485273361206, val loss 2.0521209239959717\n",
      "step 2859: train loss 1.9774484634399414, val loss 2.047356367111206\n",
      "step 2860: train loss 1.9875553846359253, val loss 2.0503439903259277\n",
      "step 2861: train loss 1.979020595550537, val loss 2.0472257137298584\n",
      "step 2862: train loss 1.9846434593200684, val loss 2.0532453060150146\n",
      "step 2863: train loss 1.9838329553604126, val loss 2.057410478591919\n",
      "step 2864: train loss 1.9927737712860107, val loss 2.0685994625091553\n",
      "step 2865: train loss 1.9924075603485107, val loss 2.0565686225891113\n",
      "step 2866: train loss 1.9972838163375854, val loss 2.0636425018310547\n",
      "step 2867: train loss 2.0039806365966797, val loss 2.057561159133911\n",
      "step 2868: train loss 1.9867453575134277, val loss 2.050962448120117\n",
      "step 2869: train loss 1.980136752128601, val loss 2.043051242828369\n",
      "step 2870: train loss 1.980176568031311, val loss 2.0504324436187744\n",
      "step 2871: train loss 1.973899483680725, val loss 2.052929639816284\n",
      "step 2872: train loss 1.9692727327346802, val loss 2.0524208545684814\n",
      "step 2873: train loss 1.9784122705459595, val loss 2.0456573963165283\n",
      "step 2874: train loss 1.9793519973754883, val loss 2.0561351776123047\n",
      "step 2875: train loss 1.9801710844039917, val loss 2.0448434352874756\n",
      "step 2876: train loss 1.973881721496582, val loss 2.0521914958953857\n",
      "step 2877: train loss 1.9753785133361816, val loss 2.0507922172546387\n",
      "step 2878: train loss 1.9847593307495117, val loss 2.064004421234131\n",
      "step 2879: train loss 1.9883781671524048, val loss 2.060439348220825\n",
      "step 2880: train loss 1.9817352294921875, val loss 2.0562784671783447\n",
      "step 2881: train loss 1.9821226596832275, val loss 2.0520925521850586\n",
      "step 2882: train loss 1.9743767976760864, val loss 2.049541473388672\n",
      "step 2883: train loss 1.9817473888397217, val loss 2.052652359008789\n",
      "step 2884: train loss 1.979972243309021, val loss 2.059464454650879\n",
      "step 2885: train loss 1.9740523099899292, val loss 2.0477051734924316\n",
      "step 2886: train loss 1.9803574085235596, val loss 2.0423200130462646\n",
      "step 2887: train loss 1.9817836284637451, val loss 2.0462377071380615\n",
      "step 2888: train loss 1.984981894493103, val loss 2.044278621673584\n",
      "step 2889: train loss 1.9766294956207275, val loss 2.047184705734253\n",
      "step 2890: train loss 1.9722874164581299, val loss 2.0447378158569336\n",
      "step 2891: train loss 1.973232626914978, val loss 2.046633720397949\n",
      "step 2892: train loss 1.9735488891601562, val loss 2.0489885807037354\n",
      "step 2893: train loss 1.980538010597229, val loss 2.0553135871887207\n",
      "step 2894: train loss 1.9790986776351929, val loss 2.0666377544403076\n",
      "step 2895: train loss 1.9772878885269165, val loss 2.0478479862213135\n",
      "step 2896: train loss 1.970770239830017, val loss 2.0526375770568848\n",
      "step 2897: train loss 1.9854419231414795, val loss 2.045109748840332\n",
      "step 2898: train loss 1.9684773683547974, val loss 2.044245958328247\n",
      "step 2899: train loss 1.960737943649292, val loss 2.050020694732666\n",
      "step 2901: train loss 1.9785683155059814, val loss 2.0443363189697266\n",
      "step 2902: train loss 1.9714466333389282, val loss 2.0454764366149902\n",
      "step 2903: train loss 1.9686753749847412, val loss 2.052455425262451\n",
      "step 2904: train loss 1.969063401222229, val loss 2.0419247150421143\n",
      "step 2905: train loss 1.9668278694152832, val loss 2.04148006439209\n",
      "step 2906: train loss 1.9694228172302246, val loss 2.0485434532165527\n",
      "step 2907: train loss 1.972645878791809, val loss 2.048301935195923\n",
      "step 2908: train loss 1.9874961376190186, val loss 2.063142776489258\n",
      "step 2909: train loss 1.9766476154327393, val loss 2.042912483215332\n",
      "step 2910: train loss 1.970408320426941, val loss 2.062878131866455\n",
      "step 2911: train loss 1.9690027236938477, val loss 2.0500831604003906\n",
      "step 2912: train loss 1.9781631231307983, val loss 2.0380656719207764\n",
      "step 2913: train loss 1.9758691787719727, val loss 2.0490429401397705\n",
      "step 2914: train loss 1.9710452556610107, val loss 2.048121452331543\n",
      "step 2915: train loss 1.9531075954437256, val loss 2.0370073318481445\n",
      "step 2916: train loss 1.9746084213256836, val loss 2.0312328338623047\n",
      "step 2917: train loss 1.9694671630859375, val loss 2.0393881797790527\n",
      "step 2918: train loss 1.961604118347168, val loss 2.0294084548950195\n",
      "step 2919: train loss 1.970125675201416, val loss 2.0436670780181885\n",
      "step 2920: train loss 1.9668242931365967, val loss 2.0389645099639893\n",
      "step 2921: train loss 1.9807316064834595, val loss 2.0507547855377197\n",
      "step 2922: train loss 1.9795808792114258, val loss 2.048704147338867\n",
      "step 2923: train loss 1.96208918094635, val loss 2.0403592586517334\n",
      "step 2924: train loss 1.9643847942352295, val loss 2.039677381515503\n",
      "step 2925: train loss 1.9748930931091309, val loss 2.0416061878204346\n",
      "step 2926: train loss 1.9665416479110718, val loss 2.048062324523926\n",
      "step 2927: train loss 1.9688023328781128, val loss 2.0447378158569336\n",
      "step 2928: train loss 1.9726163148880005, val loss 2.052187919616699\n",
      "step 2929: train loss 1.9701555967330933, val loss 2.052659511566162\n",
      "step 2930: train loss 1.9724383354187012, val loss 2.0485427379608154\n",
      "step 2931: train loss 1.969240427017212, val loss 2.0480027198791504\n",
      "step 2932: train loss 1.961397647857666, val loss 2.0333640575408936\n",
      "step 2933: train loss 1.9716421365737915, val loss 2.0434281826019287\n",
      "step 2934: train loss 1.9658408164978027, val loss 2.0449347496032715\n",
      "step 2935: train loss 1.9647334814071655, val loss 2.052558660507202\n",
      "step 2936: train loss 1.9587079286575317, val loss 2.04878306388855\n",
      "step 2937: train loss 1.9604688882827759, val loss 2.046600341796875\n",
      "step 2938: train loss 1.9572844505310059, val loss 2.0497045516967773\n",
      "step 2939: train loss 1.9722028970718384, val loss 2.0429017543792725\n",
      "step 2940: train loss 1.9650803804397583, val loss 2.0414042472839355\n",
      "step 2941: train loss 1.970494031906128, val loss 2.0520057678222656\n",
      "step 2942: train loss 1.974484920501709, val loss 2.047245740890503\n",
      "step 2943: train loss 1.972448468208313, val loss 2.0487043857574463\n",
      "step 2944: train loss 1.9794836044311523, val loss 2.059492588043213\n",
      "step 2945: train loss 1.9716037511825562, val loss 2.051067590713501\n",
      "step 2946: train loss 1.9719228744506836, val loss 2.0386579036712646\n",
      "step 2947: train loss 1.9659440517425537, val loss 2.0411875247955322\n",
      "step 2948: train loss 1.9625476598739624, val loss 2.039316415786743\n",
      "step 2949: train loss 1.962092638015747, val loss 2.0511250495910645\n",
      "step 2950: train loss 1.9747099876403809, val loss 2.048429489135742\n",
      "step 2951: train loss 1.9684926271438599, val loss 2.046137809753418\n",
      "step 2952: train loss 1.9692885875701904, val loss 2.0418777465820312\n",
      "step 2953: train loss 1.9773160219192505, val loss 2.0354726314544678\n",
      "step 2954: train loss 1.9656869173049927, val loss 2.0465407371520996\n",
      "step 2955: train loss 1.9582704305648804, val loss 2.0408802032470703\n",
      "step 2956: train loss 1.9670578241348267, val loss 2.034097671508789\n",
      "step 2957: train loss 1.9670289754867554, val loss 2.0307533740997314\n",
      "step 2958: train loss 1.970272421836853, val loss 2.0412824153900146\n",
      "step 2959: train loss 1.9647362232208252, val loss 2.0394606590270996\n",
      "step 2960: train loss 1.9705190658569336, val loss 2.043764114379883\n",
      "step 2961: train loss 1.9593048095703125, val loss 2.041496992111206\n",
      "step 2962: train loss 1.958227276802063, val loss 2.039057731628418\n",
      "step 2963: train loss 1.9643086194992065, val loss 2.0471718311309814\n",
      "step 2964: train loss 1.9718893766403198, val loss 2.0516610145568848\n",
      "step 2965: train loss 1.975386619567871, val loss 2.039616346359253\n",
      "step 2966: train loss 1.9632164239883423, val loss 2.05037784576416\n",
      "step 2967: train loss 1.9569199085235596, val loss 2.0455520153045654\n",
      "step 2968: train loss 1.9599686861038208, val loss 2.0455102920532227\n",
      "step 2969: train loss 1.9601672887802124, val loss 2.0374834537506104\n",
      "step 2970: train loss 1.9623920917510986, val loss 2.041428327560425\n",
      "step 2971: train loss 1.9610803127288818, val loss 2.0407605171203613\n",
      "step 2972: train loss 1.960821270942688, val loss 2.037003517150879\n",
      "step 2973: train loss 1.9705106019973755, val loss 2.0453076362609863\n",
      "step 2974: train loss 1.9677969217300415, val loss 2.049077033996582\n",
      "step 2975: train loss 1.9717532396316528, val loss 2.045837879180908\n",
      "step 2976: train loss 1.9728237390518188, val loss 2.048165798187256\n",
      "step 2977: train loss 1.9752169847488403, val loss 2.0419466495513916\n",
      "step 2978: train loss 1.9749521017074585, val loss 2.040032386779785\n",
      "step 2979: train loss 1.9713408946990967, val loss 2.040087938308716\n",
      "step 2980: train loss 1.9715359210968018, val loss 2.05597186088562\n",
      "step 2981: train loss 1.9661266803741455, val loss 2.0543549060821533\n",
      "step 2982: train loss 1.9818896055221558, val loss 2.0415520668029785\n",
      "step 2983: train loss 1.9760953187942505, val loss 2.044292688369751\n",
      "step 2984: train loss 1.968085765838623, val loss 2.0397369861602783\n",
      "step 2985: train loss 1.9552624225616455, val loss 2.0526366233825684\n",
      "step 2986: train loss 1.968308448791504, val loss 2.0380115509033203\n",
      "step 2987: train loss 1.9677700996398926, val loss 2.0421302318573\n",
      "step 2988: train loss 1.9665850400924683, val loss 2.040158748626709\n",
      "step 2989: train loss 1.9749373197555542, val loss 2.0403988361358643\n",
      "step 2990: train loss 1.986997365951538, val loss 2.048846483230591\n",
      "step 2991: train loss 1.9664955139160156, val loss 2.047912359237671\n",
      "step 2992: train loss 1.9591383934020996, val loss 2.036485195159912\n",
      "step 2993: train loss 1.960591435432434, val loss 2.0424070358276367\n",
      "step 2994: train loss 1.9703401327133179, val loss 2.041302442550659\n",
      "step 2995: train loss 1.968052864074707, val loss 2.0576939582824707\n",
      "step 2996: train loss 1.965080738067627, val loss 2.058250665664673\n",
      "step 2997: train loss 1.9738988876342773, val loss 2.0427377223968506\n",
      "step 2998: train loss 1.955410361289978, val loss 2.0327887535095215\n",
      "step 2999: train loss 1.958164930343628, val loss 2.043879747390747\n",
      "step 3001: train loss 1.9778441190719604, val loss 2.0593597888946533\n",
      "step 3002: train loss 1.9605827331542969, val loss 2.0552942752838135\n",
      "step 3003: train loss 1.9689679145812988, val loss 2.054054021835327\n",
      "step 3004: train loss 1.958470106124878, val loss 2.0429065227508545\n",
      "step 3005: train loss 1.966119647026062, val loss 2.040189743041992\n",
      "step 3006: train loss 1.978714108467102, val loss 2.055295944213867\n",
      "step 3007: train loss 1.9699984788894653, val loss 2.0560836791992188\n",
      "step 3008: train loss 1.98311448097229, val loss 2.051509380340576\n",
      "step 3009: train loss 1.9789749383926392, val loss 2.0688278675079346\n",
      "step 3010: train loss 1.9657461643218994, val loss 2.0490243434906006\n",
      "step 3011: train loss 1.9601526260375977, val loss 2.04044508934021\n",
      "step 3012: train loss 1.9723522663116455, val loss 2.0577800273895264\n",
      "step 3013: train loss 1.9776860475540161, val loss 2.0515432357788086\n",
      "step 3014: train loss 1.9760061502456665, val loss 2.0594260692596436\n",
      "step 3015: train loss 1.9745326042175293, val loss 2.0548980236053467\n",
      "step 3016: train loss 1.9703428745269775, val loss 2.050628185272217\n",
      "step 3017: train loss 1.972631573677063, val loss 2.0459630489349365\n",
      "step 3018: train loss 1.971306562423706, val loss 2.035956621170044\n",
      "step 3019: train loss 1.9703130722045898, val loss 2.0424203872680664\n",
      "step 3020: train loss 1.9756721258163452, val loss 2.047515392303467\n",
      "step 3021: train loss 1.9692660570144653, val loss 2.0413596630096436\n",
      "step 3022: train loss 1.978492021560669, val loss 2.049515962600708\n",
      "step 3023: train loss 1.967913031578064, val loss 2.051414728164673\n",
      "step 3024: train loss 1.9661444425582886, val loss 2.039945363998413\n",
      "step 3025: train loss 1.9619220495224, val loss 2.0306973457336426\n",
      "step 3026: train loss 1.9564895629882812, val loss 2.048409938812256\n",
      "step 3027: train loss 1.9590595960617065, val loss 2.0356626510620117\n",
      "step 3028: train loss 1.9575647115707397, val loss 2.036646842956543\n",
      "step 3029: train loss 1.9607855081558228, val loss 2.0309457778930664\n",
      "step 3030: train loss 1.9530504941940308, val loss 2.033064365386963\n",
      "step 3031: train loss 1.9510999917984009, val loss 2.0256402492523193\n",
      "step 3032: train loss 1.9640402793884277, val loss 2.04577898979187\n",
      "step 3033: train loss 1.962581753730774, val loss 2.039527654647827\n",
      "step 3034: train loss 1.9617425203323364, val loss 2.043185234069824\n",
      "step 3035: train loss 1.963741421699524, val loss 2.039585828781128\n",
      "step 3036: train loss 1.9562933444976807, val loss 2.036374807357788\n",
      "step 3037: train loss 1.956861138343811, val loss 2.0420708656311035\n",
      "step 3038: train loss 1.9589616060256958, val loss 2.0385019779205322\n",
      "step 3039: train loss 1.9534159898757935, val loss 2.049370288848877\n",
      "step 3040: train loss 1.9572395086288452, val loss 2.047668218612671\n",
      "step 3041: train loss 1.9567979574203491, val loss 2.0447418689727783\n",
      "step 3042: train loss 1.9526488780975342, val loss 2.0280449390411377\n",
      "step 3043: train loss 1.9548717737197876, val loss 2.0359840393066406\n",
      "step 3044: train loss 1.9616656303405762, val loss 2.0359115600585938\n",
      "step 3045: train loss 1.9603136777877808, val loss 2.032470226287842\n",
      "step 3046: train loss 1.9585504531860352, val loss 2.030653476715088\n",
      "step 3047: train loss 1.966102123260498, val loss 2.0347249507904053\n",
      "step 3048: train loss 1.9653964042663574, val loss 2.031919479370117\n",
      "step 3049: train loss 1.9638954401016235, val loss 2.0408928394317627\n",
      "step 3050: train loss 1.9671446084976196, val loss 2.0357303619384766\n",
      "step 3051: train loss 1.9737857580184937, val loss 2.038362979888916\n",
      "step 3052: train loss 1.9692226648330688, val loss 2.047531843185425\n",
      "step 3053: train loss 1.9710394144058228, val loss 2.0354321002960205\n",
      "step 3054: train loss 1.9672682285308838, val loss 2.041147470474243\n",
      "step 3055: train loss 1.9625474214553833, val loss 2.0353386402130127\n",
      "step 3056: train loss 1.9638524055480957, val loss 2.037064790725708\n",
      "step 3057: train loss 1.9581996202468872, val loss 2.0395498275756836\n",
      "step 3058: train loss 1.967010498046875, val loss 2.0413224697113037\n",
      "step 3059: train loss 1.960780382156372, val loss 2.0396816730499268\n",
      "step 3060: train loss 1.947499394416809, val loss 2.0374786853790283\n",
      "step 3061: train loss 1.9621773958206177, val loss 2.035794258117676\n",
      "step 3062: train loss 1.962141990661621, val loss 2.0350985527038574\n",
      "step 3063: train loss 1.9631344079971313, val loss 2.0304758548736572\n",
      "step 3064: train loss 1.9622125625610352, val loss 2.0345826148986816\n",
      "step 3065: train loss 1.963492751121521, val loss 2.032465934753418\n",
      "step 3066: train loss 1.9596078395843506, val loss 2.0421974658966064\n",
      "step 3067: train loss 1.9560937881469727, val loss 2.040022850036621\n",
      "step 3068: train loss 1.959539532661438, val loss 2.0403730869293213\n",
      "step 3069: train loss 1.9630911350250244, val loss 2.0433220863342285\n",
      "step 3070: train loss 1.960206151008606, val loss 2.0302624702453613\n",
      "step 3071: train loss 1.9629310369491577, val loss 2.025156259536743\n",
      "step 3072: train loss 1.964678168296814, val loss 2.038674831390381\n",
      "step 3073: train loss 1.9620569944381714, val loss 2.0318591594696045\n",
      "step 3074: train loss 1.9625588655471802, val loss 2.0437777042388916\n",
      "step 3075: train loss 1.9695231914520264, val loss 2.040647029876709\n",
      "step 3076: train loss 1.9599366188049316, val loss 2.040853977203369\n",
      "step 3077: train loss 1.9632405042648315, val loss 2.0341403484344482\n",
      "step 3078: train loss 1.956993818283081, val loss 2.032421827316284\n",
      "step 3079: train loss 1.9553555250167847, val loss 2.033339262008667\n",
      "step 3080: train loss 1.9495450258255005, val loss 2.0291762351989746\n",
      "step 3081: train loss 1.955405592918396, val loss 2.030365467071533\n",
      "step 3082: train loss 1.9488400220870972, val loss 2.029539108276367\n",
      "step 3083: train loss 1.9592891931533813, val loss 2.037652015686035\n",
      "step 3084: train loss 1.9579335451126099, val loss 2.033952236175537\n",
      "step 3085: train loss 1.9549424648284912, val loss 2.034651041030884\n",
      "step 3086: train loss 1.9600809812545776, val loss 2.0327513217926025\n",
      "step 3087: train loss 1.9492563009262085, val loss 2.0331945419311523\n",
      "step 3088: train loss 1.9559423923492432, val loss 2.023573875427246\n",
      "step 3089: train loss 1.9544752836227417, val loss 2.0349228382110596\n",
      "step 3090: train loss 1.9615952968597412, val loss 2.036731481552124\n",
      "step 3091: train loss 1.9487826824188232, val loss 2.034842014312744\n",
      "step 3092: train loss 1.9403530359268188, val loss 2.043964147567749\n",
      "step 3093: train loss 1.9469836950302124, val loss 2.029170274734497\n",
      "step 3094: train loss 1.9444383382797241, val loss 2.02980899810791\n",
      "step 3095: train loss 1.9490107297897339, val loss 2.0245745182037354\n",
      "step 3096: train loss 1.9447282552719116, val loss 2.0347630977630615\n",
      "step 3097: train loss 1.950679898262024, val loss 2.0316386222839355\n",
      "step 3098: train loss 1.9412198066711426, val loss 2.0296971797943115\n",
      "step 3099: train loss 1.9562150239944458, val loss 2.0268921852111816\n",
      "step 3101: train loss 1.949918270111084, val loss 2.0308446884155273\n",
      "step 3102: train loss 1.9551727771759033, val loss 2.0367934703826904\n",
      "step 3103: train loss 1.9560548067092896, val loss 2.037365674972534\n",
      "step 3104: train loss 1.9458624124526978, val loss 2.038087844848633\n",
      "step 3105: train loss 1.960170030593872, val loss 2.03245210647583\n",
      "step 3106: train loss 1.9608768224716187, val loss 2.041037082672119\n",
      "step 3107: train loss 1.9465911388397217, val loss 2.045351028442383\n",
      "step 3108: train loss 1.9567583799362183, val loss 2.0454859733581543\n",
      "step 3109: train loss 1.9461613893508911, val loss 2.0388083457946777\n",
      "step 3110: train loss 1.950858473777771, val loss 2.0313124656677246\n",
      "step 3111: train loss 1.9395127296447754, val loss 2.0343117713928223\n",
      "step 3112: train loss 1.9477837085723877, val loss 2.031209945678711\n",
      "step 3113: train loss 1.9464248418807983, val loss 2.0344018936157227\n",
      "step 3114: train loss 1.9539636373519897, val loss 2.033503532409668\n",
      "step 3115: train loss 1.9472107887268066, val loss 2.0301613807678223\n",
      "step 3116: train loss 1.950345754623413, val loss 2.027949333190918\n",
      "step 3117: train loss 1.9607009887695312, val loss 2.0385868549346924\n",
      "step 3118: train loss 1.952622652053833, val loss 2.036019802093506\n",
      "step 3119: train loss 1.9542979001998901, val loss 2.037755012512207\n",
      "step 3120: train loss 1.9533801078796387, val loss 2.042501926422119\n",
      "step 3121: train loss 1.959748387336731, val loss 2.0467145442962646\n",
      "step 3122: train loss 1.9580025672912598, val loss 2.035174608230591\n",
      "step 3123: train loss 1.9544570446014404, val loss 2.049992561340332\n",
      "step 3124: train loss 1.9588323831558228, val loss 2.0407748222351074\n",
      "step 3125: train loss 1.960573434829712, val loss 2.045382022857666\n",
      "step 3126: train loss 1.9465036392211914, val loss 2.0289132595062256\n",
      "step 3127: train loss 1.9503387212753296, val loss 2.029409408569336\n",
      "step 3128: train loss 1.949922800064087, val loss 2.0284197330474854\n",
      "step 3129: train loss 1.9537043571472168, val loss 2.0290095806121826\n",
      "step 3130: train loss 1.9521763324737549, val loss 2.030299186706543\n",
      "step 3131: train loss 1.959049105644226, val loss 2.0381505489349365\n",
      "step 3132: train loss 1.9585238695144653, val loss 2.0425000190734863\n",
      "step 3133: train loss 1.9575812816619873, val loss 2.022125244140625\n",
      "step 3134: train loss 1.957434058189392, val loss 2.034489631652832\n",
      "step 3135: train loss 1.950045108795166, val loss 2.020826816558838\n",
      "step 3136: train loss 1.9490503072738647, val loss 2.030758857727051\n",
      "step 3137: train loss 1.9447391033172607, val loss 2.0354275703430176\n",
      "step 3138: train loss 1.9541113376617432, val loss 2.0354528427124023\n",
      "step 3139: train loss 1.953374981880188, val loss 2.0361876487731934\n",
      "step 3140: train loss 1.9462189674377441, val loss 2.0317459106445312\n",
      "step 3141: train loss 1.9420623779296875, val loss 2.0326731204986572\n",
      "step 3142: train loss 1.9502133131027222, val loss 2.039520502090454\n",
      "step 3143: train loss 1.9414023160934448, val loss 2.040583610534668\n",
      "step 3144: train loss 1.948519229888916, val loss 2.0290327072143555\n",
      "step 3145: train loss 1.9480198621749878, val loss 2.031654119491577\n",
      "step 3146: train loss 1.947543978691101, val loss 2.026888847351074\n",
      "step 3147: train loss 1.9557745456695557, val loss 2.0244531631469727\n",
      "step 3148: train loss 1.9432308673858643, val loss 2.0325374603271484\n",
      "step 3149: train loss 1.944207787513733, val loss 2.036921262741089\n",
      "step 3150: train loss 1.9489433765411377, val loss 2.042287826538086\n",
      "step 3151: train loss 1.9509838819503784, val loss 2.0367653369903564\n",
      "step 3152: train loss 1.9480029344558716, val loss 2.029350519180298\n",
      "step 3153: train loss 1.955680251121521, val loss 2.029388666152954\n",
      "step 3154: train loss 1.9496456384658813, val loss 2.034174919128418\n",
      "step 3155: train loss 1.9504281282424927, val loss 2.028264284133911\n",
      "step 3156: train loss 1.9504128694534302, val loss 2.0278818607330322\n",
      "step 3157: train loss 1.9482861757278442, val loss 2.0406687259674072\n",
      "step 3158: train loss 1.9465842247009277, val loss 2.02775502204895\n",
      "step 3159: train loss 1.939477801322937, val loss 2.0328621864318848\n",
      "step 3160: train loss 1.951072335243225, val loss 2.0410430431365967\n",
      "step 3161: train loss 1.950419306755066, val loss 2.0326528549194336\n",
      "step 3162: train loss 1.957746982574463, val loss 2.036821126937866\n",
      "step 3163: train loss 1.9505224227905273, val loss 2.0267553329467773\n",
      "step 3164: train loss 1.9489026069641113, val loss 2.031400203704834\n",
      "step 3165: train loss 1.9476889371871948, val loss 2.035691261291504\n",
      "step 3166: train loss 1.9554874897003174, val loss 2.0294442176818848\n",
      "step 3167: train loss 1.9488708972930908, val loss 2.036682367324829\n",
      "step 3168: train loss 1.9501374959945679, val loss 2.03381609916687\n",
      "step 3169: train loss 1.9486274719238281, val loss 2.0348269939422607\n",
      "step 3170: train loss 1.9428832530975342, val loss 2.0231645107269287\n",
      "step 3171: train loss 1.9410651922225952, val loss 2.025315761566162\n",
      "step 3172: train loss 1.9435309171676636, val loss 2.031080484390259\n",
      "step 3173: train loss 1.9361900091171265, val loss 2.0325870513916016\n",
      "step 3174: train loss 1.9486101865768433, val loss 2.039008378982544\n",
      "step 3175: train loss 1.9509363174438477, val loss 2.0322346687316895\n",
      "step 3176: train loss 1.9491565227508545, val loss 2.035346031188965\n",
      "step 3177: train loss 1.9464874267578125, val loss 2.039547920227051\n",
      "step 3178: train loss 1.9489163160324097, val loss 2.040135383605957\n",
      "step 3179: train loss 1.9552656412124634, val loss 2.0307037830352783\n",
      "step 3180: train loss 1.955229640007019, val loss 2.037665367126465\n",
      "step 3181: train loss 1.9546412229537964, val loss 2.0326664447784424\n",
      "step 3182: train loss 1.9468950033187866, val loss 2.0364561080932617\n",
      "step 3183: train loss 1.9495773315429688, val loss 2.040987491607666\n",
      "step 3184: train loss 1.9410399198532104, val loss 2.0362935066223145\n",
      "step 3185: train loss 1.9454411268234253, val loss 2.0354785919189453\n",
      "step 3186: train loss 1.9433403015136719, val loss 2.015350580215454\n",
      "step 3187: train loss 1.942655324935913, val loss 2.0422415733337402\n",
      "step 3188: train loss 1.9479784965515137, val loss 2.0236704349517822\n",
      "step 3189: train loss 1.94515061378479, val loss 2.02694034576416\n",
      "step 3190: train loss 1.9457167387008667, val loss 2.034416437149048\n",
      "step 3191: train loss 1.9524034261703491, val loss 2.043787956237793\n",
      "step 3192: train loss 1.9474120140075684, val loss 2.0332143306732178\n",
      "step 3193: train loss 1.952675223350525, val loss 2.0381722450256348\n",
      "step 3194: train loss 1.9463272094726562, val loss 2.0420918464660645\n",
      "step 3195: train loss 1.9520370960235596, val loss 2.0352847576141357\n",
      "step 3196: train loss 1.9588509798049927, val loss 2.035025119781494\n",
      "step 3197: train loss 1.9468202590942383, val loss 2.02803373336792\n",
      "step 3198: train loss 1.9441261291503906, val loss 2.0374298095703125\n",
      "step 3199: train loss 1.9484946727752686, val loss 2.0274338722229004\n",
      "step 3201: train loss 1.951052188873291, val loss 2.021746873855591\n",
      "step 3202: train loss 1.9463618993759155, val loss 2.032433271408081\n",
      "step 3203: train loss 1.945559024810791, val loss 2.037076711654663\n",
      "step 3204: train loss 1.9400324821472168, val loss 2.0253708362579346\n",
      "step 3205: train loss 1.938442349433899, val loss 2.0298585891723633\n",
      "step 3206: train loss 1.936655879020691, val loss 2.026482582092285\n",
      "step 3207: train loss 1.9391388893127441, val loss 2.0242295265197754\n",
      "step 3208: train loss 1.9429199695587158, val loss 2.029926061630249\n",
      "step 3209: train loss 1.9419951438903809, val loss 2.0316946506500244\n",
      "step 3210: train loss 1.9402108192443848, val loss 2.0268163681030273\n",
      "step 3211: train loss 1.9435760974884033, val loss 2.0438132286071777\n",
      "step 3212: train loss 1.936282992362976, val loss 2.0231871604919434\n",
      "step 3213: train loss 1.942671775817871, val loss 2.028007745742798\n",
      "step 3214: train loss 1.9380316734313965, val loss 2.0309200286865234\n",
      "step 3215: train loss 1.940903663635254, val loss 2.021510124206543\n",
      "step 3216: train loss 1.9380860328674316, val loss 2.034201145172119\n",
      "step 3217: train loss 1.9430170059204102, val loss 2.0161280632019043\n",
      "step 3218: train loss 1.9516892433166504, val loss 2.027045965194702\n",
      "step 3219: train loss 1.949004054069519, val loss 2.0321340560913086\n",
      "step 3220: train loss 1.9386579990386963, val loss 2.032360553741455\n",
      "step 3221: train loss 1.9469823837280273, val loss 2.0431580543518066\n",
      "step 3222: train loss 1.9396929740905762, val loss 2.035226345062256\n",
      "step 3223: train loss 1.9566340446472168, val loss 2.0452616214752197\n",
      "step 3224: train loss 1.9523762464523315, val loss 2.0370442867279053\n",
      "step 3225: train loss 1.9586230516433716, val loss 2.0446016788482666\n",
      "step 3226: train loss 1.9577527046203613, val loss 2.040591239929199\n",
      "step 3227: train loss 1.9551827907562256, val loss 2.042297601699829\n",
      "step 3228: train loss 1.9620016813278198, val loss 2.0349509716033936\n",
      "step 3229: train loss 1.9458051919937134, val loss 2.0315206050872803\n",
      "step 3230: train loss 1.9427436590194702, val loss 2.024585485458374\n",
      "step 3231: train loss 1.9471633434295654, val loss 2.0266737937927246\n",
      "step 3232: train loss 1.9423744678497314, val loss 2.0408835411071777\n",
      "step 3233: train loss 1.9540961980819702, val loss 2.032553195953369\n",
      "step 3234: train loss 1.9550390243530273, val loss 2.0401787757873535\n",
      "step 3235: train loss 1.9483410120010376, val loss 2.0355236530303955\n",
      "step 3236: train loss 1.9393786191940308, val loss 2.036431312561035\n",
      "step 3237: train loss 1.9400770664215088, val loss 2.0231409072875977\n",
      "step 3238: train loss 1.9386006593704224, val loss 2.0177767276763916\n",
      "step 3239: train loss 1.9457529783248901, val loss 2.0284316539764404\n",
      "step 3240: train loss 1.943196415901184, val loss 2.0259580612182617\n",
      "step 3241: train loss 1.9537653923034668, val loss 2.0393526554107666\n",
      "step 3242: train loss 1.9491957426071167, val loss 2.040553331375122\n",
      "step 3243: train loss 1.944503903388977, val loss 2.030080795288086\n",
      "step 3244: train loss 1.9445418119430542, val loss 2.0322928428649902\n",
      "step 3245: train loss 1.9397186040878296, val loss 2.0334832668304443\n",
      "step 3246: train loss 1.947977900505066, val loss 2.027001142501831\n",
      "step 3247: train loss 1.9503586292266846, val loss 2.030366897583008\n",
      "step 3248: train loss 1.9535174369812012, val loss 2.0325117111206055\n",
      "step 3249: train loss 1.9364663362503052, val loss 2.025797128677368\n",
      "step 3250: train loss 1.9464596509933472, val loss 2.0339391231536865\n",
      "step 3251: train loss 1.943006157875061, val loss 2.019796133041382\n",
      "step 3252: train loss 1.95424222946167, val loss 2.038989305496216\n",
      "step 3253: train loss 1.9395339488983154, val loss 2.0203378200531006\n",
      "step 3254: train loss 1.9338340759277344, val loss 2.020382881164551\n",
      "step 3255: train loss 1.9386249780654907, val loss 2.012033700942993\n",
      "step 3256: train loss 1.9408440589904785, val loss 2.0244264602661133\n",
      "step 3257: train loss 1.9457130432128906, val loss 2.017275333404541\n",
      "step 3258: train loss 1.9552001953125, val loss 2.0276403427124023\n",
      "step 3259: train loss 1.9507360458374023, val loss 2.0245888233184814\n",
      "step 3260: train loss 1.9501686096191406, val loss 2.0216588973999023\n",
      "step 3261: train loss 1.9428213834762573, val loss 2.029233932495117\n",
      "step 3262: train loss 1.9441663026809692, val loss 2.023409128189087\n",
      "step 3263: train loss 1.943494439125061, val loss 2.0189452171325684\n",
      "step 3264: train loss 1.934602975845337, val loss 2.0217511653900146\n",
      "step 3265: train loss 1.9449124336242676, val loss 2.0168683528900146\n",
      "step 3266: train loss 1.9403613805770874, val loss 2.022728204727173\n",
      "step 3267: train loss 1.9399436712265015, val loss 2.0260815620422363\n",
      "step 3268: train loss 1.93831205368042, val loss 2.0324349403381348\n",
      "step 3269: train loss 1.9364742040634155, val loss 2.0313515663146973\n",
      "step 3270: train loss 1.9350727796554565, val loss 2.021644353866577\n",
      "step 3271: train loss 1.9320820569992065, val loss 2.0281636714935303\n",
      "step 3272: train loss 1.9350167512893677, val loss 2.0180604457855225\n",
      "step 3273: train loss 1.9281367063522339, val loss 2.028470993041992\n",
      "step 3274: train loss 1.9386836290359497, val loss 2.0300562381744385\n",
      "step 3275: train loss 1.9428799152374268, val loss 2.0261330604553223\n",
      "step 3276: train loss 1.9486202001571655, val loss 2.0244808197021484\n",
      "step 3277: train loss 1.9554507732391357, val loss 2.032989025115967\n",
      "step 3278: train loss 1.937130331993103, val loss 2.031512498855591\n",
      "step 3279: train loss 1.9375391006469727, val loss 2.0372495651245117\n",
      "step 3280: train loss 1.9471203088760376, val loss 2.033726692199707\n",
      "step 3281: train loss 1.9380630254745483, val loss 2.0317468643188477\n",
      "step 3282: train loss 1.9371542930603027, val loss 2.0240705013275146\n",
      "step 3283: train loss 1.9365379810333252, val loss 2.0178229808807373\n",
      "step 3284: train loss 1.9416557550430298, val loss 2.0223472118377686\n",
      "step 3285: train loss 1.9362494945526123, val loss 2.027238607406616\n",
      "step 3286: train loss 1.9387282133102417, val loss 2.029634952545166\n",
      "step 3287: train loss 1.9499658346176147, val loss 2.0350453853607178\n",
      "step 3288: train loss 1.9488590955734253, val loss 2.0314064025878906\n",
      "step 3289: train loss 1.9434186220169067, val loss 2.029721260070801\n",
      "step 3290: train loss 1.9500224590301514, val loss 2.0294177532196045\n",
      "step 3291: train loss 1.9474740028381348, val loss 2.0290939807891846\n",
      "step 3292: train loss 1.9374284744262695, val loss 2.0328004360198975\n",
      "step 3293: train loss 1.9438914060592651, val loss 2.035337209701538\n",
      "step 3294: train loss 1.9506521224975586, val loss 2.0243592262268066\n",
      "step 3295: train loss 1.9471423625946045, val loss 2.03800892829895\n",
      "step 3296: train loss 1.960872769355774, val loss 2.0169363021850586\n",
      "step 3297: train loss 1.9408663511276245, val loss 2.0213115215301514\n",
      "step 3298: train loss 1.9365729093551636, val loss 2.026033401489258\n",
      "step 3299: train loss 1.9255280494689941, val loss 2.0291225910186768\n",
      "step 3301: train loss 1.9445499181747437, val loss 2.021573543548584\n",
      "step 3302: train loss 1.9369562864303589, val loss 2.027595281600952\n",
      "step 3303: train loss 1.9327895641326904, val loss 2.0338408946990967\n",
      "step 3304: train loss 1.9404832124710083, val loss 2.040649652481079\n",
      "step 3305: train loss 1.9369490146636963, val loss 2.0239152908325195\n",
      "step 3306: train loss 1.9305598735809326, val loss 2.024693489074707\n",
      "step 3307: train loss 1.9394192695617676, val loss 2.0252296924591064\n",
      "step 3308: train loss 1.9281038045883179, val loss 2.0216174125671387\n",
      "step 3309: train loss 1.933967113494873, val loss 2.027611255645752\n",
      "step 3310: train loss 1.9230540990829468, val loss 2.0202720165252686\n",
      "step 3311: train loss 1.9301154613494873, val loss 2.021358013153076\n",
      "step 3312: train loss 1.9325151443481445, val loss 2.020432949066162\n",
      "step 3313: train loss 1.9297107458114624, val loss 2.0133893489837646\n",
      "step 3314: train loss 1.9310429096221924, val loss 2.015915870666504\n",
      "step 3315: train loss 1.9297715425491333, val loss 2.017136335372925\n",
      "step 3316: train loss 1.9341278076171875, val loss 2.011918067932129\n",
      "step 3317: train loss 1.92515230178833, val loss 2.010503053665161\n",
      "step 3318: train loss 1.922629952430725, val loss 2.0116477012634277\n",
      "step 3319: train loss 1.9228283166885376, val loss 2.020787000656128\n",
      "step 3320: train loss 1.9375014305114746, val loss 2.024129867553711\n",
      "step 3321: train loss 1.9350688457489014, val loss 2.021739959716797\n",
      "step 3322: train loss 1.938625693321228, val loss 2.0184476375579834\n",
      "step 3323: train loss 1.9324253797531128, val loss 2.021289110183716\n",
      "step 3324: train loss 1.9260929822921753, val loss 2.021362781524658\n",
      "step 3325: train loss 1.932706356048584, val loss 2.0121877193450928\n",
      "step 3326: train loss 1.927697777748108, val loss 2.0214791297912598\n",
      "step 3327: train loss 1.9457800388336182, val loss 2.0218381881713867\n",
      "step 3328: train loss 1.9325904846191406, val loss 2.0218677520751953\n",
      "step 3329: train loss 1.930065631866455, val loss 2.0187530517578125\n",
      "step 3330: train loss 1.930883765220642, val loss 2.0174190998077393\n",
      "step 3331: train loss 1.9297784566879272, val loss 2.0183866024017334\n",
      "step 3332: train loss 1.9268829822540283, val loss 2.020486831665039\n",
      "step 3333: train loss 1.9352771043777466, val loss 2.0151782035827637\n",
      "step 3334: train loss 1.9394906759262085, val loss 2.0195271968841553\n",
      "step 3335: train loss 1.9269267320632935, val loss 2.0269014835357666\n",
      "step 3336: train loss 1.9381440877914429, val loss 2.026381492614746\n",
      "step 3337: train loss 1.9438188076019287, val loss 2.0279364585876465\n",
      "step 3338: train loss 1.9371733665466309, val loss 2.0231125354766846\n",
      "step 3339: train loss 1.938232421875, val loss 2.0233922004699707\n",
      "step 3340: train loss 1.9369999170303345, val loss 2.0221974849700928\n",
      "step 3341: train loss 1.9355205297470093, val loss 2.031400680541992\n",
      "step 3342: train loss 1.9350422620773315, val loss 2.018193006515503\n",
      "step 3343: train loss 1.926676630973816, val loss 2.0146665573120117\n",
      "step 3344: train loss 1.9172555208206177, val loss 2.011756181716919\n",
      "step 3345: train loss 1.9281032085418701, val loss 2.008923292160034\n",
      "step 3346: train loss 1.9179996252059937, val loss 2.0117855072021484\n",
      "step 3347: train loss 1.9294058084487915, val loss 2.0119504928588867\n",
      "step 3348: train loss 1.91898512840271, val loss 2.008601188659668\n",
      "step 3349: train loss 1.929447054862976, val loss 2.012685775756836\n",
      "step 3350: train loss 1.923812985420227, val loss 2.0086915493011475\n",
      "step 3351: train loss 1.923977255821228, val loss 2.0172181129455566\n",
      "step 3352: train loss 1.9221957921981812, val loss 2.0073790550231934\n",
      "step 3353: train loss 1.9218776226043701, val loss 2.0174498558044434\n",
      "step 3354: train loss 1.9210113286972046, val loss 2.009220838546753\n",
      "step 3355: train loss 1.9184772968292236, val loss 2.024829149246216\n",
      "step 3356: train loss 1.9183224439620972, val loss 2.0120880603790283\n",
      "step 3357: train loss 1.920691728591919, val loss 2.014793634414673\n",
      "step 3358: train loss 1.923006296157837, val loss 2.0118143558502197\n",
      "step 3359: train loss 1.930822491645813, val loss 2.0196056365966797\n",
      "step 3360: train loss 1.9344595670700073, val loss 2.003990650177002\n",
      "step 3361: train loss 1.9253203868865967, val loss 2.0148048400878906\n",
      "step 3362: train loss 1.924509882926941, val loss 2.0139925479888916\n",
      "step 3363: train loss 1.9254847764968872, val loss 2.0104031562805176\n",
      "step 3364: train loss 1.9292494058609009, val loss 2.0115060806274414\n",
      "step 3365: train loss 1.9310815334320068, val loss 2.001533031463623\n",
      "step 3366: train loss 1.9301763772964478, val loss 2.018486738204956\n",
      "step 3367: train loss 1.9235825538635254, val loss 2.009934663772583\n",
      "step 3368: train loss 1.928178071975708, val loss 2.0056166648864746\n",
      "step 3369: train loss 1.9338927268981934, val loss 2.008268356323242\n",
      "step 3370: train loss 1.9273492097854614, val loss 2.0230672359466553\n",
      "step 3371: train loss 1.9299982786178589, val loss 2.0205657482147217\n",
      "step 3372: train loss 1.9430365562438965, val loss 2.0287575721740723\n",
      "step 3373: train loss 1.9385565519332886, val loss 2.022239923477173\n",
      "step 3374: train loss 1.9243396520614624, val loss 2.013474941253662\n",
      "step 3375: train loss 1.9195884466171265, val loss 2.0192208290100098\n",
      "step 3376: train loss 1.9263004064559937, val loss 2.0062665939331055\n",
      "step 3377: train loss 1.9325894117355347, val loss 2.0145795345306396\n",
      "step 3378: train loss 1.9304081201553345, val loss 2.024768590927124\n",
      "step 3379: train loss 1.9369763135910034, val loss 2.0259287357330322\n",
      "step 3380: train loss 1.9339159727096558, val loss 2.015777349472046\n",
      "step 3381: train loss 1.9200775623321533, val loss 2.0072784423828125\n",
      "step 3382: train loss 1.9236657619476318, val loss 2.020843505859375\n",
      "step 3383: train loss 1.9317572116851807, val loss 2.0232372283935547\n",
      "step 3384: train loss 1.9352974891662598, val loss 2.0237128734588623\n",
      "step 3385: train loss 1.938459873199463, val loss 2.0367214679718018\n",
      "step 3386: train loss 1.9361213445663452, val loss 2.0306131839752197\n",
      "step 3387: train loss 1.9344311952590942, val loss 2.027646541595459\n",
      "step 3388: train loss 1.9360688924789429, val loss 2.0209336280822754\n",
      "step 3389: train loss 1.930716872215271, val loss 2.012702703475952\n",
      "step 3390: train loss 1.9352275133132935, val loss 2.019315242767334\n",
      "step 3391: train loss 1.9293615818023682, val loss 2.0302281379699707\n",
      "step 3392: train loss 1.941745400428772, val loss 2.0243704319000244\n",
      "step 3393: train loss 1.934096097946167, val loss 2.0198283195495605\n",
      "step 3394: train loss 1.94124436378479, val loss 2.0162525177001953\n",
      "step 3395: train loss 1.9371920824050903, val loss 2.019824743270874\n",
      "step 3396: train loss 1.9299070835113525, val loss 2.0150563716888428\n",
      "step 3397: train loss 1.9297503232955933, val loss 2.003757953643799\n",
      "step 3398: train loss 1.9316542148590088, val loss 2.0176918506622314\n",
      "step 3399: train loss 1.926724910736084, val loss 2.014132022857666\n",
      "step 3401: train loss 1.9310592412948608, val loss 2.010929822921753\n",
      "step 3402: train loss 1.9217605590820312, val loss 2.0218849182128906\n",
      "step 3403: train loss 1.9348469972610474, val loss 2.0138978958129883\n",
      "step 3404: train loss 1.9293137788772583, val loss 2.012542724609375\n",
      "step 3405: train loss 1.9332938194274902, val loss 2.016106605529785\n",
      "step 3406: train loss 1.9255768060684204, val loss 2.007110834121704\n",
      "step 3407: train loss 1.930639386177063, val loss 2.01996111869812\n",
      "step 3408: train loss 1.9228471517562866, val loss 2.0154378414154053\n",
      "step 3409: train loss 1.923454761505127, val loss 2.016677141189575\n",
      "step 3410: train loss 1.9239037036895752, val loss 2.011526584625244\n",
      "step 3411: train loss 1.9260996580123901, val loss 2.0161314010620117\n",
      "step 3412: train loss 1.9139765501022339, val loss 2.0041513442993164\n",
      "step 3413: train loss 1.9233797788619995, val loss 2.015165328979492\n",
      "step 3414: train loss 1.9251186847686768, val loss 2.004878282546997\n",
      "step 3415: train loss 1.9294712543487549, val loss 2.010918378829956\n",
      "step 3416: train loss 1.9283463954925537, val loss 2.0045440196990967\n",
      "step 3417: train loss 1.9307562112808228, val loss 2.0037875175476074\n",
      "step 3418: train loss 1.9231878519058228, val loss 2.024346351623535\n",
      "step 3419: train loss 1.9319357872009277, val loss 2.012528896331787\n",
      "step 3420: train loss 1.9319431781768799, val loss 2.010741949081421\n",
      "step 3421: train loss 1.925492286682129, val loss 2.006582260131836\n",
      "step 3422: train loss 1.9275779724121094, val loss 2.002358913421631\n",
      "step 3423: train loss 1.9318969249725342, val loss 2.0036325454711914\n",
      "step 3424: train loss 1.92220139503479, val loss 2.0186922550201416\n",
      "step 3425: train loss 1.922284722328186, val loss 2.0154497623443604\n",
      "step 3426: train loss 1.9190843105316162, val loss 2.010117769241333\n",
      "step 3427: train loss 1.9244760274887085, val loss 2.016986608505249\n",
      "step 3428: train loss 1.9180488586425781, val loss 2.0112850666046143\n",
      "step 3429: train loss 1.9229730367660522, val loss 2.0237085819244385\n",
      "step 3430: train loss 1.9315201044082642, val loss 2.0201103687286377\n",
      "step 3431: train loss 1.9307774305343628, val loss 2.0106217861175537\n",
      "step 3432: train loss 1.9183109998703003, val loss 2.0132479667663574\n",
      "step 3433: train loss 1.9258630275726318, val loss 2.014756202697754\n",
      "step 3434: train loss 1.9208701848983765, val loss 2.0204272270202637\n",
      "step 3435: train loss 1.9179376363754272, val loss 2.014927864074707\n",
      "step 3436: train loss 1.9253289699554443, val loss 2.009369373321533\n",
      "step 3437: train loss 1.9217615127563477, val loss 2.01717209815979\n",
      "step 3438: train loss 1.9250998497009277, val loss 2.0132553577423096\n",
      "step 3439: train loss 1.9199403524398804, val loss 2.0097005367279053\n",
      "step 3440: train loss 1.922997236251831, val loss 2.0102410316467285\n",
      "step 3441: train loss 1.9187982082366943, val loss 2.016310214996338\n",
      "step 3442: train loss 1.9252392053604126, val loss 2.0215888023376465\n",
      "step 3443: train loss 1.9210882186889648, val loss 2.00807523727417\n",
      "step 3444: train loss 1.9198132753372192, val loss 2.0183379650115967\n",
      "step 3445: train loss 1.925972580909729, val loss 2.0207080841064453\n",
      "step 3446: train loss 1.9200085401535034, val loss 2.014294385910034\n",
      "step 3447: train loss 1.9208062887191772, val loss 2.015085458755493\n",
      "step 3448: train loss 1.9310094118118286, val loss 2.0154778957366943\n",
      "step 3449: train loss 1.920328974723816, val loss 2.0136823654174805\n",
      "step 3450: train loss 1.92814302444458, val loss 2.0220916271209717\n",
      "step 3451: train loss 1.9324876070022583, val loss 2.0188894271850586\n",
      "step 3452: train loss 1.9300720691680908, val loss 2.018141269683838\n",
      "step 3453: train loss 1.9288870096206665, val loss 2.021240711212158\n",
      "step 3454: train loss 1.9245373010635376, val loss 2.023388147354126\n",
      "step 3455: train loss 1.9200061559677124, val loss 2.0100748538970947\n",
      "step 3456: train loss 1.9233494997024536, val loss 2.0100338459014893\n",
      "step 3457: train loss 1.9259934425354004, val loss 2.013726234436035\n",
      "step 3458: train loss 1.9275540113449097, val loss 2.0161588191986084\n",
      "step 3459: train loss 1.9270800352096558, val loss 2.021247625350952\n",
      "step 3460: train loss 1.9271214008331299, val loss 2.019066572189331\n",
      "step 3461: train loss 1.933201789855957, val loss 2.017374277114868\n",
      "step 3462: train loss 1.9221882820129395, val loss 2.0184879302978516\n",
      "step 3463: train loss 1.9266167879104614, val loss 2.015625\n",
      "step 3464: train loss 1.922479271888733, val loss 2.0155792236328125\n",
      "step 3465: train loss 1.919327735900879, val loss 2.01387882232666\n",
      "step 3466: train loss 1.9160264730453491, val loss 2.0326406955718994\n",
      "step 3467: train loss 1.91758394241333, val loss 2.0152976512908936\n",
      "step 3468: train loss 1.9183353185653687, val loss 2.0176734924316406\n",
      "step 3469: train loss 1.9354488849639893, val loss 2.021695137023926\n",
      "step 3470: train loss 1.9338786602020264, val loss 2.0152993202209473\n",
      "step 3471: train loss 1.918336033821106, val loss 1.9980905055999756\n",
      "step 3472: train loss 1.9075764417648315, val loss 2.0115816593170166\n",
      "step 3473: train loss 1.9213424921035767, val loss 2.009742021560669\n",
      "step 3474: train loss 1.9283349514007568, val loss 2.0039422512054443\n",
      "step 3475: train loss 1.9232243299484253, val loss 2.0201070308685303\n",
      "step 3476: train loss 1.9235731363296509, val loss 2.028538227081299\n",
      "step 3477: train loss 1.9261670112609863, val loss 2.015626907348633\n",
      "step 3478: train loss 1.9173637628555298, val loss 2.0041136741638184\n",
      "step 3479: train loss 1.9127204418182373, val loss 2.003736972808838\n",
      "step 3480: train loss 1.9188988208770752, val loss 2.0047218799591064\n",
      "step 3481: train loss 1.9186378717422485, val loss 2.0080268383026123\n",
      "step 3482: train loss 1.9193775653839111, val loss 1.995539903640747\n",
      "step 3483: train loss 1.9251339435577393, val loss 2.003756284713745\n",
      "step 3484: train loss 1.913723349571228, val loss 2.010131597518921\n",
      "step 3485: train loss 1.916764497756958, val loss 1.9989944696426392\n",
      "step 3486: train loss 1.9096759557724, val loss 1.9902247190475464\n",
      "step 3487: train loss 1.9156192541122437, val loss 1.995832920074463\n",
      "step 3488: train loss 1.9289506673812866, val loss 2.0061779022216797\n",
      "step 3489: train loss 1.9234188795089722, val loss 2.0129499435424805\n",
      "step 3490: train loss 1.9235550165176392, val loss 2.0110373497009277\n",
      "step 3491: train loss 1.9190808534622192, val loss 2.007023572921753\n",
      "step 3492: train loss 1.9249770641326904, val loss 2.00765323638916\n",
      "step 3493: train loss 1.9259960651397705, val loss 2.0078911781311035\n",
      "step 3494: train loss 1.9356564283370972, val loss 2.005708694458008\n",
      "step 3495: train loss 1.9217584133148193, val loss 2.0091402530670166\n",
      "step 3496: train loss 1.9177114963531494, val loss 2.0093867778778076\n",
      "step 3497: train loss 1.9269607067108154, val loss 2.0059850215911865\n",
      "step 3498: train loss 1.9154717922210693, val loss 2.007510185241699\n",
      "step 3499: train loss 1.9208346605300903, val loss 2.0175347328186035\n",
      "step 3501: train loss 1.9072959423065186, val loss 2.007793664932251\n",
      "step 3502: train loss 1.9113351106643677, val loss 2.0107831954956055\n",
      "step 3503: train loss 1.9271283149719238, val loss 2.016672372817993\n",
      "step 3504: train loss 1.9179478883743286, val loss 2.016519546508789\n",
      "step 3505: train loss 1.9174014329910278, val loss 2.013152837753296\n",
      "step 3506: train loss 1.9254367351531982, val loss 2.009068489074707\n",
      "step 3507: train loss 1.92421293258667, val loss 2.018369674682617\n",
      "step 3508: train loss 1.9223458766937256, val loss 2.0245816707611084\n",
      "step 3509: train loss 1.9207372665405273, val loss 2.01287841796875\n",
      "step 3510: train loss 1.9191677570343018, val loss 2.0169217586517334\n",
      "step 3511: train loss 1.9213004112243652, val loss 2.0140411853790283\n",
      "step 3512: train loss 1.9143381118774414, val loss 2.0089879035949707\n",
      "step 3513: train loss 1.9114925861358643, val loss 2.0062906742095947\n",
      "step 3514: train loss 1.920518159866333, val loss 2.0065042972564697\n",
      "step 3515: train loss 1.9238423109054565, val loss 2.0066580772399902\n",
      "step 3516: train loss 1.930311679840088, val loss 2.0134572982788086\n",
      "step 3517: train loss 1.9267833232879639, val loss 2.009094476699829\n",
      "step 3518: train loss 1.9081116914749146, val loss 2.009765863418579\n",
      "step 3519: train loss 1.9194793701171875, val loss 2.0163068771362305\n",
      "step 3520: train loss 1.9196460247039795, val loss 2.0057647228240967\n",
      "step 3521: train loss 1.9114283323287964, val loss 1.9962735176086426\n",
      "step 3522: train loss 1.9217380285263062, val loss 2.0172173976898193\n",
      "step 3523: train loss 1.9140814542770386, val loss 1.9985133409500122\n",
      "step 3524: train loss 1.917992353439331, val loss 2.013791561126709\n",
      "step 3525: train loss 1.9289807081222534, val loss 2.022825241088867\n",
      "step 3526: train loss 1.9260907173156738, val loss 2.025946617126465\n",
      "step 3527: train loss 1.915216088294983, val loss 2.0130491256713867\n",
      "step 3528: train loss 1.919972538948059, val loss 2.011763334274292\n",
      "step 3529: train loss 1.923801302909851, val loss 2.023632764816284\n",
      "step 3530: train loss 1.9174621105194092, val loss 2.020784378051758\n",
      "step 3531: train loss 1.9241403341293335, val loss 2.018366813659668\n",
      "step 3532: train loss 1.9217900037765503, val loss 2.0076024532318115\n",
      "step 3533: train loss 1.9151474237442017, val loss 1.994957685470581\n",
      "step 3534: train loss 1.9213746786117554, val loss 2.0006611347198486\n",
      "step 3535: train loss 1.919553518295288, val loss 2.0139071941375732\n",
      "step 3536: train loss 1.9278755187988281, val loss 2.011363983154297\n",
      "step 3537: train loss 1.9306427240371704, val loss 2.021106243133545\n",
      "step 3538: train loss 1.923903465270996, val loss 2.0061442852020264\n",
      "step 3539: train loss 1.9115777015686035, val loss 2.002415657043457\n",
      "step 3540: train loss 1.9144877195358276, val loss 1.995871901512146\n",
      "step 3541: train loss 1.9066544771194458, val loss 2.003643035888672\n",
      "step 3542: train loss 1.9074865579605103, val loss 2.006382942199707\n",
      "step 3543: train loss 1.9187833070755005, val loss 2.0048224925994873\n",
      "step 3544: train loss 1.901107907295227, val loss 2.000988245010376\n",
      "step 3545: train loss 1.913084864616394, val loss 1.9980868101119995\n",
      "step 3546: train loss 1.9083011150360107, val loss 1.995608925819397\n",
      "step 3547: train loss 1.9117465019226074, val loss 2.002537727355957\n",
      "step 3548: train loss 1.9072051048278809, val loss 2.007472276687622\n",
      "step 3549: train loss 1.9078068733215332, val loss 2.00280499458313\n",
      "step 3550: train loss 1.9067060947418213, val loss 1.9997187852859497\n",
      "step 3551: train loss 1.9107484817504883, val loss 1.9926211833953857\n",
      "step 3552: train loss 1.9063359498977661, val loss 2.0016427040100098\n",
      "step 3553: train loss 1.9024293422698975, val loss 1.9918856620788574\n",
      "step 3554: train loss 1.9196711778640747, val loss 2.002939224243164\n",
      "step 3555: train loss 1.9137451648712158, val loss 2.0074660778045654\n",
      "step 3556: train loss 1.919130802154541, val loss 2.0086631774902344\n",
      "step 3557: train loss 1.9223824739456177, val loss 2.0109713077545166\n",
      "step 3558: train loss 1.9160449504852295, val loss 2.0173885822296143\n",
      "step 3559: train loss 1.9084011316299438, val loss 2.0148401260375977\n",
      "step 3560: train loss 1.909013271331787, val loss 2.009234666824341\n",
      "step 3561: train loss 1.9073302745819092, val loss 2.0044472217559814\n",
      "step 3562: train loss 1.916447639465332, val loss 1.9969968795776367\n",
      "step 3563: train loss 1.9018298387527466, val loss 1.9996774196624756\n",
      "step 3564: train loss 1.91410231590271, val loss 1.9955923557281494\n",
      "step 3565: train loss 1.9089610576629639, val loss 2.009584903717041\n",
      "step 3566: train loss 1.900681495666504, val loss 2.0013723373413086\n",
      "step 3567: train loss 1.9047348499298096, val loss 1.9954487085342407\n",
      "step 3568: train loss 1.9056274890899658, val loss 1.9899544715881348\n",
      "step 3569: train loss 1.9051569700241089, val loss 1.9950015544891357\n",
      "step 3570: train loss 1.905147671699524, val loss 1.9951013326644897\n",
      "step 3571: train loss 1.9127117395401, val loss 2.0015294551849365\n",
      "step 3572: train loss 1.916131854057312, val loss 1.9961824417114258\n",
      "step 3573: train loss 1.9079616069793701, val loss 2.003567695617676\n",
      "step 3574: train loss 1.9201362133026123, val loss 1.9920159578323364\n",
      "step 3575: train loss 1.9135212898254395, val loss 1.9910327196121216\n",
      "step 3576: train loss 1.9059052467346191, val loss 1.9972915649414062\n",
      "step 3577: train loss 1.9127649068832397, val loss 1.9925808906555176\n",
      "step 3578: train loss 1.9140520095825195, val loss 1.991916537284851\n",
      "step 3579: train loss 1.9075905084609985, val loss 1.9969305992126465\n",
      "step 3580: train loss 1.9103386402130127, val loss 2.0142149925231934\n",
      "step 3581: train loss 1.9080215692520142, val loss 1.9974006414413452\n",
      "step 3582: train loss 1.9060481786727905, val loss 1.9978450536727905\n",
      "step 3583: train loss 1.9047774076461792, val loss 2.0012505054473877\n",
      "step 3584: train loss 1.9069480895996094, val loss 1.9929031133651733\n",
      "step 3585: train loss 1.9161949157714844, val loss 1.9977554082870483\n",
      "step 3586: train loss 1.9139938354492188, val loss 2.003610134124756\n",
      "step 3587: train loss 1.9041907787322998, val loss 1.9997851848602295\n",
      "step 3588: train loss 1.9086564779281616, val loss 1.997733473777771\n",
      "step 3589: train loss 1.91294527053833, val loss 1.9983317852020264\n",
      "step 3590: train loss 1.9129557609558105, val loss 1.9962190389633179\n",
      "step 3591: train loss 1.9113229513168335, val loss 2.0050148963928223\n",
      "step 3592: train loss 1.9171441793441772, val loss 2.000990629196167\n",
      "step 3593: train loss 1.9049935340881348, val loss 2.0018866062164307\n",
      "step 3594: train loss 1.9111236333847046, val loss 2.0065910816192627\n",
      "step 3595: train loss 1.8994635343551636, val loss 2.0025808811187744\n",
      "step 3596: train loss 1.9039231538772583, val loss 2.0064454078674316\n",
      "step 3597: train loss 1.9076217412948608, val loss 2.0054917335510254\n",
      "step 3598: train loss 1.9056413173675537, val loss 2.001248359680176\n",
      "step 3599: train loss 1.9225096702575684, val loss 2.000126600265503\n",
      "step 3601: train loss 1.905206561088562, val loss 2.002990961074829\n",
      "step 3602: train loss 1.9044479131698608, val loss 2.00524640083313\n",
      "step 3603: train loss 1.8991938829421997, val loss 1.9983832836151123\n",
      "step 3604: train loss 1.9058849811553955, val loss 1.9910355806350708\n",
      "step 3605: train loss 1.9130523204803467, val loss 2.0040361881256104\n",
      "step 3606: train loss 1.912613034248352, val loss 2.013148546218872\n",
      "step 3607: train loss 1.907646894454956, val loss 2.0019659996032715\n",
      "step 3608: train loss 1.9011949300765991, val loss 2.0013561248779297\n",
      "step 3609: train loss 1.9128719568252563, val loss 2.008842706680298\n",
      "step 3610: train loss 1.908806562423706, val loss 1.9932631254196167\n",
      "step 3611: train loss 1.9053077697753906, val loss 1.99590265750885\n",
      "step 3612: train loss 1.9145567417144775, val loss 1.9977459907531738\n",
      "step 3613: train loss 1.9198497533798218, val loss 2.002284049987793\n",
      "step 3614: train loss 1.9174323081970215, val loss 1.992106318473816\n",
      "step 3615: train loss 1.9132341146469116, val loss 1.9994338750839233\n",
      "step 3616: train loss 1.908647060394287, val loss 1.9990627765655518\n",
      "step 3617: train loss 1.9036694765090942, val loss 2.0034241676330566\n",
      "step 3618: train loss 1.9022098779678345, val loss 1.9959831237792969\n",
      "step 3619: train loss 1.9014972448349, val loss 2.011157274246216\n",
      "step 3620: train loss 1.8945245742797852, val loss 2.0014820098876953\n",
      "step 3621: train loss 1.9032084941864014, val loss 1.9953491687774658\n",
      "step 3622: train loss 1.9002556800842285, val loss 1.9982699155807495\n",
      "step 3623: train loss 1.8950412273406982, val loss 1.990286111831665\n",
      "step 3624: train loss 1.9060909748077393, val loss 1.9960427284240723\n",
      "step 3625: train loss 1.9032996892929077, val loss 2.0041496753692627\n",
      "step 3626: train loss 1.8947182893753052, val loss 2.0064761638641357\n",
      "step 3627: train loss 1.9091289043426514, val loss 1.9955263137817383\n",
      "step 3628: train loss 1.9023715257644653, val loss 2.00357723236084\n",
      "step 3629: train loss 1.8974297046661377, val loss 1.9992001056671143\n",
      "step 3630: train loss 1.9013046026229858, val loss 1.996290683746338\n",
      "step 3631: train loss 1.9017131328582764, val loss 1.9921172857284546\n",
      "step 3632: train loss 1.8986573219299316, val loss 1.9948078393936157\n",
      "step 3633: train loss 1.9029998779296875, val loss 1.9981741905212402\n",
      "step 3634: train loss 1.893245816230774, val loss 1.9924496412277222\n",
      "step 3635: train loss 1.8986610174179077, val loss 1.993491530418396\n",
      "step 3636: train loss 1.907467246055603, val loss 1.991062879562378\n",
      "step 3637: train loss 1.9011503458023071, val loss 1.9899710416793823\n",
      "step 3638: train loss 1.8992260694503784, val loss 1.987037181854248\n",
      "step 3639: train loss 1.9046084880828857, val loss 1.995785117149353\n",
      "step 3640: train loss 1.902572751045227, val loss 2.007401943206787\n",
      "step 3641: train loss 1.913362979888916, val loss 2.006352424621582\n",
      "step 3642: train loss 1.9011834859848022, val loss 2.0072174072265625\n",
      "step 3643: train loss 1.907350778579712, val loss 1.9961391687393188\n",
      "step 3644: train loss 1.9148370027542114, val loss 2.0048270225524902\n",
      "step 3645: train loss 1.9207746982574463, val loss 2.0116262435913086\n",
      "step 3646: train loss 1.9142613410949707, val loss 1.9991888999938965\n",
      "step 3647: train loss 1.9101104736328125, val loss 2.008596658706665\n",
      "step 3648: train loss 1.9009753465652466, val loss 2.003227472305298\n",
      "step 3649: train loss 1.9028499126434326, val loss 2.0011391639709473\n",
      "step 3650: train loss 1.9099104404449463, val loss 2.0071353912353516\n",
      "step 3651: train loss 1.9133706092834473, val loss 2.0146372318267822\n",
      "step 3652: train loss 1.9154150485992432, val loss 2.0206611156463623\n",
      "step 3653: train loss 1.902838110923767, val loss 2.0273635387420654\n",
      "step 3654: train loss 1.9092893600463867, val loss 2.0201401710510254\n",
      "step 3655: train loss 1.9126182794570923, val loss 2.005373954772949\n",
      "step 3656: train loss 1.8987624645233154, val loss 2.0140109062194824\n",
      "step 3657: train loss 1.909220576286316, val loss 2.015641212463379\n",
      "step 3658: train loss 1.910775899887085, val loss 2.018148899078369\n",
      "step 3659: train loss 1.9051198959350586, val loss 2.015108346939087\n",
      "step 3660: train loss 1.9080171585083008, val loss 2.0083670616149902\n",
      "step 3661: train loss 1.909741997718811, val loss 2.011746406555176\n",
      "step 3662: train loss 1.9158082008361816, val loss 2.001401662826538\n",
      "step 3663: train loss 1.8979846239089966, val loss 2.004479169845581\n",
      "step 3664: train loss 1.8991130590438843, val loss 2.0050599575042725\n",
      "step 3665: train loss 1.8989671468734741, val loss 2.0030102729797363\n",
      "step 3666: train loss 1.8974653482437134, val loss 1.9898695945739746\n",
      "step 3667: train loss 1.9068759679794312, val loss 2.0124237537384033\n",
      "step 3668: train loss 1.908525824546814, val loss 2.0053794384002686\n",
      "step 3669: train loss 1.9059138298034668, val loss 2.005531072616577\n",
      "step 3670: train loss 1.899898648262024, val loss 2.009207010269165\n",
      "step 3671: train loss 1.9123934507369995, val loss 2.0076770782470703\n",
      "step 3672: train loss 1.9057822227478027, val loss 1.9964426755905151\n",
      "step 3673: train loss 1.9011574983596802, val loss 2.0087554454803467\n",
      "step 3674: train loss 1.9074000120162964, val loss 2.011752128601074\n",
      "step 3675: train loss 1.9039732217788696, val loss 2.007293701171875\n",
      "step 3676: train loss 1.9008063077926636, val loss 2.0200326442718506\n",
      "step 3677: train loss 1.9037913084030151, val loss 2.012301445007324\n",
      "step 3678: train loss 1.9071266651153564, val loss 2.008720874786377\n",
      "step 3679: train loss 1.9059405326843262, val loss 1.9996962547302246\n",
      "step 3680: train loss 1.9006069898605347, val loss 1.9966968297958374\n",
      "step 3681: train loss 1.9043629169464111, val loss 2.005568504333496\n",
      "step 3682: train loss 1.9029780626296997, val loss 2.0105955600738525\n",
      "step 3683: train loss 1.9051073789596558, val loss 1.9963963031768799\n",
      "step 3684: train loss 1.906111717224121, val loss 2.003094434738159\n",
      "step 3685: train loss 1.9058688879013062, val loss 2.0078158378601074\n",
      "step 3686: train loss 1.9159002304077148, val loss 1.998487949371338\n",
      "step 3687: train loss 1.908168911933899, val loss 2.001497745513916\n",
      "step 3688: train loss 1.9056427478790283, val loss 2.0070438385009766\n",
      "step 3689: train loss 1.907875418663025, val loss 2.006711959838867\n",
      "step 3690: train loss 1.9067153930664062, val loss 1.9960665702819824\n",
      "step 3691: train loss 1.8990488052368164, val loss 1.9989639520645142\n",
      "step 3692: train loss 1.9038537740707397, val loss 2.0012996196746826\n",
      "step 3693: train loss 1.904633641242981, val loss 2.0081663131713867\n",
      "step 3694: train loss 1.8918946981430054, val loss 2.0032646656036377\n",
      "step 3695: train loss 1.8919005393981934, val loss 1.9917124509811401\n",
      "step 3696: train loss 1.8884761333465576, val loss 1.997505784034729\n",
      "step 3697: train loss 1.902753472328186, val loss 1.9871989488601685\n",
      "step 3698: train loss 1.8929321765899658, val loss 1.9851781129837036\n",
      "step 3699: train loss 1.8943041563034058, val loss 1.993564248085022\n",
      "step 3701: train loss 1.910764455795288, val loss 1.9968769550323486\n",
      "step 3702: train loss 1.9036979675292969, val loss 1.9926786422729492\n",
      "step 3703: train loss 1.8970940113067627, val loss 1.9931713342666626\n",
      "step 3704: train loss 1.8949856758117676, val loss 1.989208698272705\n",
      "step 3705: train loss 1.901023268699646, val loss 1.9932595491409302\n",
      "step 3706: train loss 1.9013571739196777, val loss 1.997306227684021\n",
      "step 3707: train loss 1.9050177335739136, val loss 1.9952263832092285\n",
      "step 3708: train loss 1.9029842615127563, val loss 1.996920108795166\n",
      "step 3709: train loss 1.890161156654358, val loss 1.9966540336608887\n",
      "step 3710: train loss 1.896010160446167, val loss 1.9910238981246948\n",
      "step 3711: train loss 1.8991518020629883, val loss 1.9987034797668457\n",
      "step 3712: train loss 1.911795973777771, val loss 1.9995936155319214\n",
      "step 3713: train loss 1.9005804061889648, val loss 2.0045340061187744\n",
      "step 3714: train loss 1.9035696983337402, val loss 1.9950686693191528\n",
      "step 3715: train loss 1.8961536884307861, val loss 1.999662160873413\n",
      "step 3716: train loss 1.9033253192901611, val loss 1.9904944896697998\n",
      "step 3717: train loss 1.895966649055481, val loss 1.993443489074707\n",
      "step 3718: train loss 1.8979955911636353, val loss 1.9960838556289673\n",
      "step 3719: train loss 1.892978310585022, val loss 2.011014461517334\n",
      "step 3720: train loss 1.9031169414520264, val loss 2.0090041160583496\n",
      "step 3721: train loss 1.896944522857666, val loss 2.008512496948242\n",
      "step 3722: train loss 1.9022698402404785, val loss 2.0132884979248047\n",
      "step 3723: train loss 1.8930493593215942, val loss 2.0026164054870605\n",
      "step 3724: train loss 1.906907081604004, val loss 2.0069382190704346\n",
      "step 3725: train loss 1.8966116905212402, val loss 2.0015947818756104\n",
      "step 3726: train loss 1.8937798738479614, val loss 1.9961169958114624\n",
      "step 3727: train loss 1.896772027015686, val loss 2.0054855346679688\n",
      "step 3728: train loss 1.8952035903930664, val loss 2.000056505203247\n",
      "step 3729: train loss 1.9055863618850708, val loss 2.0078749656677246\n",
      "step 3730: train loss 1.8995201587677002, val loss 2.0034360885620117\n",
      "step 3731: train loss 1.8947787284851074, val loss 2.0116117000579834\n",
      "step 3732: train loss 1.8954658508300781, val loss 2.0051355361938477\n",
      "step 3733: train loss 1.9001555442810059, val loss 2.0069739818573\n",
      "step 3734: train loss 1.9007450342178345, val loss 2.006732702255249\n",
      "step 3735: train loss 1.8953214883804321, val loss 2.006876230239868\n",
      "step 3736: train loss 1.8835505247116089, val loss 1.9993418455123901\n",
      "step 3737: train loss 1.8950293064117432, val loss 1.9942522048950195\n",
      "step 3738: train loss 1.898036003112793, val loss 2.0035898685455322\n",
      "step 3739: train loss 1.898256540298462, val loss 2.0006771087646484\n",
      "step 3740: train loss 1.898101806640625, val loss 2.012320041656494\n",
      "step 3741: train loss 1.905439853668213, val loss 2.0095179080963135\n",
      "step 3742: train loss 1.9109748601913452, val loss 2.0122432708740234\n",
      "step 3743: train loss 1.908511757850647, val loss 2.014293909072876\n",
      "step 3744: train loss 1.9010196924209595, val loss 2.008836269378662\n",
      "step 3745: train loss 1.8903290033340454, val loss 2.0105772018432617\n",
      "step 3746: train loss 1.8996254205703735, val loss 2.00927996635437\n",
      "step 3747: train loss 1.8969509601593018, val loss 2.0123226642608643\n",
      "step 3748: train loss 1.9008041620254517, val loss 2.0096333026885986\n",
      "step 3749: train loss 1.8980345726013184, val loss 2.010479688644409\n",
      "step 3750: train loss 1.901831865310669, val loss 1.9972202777862549\n",
      "step 3751: train loss 1.899539828300476, val loss 2.0160207748413086\n",
      "step 3752: train loss 1.9048751592636108, val loss 2.0113472938537598\n",
      "step 3753: train loss 1.899742603302002, val loss 2.0097408294677734\n",
      "step 3754: train loss 1.8989176750183105, val loss 1.9900436401367188\n",
      "step 3755: train loss 1.8976373672485352, val loss 2.0041513442993164\n",
      "step 3756: train loss 1.899678349494934, val loss 2.01528263092041\n",
      "step 3757: train loss 1.9048229455947876, val loss 2.0056378841400146\n",
      "step 3758: train loss 1.895607352256775, val loss 1.9984421730041504\n",
      "step 3759: train loss 1.8901687860488892, val loss 2.0080747604370117\n",
      "step 3760: train loss 1.889310359954834, val loss 2.006948471069336\n",
      "step 3761: train loss 1.8919677734375, val loss 2.003105401992798\n",
      "step 3762: train loss 1.8876309394836426, val loss 1.9998055696487427\n",
      "step 3763: train loss 1.8938406705856323, val loss 2.0035321712493896\n",
      "step 3764: train loss 1.8925286531448364, val loss 2.0112035274505615\n",
      "step 3765: train loss 1.9005569219589233, val loss 2.000988245010376\n",
      "step 3766: train loss 1.8967477083206177, val loss 1.9966925382614136\n",
      "step 3767: train loss 1.8976150751113892, val loss 1.995522141456604\n",
      "step 3768: train loss 1.8955051898956299, val loss 1.9956655502319336\n",
      "step 3769: train loss 1.903315782546997, val loss 2.0080394744873047\n",
      "step 3770: train loss 1.8881409168243408, val loss 2.005737543106079\n",
      "step 3771: train loss 1.8997043371200562, val loss 2.003443717956543\n",
      "step 3772: train loss 1.890754222869873, val loss 2.0036075115203857\n",
      "step 3773: train loss 1.8953306674957275, val loss 1.994578242301941\n",
      "step 3774: train loss 1.8891302347183228, val loss 1.987117886543274\n",
      "step 3775: train loss 1.9006869792938232, val loss 1.9906795024871826\n",
      "step 3776: train loss 1.8869649171829224, val loss 1.993567705154419\n",
      "step 3777: train loss 1.8831405639648438, val loss 2.0004467964172363\n",
      "step 3778: train loss 1.8847543001174927, val loss 1.9997097253799438\n",
      "step 3779: train loss 1.8907214403152466, val loss 1.9915357828140259\n",
      "step 3780: train loss 1.8957144021987915, val loss 1.9982446432113647\n",
      "step 3781: train loss 1.9007678031921387, val loss 1.9914134740829468\n",
      "step 3782: train loss 1.894737958908081, val loss 2.00578236579895\n",
      "step 3783: train loss 1.8790161609649658, val loss 1.9904167652130127\n",
      "step 3784: train loss 1.8852726221084595, val loss 1.9944627285003662\n",
      "step 3785: train loss 1.8986878395080566, val loss 1.9954404830932617\n",
      "step 3786: train loss 1.8901290893554688, val loss 2.0010552406311035\n",
      "step 3787: train loss 1.8923872709274292, val loss 1.998938798904419\n",
      "step 3788: train loss 1.8897706270217896, val loss 1.9942831993103027\n",
      "step 3789: train loss 1.9049817323684692, val loss 2.0014572143554688\n",
      "step 3790: train loss 1.891085147857666, val loss 1.9956485033035278\n",
      "step 3791: train loss 1.9043563604354858, val loss 1.9879940748214722\n",
      "step 3792: train loss 1.8977795839309692, val loss 1.9933472871780396\n",
      "step 3793: train loss 1.9041465520858765, val loss 1.9967319965362549\n",
      "step 3794: train loss 1.8942971229553223, val loss 1.996321678161621\n",
      "step 3795: train loss 1.8940547704696655, val loss 1.9976675510406494\n",
      "step 3796: train loss 1.8965915441513062, val loss 2.005204439163208\n",
      "step 3797: train loss 1.8960360288619995, val loss 1.9912209510803223\n",
      "step 3798: train loss 1.8945212364196777, val loss 1.9906797409057617\n",
      "step 3799: train loss 1.8857395648956299, val loss 1.9936952590942383\n",
      "step 3801: train loss 1.8966931104660034, val loss 1.9941787719726562\n",
      "step 3802: train loss 1.9047520160675049, val loss 1.9948668479919434\n",
      "step 3803: train loss 1.8966889381408691, val loss 1.997715711593628\n",
      "step 3804: train loss 1.896874189376831, val loss 2.0013561248779297\n",
      "step 3805: train loss 1.898016333580017, val loss 1.9880714416503906\n",
      "step 3806: train loss 1.900179386138916, val loss 1.9966437816619873\n",
      "step 3807: train loss 1.8943188190460205, val loss 1.9976260662078857\n",
      "step 3808: train loss 1.9027687311172485, val loss 2.0048651695251465\n",
      "step 3809: train loss 1.900600790977478, val loss 2.0032126903533936\n",
      "step 3810: train loss 1.9033054113388062, val loss 2.006035566329956\n",
      "step 3811: train loss 1.9001425504684448, val loss 2.0037806034088135\n",
      "step 3812: train loss 1.8824610710144043, val loss 1.9901597499847412\n",
      "step 3813: train loss 1.8879716396331787, val loss 1.9998860359191895\n",
      "step 3814: train loss 1.8924866914749146, val loss 2.000169277191162\n",
      "step 3815: train loss 1.897364854812622, val loss 2.001093864440918\n",
      "step 3816: train loss 1.902405858039856, val loss 1.9908850193023682\n",
      "step 3817: train loss 1.8833434581756592, val loss 1.9989007711410522\n",
      "step 3818: train loss 1.883165717124939, val loss 1.98860764503479\n",
      "step 3819: train loss 1.8834137916564941, val loss 2.0036513805389404\n",
      "step 3820: train loss 1.9036927223205566, val loss 2.003838539123535\n",
      "step 3821: train loss 1.8922131061553955, val loss 2.0013341903686523\n",
      "step 3822: train loss 1.8888905048370361, val loss 2.0061795711517334\n",
      "step 3823: train loss 1.886803150177002, val loss 2.0099689960479736\n",
      "step 3824: train loss 1.8982009887695312, val loss 2.0059783458709717\n",
      "step 3825: train loss 1.8891161680221558, val loss 2.0004806518554688\n",
      "step 3826: train loss 1.8974363803863525, val loss 2.000886917114258\n",
      "step 3827: train loss 1.8916834592819214, val loss 2.0007011890411377\n",
      "step 3828: train loss 1.901570439338684, val loss 1.9960191249847412\n",
      "step 3829: train loss 1.895862102508545, val loss 1.9931206703186035\n",
      "step 3830: train loss 1.8985414505004883, val loss 2.005373239517212\n",
      "step 3831: train loss 1.8933213949203491, val loss 1.9895579814910889\n",
      "step 3832: train loss 1.8827227354049683, val loss 1.9982279539108276\n",
      "step 3833: train loss 1.8823078870773315, val loss 2.0003502368927\n",
      "step 3834: train loss 1.8969987630844116, val loss 2.005992889404297\n",
      "step 3835: train loss 1.8890858888626099, val loss 2.007578134536743\n",
      "step 3836: train loss 1.8877384662628174, val loss 1.9938938617706299\n",
      "step 3837: train loss 1.8934807777404785, val loss 2.001217842102051\n",
      "step 3838: train loss 1.882204294204712, val loss 1.989984154701233\n",
      "step 3839: train loss 1.888952612876892, val loss 1.9968072175979614\n",
      "step 3840: train loss 1.8985791206359863, val loss 2.00925874710083\n",
      "step 3841: train loss 1.8959367275238037, val loss 2.0127410888671875\n",
      "step 3842: train loss 1.8923547267913818, val loss 2.004696846008301\n",
      "step 3843: train loss 1.894405722618103, val loss 1.9915454387664795\n",
      "step 3844: train loss 1.8875235319137573, val loss 1.999403476715088\n",
      "step 3845: train loss 1.8916540145874023, val loss 1.9951063394546509\n",
      "step 3846: train loss 1.8803225755691528, val loss 1.9968990087509155\n",
      "step 3847: train loss 1.893217921257019, val loss 1.9869967699050903\n",
      "step 3848: train loss 1.8872257471084595, val loss 1.985590934753418\n",
      "step 3849: train loss 1.8864943981170654, val loss 1.9803234338760376\n",
      "step 3850: train loss 1.8908249139785767, val loss 1.9979910850524902\n",
      "step 3851: train loss 1.8862581253051758, val loss 1.9857081174850464\n",
      "step 3852: train loss 1.875238060951233, val loss 1.9805032014846802\n",
      "step 3853: train loss 1.8832796812057495, val loss 1.9819833040237427\n",
      "step 3854: train loss 1.880021572113037, val loss 1.9898276329040527\n",
      "step 3855: train loss 1.8834341764450073, val loss 1.9867151975631714\n",
      "step 3856: train loss 1.883023738861084, val loss 1.9869835376739502\n",
      "step 3857: train loss 1.8872708082199097, val loss 1.9983859062194824\n",
      "step 3858: train loss 1.8824337720870972, val loss 1.9748547077178955\n",
      "step 3859: train loss 1.8858758211135864, val loss 1.984228253364563\n",
      "step 3860: train loss 1.883412480354309, val loss 1.9871010780334473\n",
      "step 3861: train loss 1.878637671470642, val loss 1.9869426488876343\n",
      "step 3862: train loss 1.8932366371154785, val loss 1.9790065288543701\n",
      "step 3863: train loss 1.8863921165466309, val loss 1.9843318462371826\n",
      "step 3864: train loss 1.8852462768554688, val loss 1.9902024269104004\n",
      "step 3865: train loss 1.8876712322235107, val loss 1.985106348991394\n",
      "step 3866: train loss 1.8818719387054443, val loss 1.984432578086853\n",
      "step 3867: train loss 1.8791502714157104, val loss 1.9855822324752808\n",
      "step 3868: train loss 1.8894610404968262, val loss 1.97908353805542\n",
      "step 3869: train loss 1.8851925134658813, val loss 1.9919639825820923\n",
      "step 3870: train loss 1.9056174755096436, val loss 1.9888192415237427\n",
      "step 3871: train loss 1.9001660346984863, val loss 1.9919275045394897\n",
      "step 3872: train loss 1.8938003778457642, val loss 1.9952857494354248\n",
      "step 3873: train loss 1.8961975574493408, val loss 1.999993085861206\n",
      "step 3874: train loss 1.8860528469085693, val loss 1.9929150342941284\n",
      "step 3875: train loss 1.8877296447753906, val loss 1.9968671798706055\n",
      "step 3876: train loss 1.8895384073257446, val loss 1.9892734289169312\n",
      "step 3877: train loss 1.8777263164520264, val loss 1.9886810779571533\n",
      "step 3878: train loss 1.8825427293777466, val loss 1.9984524250030518\n",
      "step 3879: train loss 1.8840011358261108, val loss 1.9920517206192017\n",
      "step 3880: train loss 1.8764028549194336, val loss 1.9893685579299927\n",
      "step 3881: train loss 1.8751314878463745, val loss 2.0011074542999268\n",
      "step 3882: train loss 1.8781335353851318, val loss 1.9997934103012085\n",
      "step 3883: train loss 1.8861966133117676, val loss 1.9900823831558228\n",
      "step 3884: train loss 1.8912041187286377, val loss 2.0101046562194824\n",
      "step 3885: train loss 1.8832671642303467, val loss 2.006242275238037\n",
      "step 3886: train loss 1.889682412147522, val loss 1.9957407712936401\n",
      "step 3887: train loss 1.8868086338043213, val loss 1.9884555339813232\n",
      "step 3888: train loss 1.8840093612670898, val loss 2.0035040378570557\n",
      "step 3889: train loss 1.8893425464630127, val loss 1.9894217252731323\n",
      "step 3890: train loss 1.875359296798706, val loss 1.9865634441375732\n",
      "step 3891: train loss 1.8906441926956177, val loss 1.9849827289581299\n",
      "step 3892: train loss 1.8821160793304443, val loss 1.9936985969543457\n",
      "step 3893: train loss 1.880373477935791, val loss 1.9866522550582886\n",
      "step 3894: train loss 1.8853983879089355, val loss 1.9869122505187988\n",
      "step 3895: train loss 1.8842720985412598, val loss 1.9968938827514648\n",
      "step 3896: train loss 1.8938521146774292, val loss 1.9963078498840332\n",
      "step 3897: train loss 1.8922603130340576, val loss 2.006368637084961\n",
      "step 3898: train loss 1.8898910284042358, val loss 1.9978762865066528\n",
      "step 3899: train loss 1.8811947107315063, val loss 1.999198317527771\n",
      "step 3901: train loss 1.8936172723770142, val loss 1.9956870079040527\n",
      "step 3902: train loss 1.8993076086044312, val loss 1.9975883960723877\n",
      "step 3903: train loss 1.8977714776992798, val loss 1.9970670938491821\n",
      "step 3904: train loss 1.898317813873291, val loss 1.9955214262008667\n",
      "step 3905: train loss 1.8960320949554443, val loss 1.9990136623382568\n",
      "step 3906: train loss 1.896472454071045, val loss 1.9859135150909424\n",
      "step 3907: train loss 1.9015051126480103, val loss 1.9952151775360107\n",
      "step 3908: train loss 1.9066894054412842, val loss 2.003338098526001\n",
      "step 3909: train loss 1.8961387872695923, val loss 2.0063817501068115\n",
      "step 3910: train loss 1.8933604955673218, val loss 1.9919270277023315\n",
      "step 3911: train loss 1.8941864967346191, val loss 2.0018694400787354\n",
      "step 3912: train loss 1.8992708921432495, val loss 2.009110927581787\n",
      "step 3913: train loss 1.906339168548584, val loss 2.023285388946533\n",
      "step 3914: train loss 1.908375859260559, val loss 2.0215883255004883\n",
      "step 3915: train loss 1.9058934450149536, val loss 2.00877046585083\n",
      "step 3916: train loss 1.9144477844238281, val loss 2.010531425476074\n",
      "step 3917: train loss 1.8965325355529785, val loss 1.9929808378219604\n",
      "step 3918: train loss 1.8879696130752563, val loss 1.9886969327926636\n",
      "step 3919: train loss 1.8938959836959839, val loss 1.9836373329162598\n",
      "step 3920: train loss 1.8926299810409546, val loss 1.9904730319976807\n",
      "step 3921: train loss 1.8833858966827393, val loss 1.9734396934509277\n",
      "step 3922: train loss 1.896572232246399, val loss 1.9853729009628296\n",
      "step 3923: train loss 1.8845306634902954, val loss 1.9811359643936157\n",
      "step 3924: train loss 1.8811813592910767, val loss 1.9734994173049927\n",
      "step 3925: train loss 1.878593921661377, val loss 1.9844160079956055\n",
      "step 3926: train loss 1.872868299484253, val loss 1.9924349784851074\n",
      "step 3927: train loss 1.885459303855896, val loss 1.9810444116592407\n",
      "step 3928: train loss 1.8887038230895996, val loss 1.9847720861434937\n",
      "step 3929: train loss 1.881689190864563, val loss 1.979047417640686\n",
      "step 3930: train loss 1.8875524997711182, val loss 1.9923089742660522\n",
      "step 3931: train loss 1.8905918598175049, val loss 1.993661642074585\n",
      "step 3932: train loss 1.8768287897109985, val loss 1.9859882593154907\n",
      "step 3933: train loss 1.8862030506134033, val loss 1.9821803569793701\n",
      "step 3934: train loss 1.8796910047531128, val loss 1.9779024124145508\n",
      "step 3935: train loss 1.8805924654006958, val loss 1.9872585535049438\n",
      "step 3936: train loss 1.8799240589141846, val loss 1.9848970174789429\n",
      "step 3937: train loss 1.8901662826538086, val loss 1.989104151725769\n",
      "step 3938: train loss 1.8823274374008179, val loss 1.9882372617721558\n",
      "step 3939: train loss 1.8798831701278687, val loss 1.9845672845840454\n",
      "step 3940: train loss 1.879684329032898, val loss 1.9880914688110352\n",
      "step 3941: train loss 1.882026195526123, val loss 1.9970039129257202\n",
      "step 3942: train loss 1.8761383295059204, val loss 1.9850103855133057\n",
      "step 3943: train loss 1.8909637928009033, val loss 1.9908404350280762\n",
      "step 3944: train loss 1.8847992420196533, val loss 1.984696388244629\n",
      "step 3945: train loss 1.8866397142410278, val loss 1.991693139076233\n",
      "step 3946: train loss 1.8882229328155518, val loss 1.9902541637420654\n",
      "step 3947: train loss 1.875611424446106, val loss 1.9908751249313354\n",
      "step 3948: train loss 1.879500150680542, val loss 1.9811835289001465\n",
      "step 3949: train loss 1.873701810836792, val loss 1.9970169067382812\n",
      "step 3950: train loss 1.879209041595459, val loss 1.9877253770828247\n",
      "step 3951: train loss 1.885582447052002, val loss 1.9967950582504272\n",
      "step 3952: train loss 1.8837231397628784, val loss 2.0032975673675537\n",
      "step 3953: train loss 1.8822745084762573, val loss 1.9932643175125122\n",
      "step 3954: train loss 1.896367073059082, val loss 2.000469923019409\n",
      "step 3955: train loss 1.8931888341903687, val loss 1.9988200664520264\n",
      "step 3956: train loss 1.8974593877792358, val loss 1.9911231994628906\n",
      "step 3957: train loss 1.8772366046905518, val loss 1.999890923500061\n",
      "step 3958: train loss 1.8755356073379517, val loss 1.9900137186050415\n",
      "step 3959: train loss 1.882344365119934, val loss 1.9921314716339111\n",
      "step 3960: train loss 1.8826723098754883, val loss 1.9824097156524658\n",
      "step 3961: train loss 1.8759535551071167, val loss 1.9787917137145996\n",
      "step 3962: train loss 1.877516508102417, val loss 1.9794384241104126\n",
      "step 3963: train loss 1.8824985027313232, val loss 1.9906442165374756\n",
      "step 3964: train loss 1.8722939491271973, val loss 1.9820938110351562\n",
      "step 3965: train loss 1.8905110359191895, val loss 1.9790207147598267\n",
      "step 3966: train loss 1.8824896812438965, val loss 1.9809281826019287\n",
      "step 3967: train loss 1.8828848600387573, val loss 1.982340693473816\n",
      "step 3968: train loss 1.8813750743865967, val loss 1.9818089008331299\n",
      "step 3969: train loss 1.888331413269043, val loss 1.9879274368286133\n",
      "step 3970: train loss 1.8904621601104736, val loss 1.9884915351867676\n",
      "step 3971: train loss 1.8817750215530396, val loss 1.982101321220398\n",
      "step 3972: train loss 1.8779484033584595, val loss 1.984257698059082\n",
      "step 3973: train loss 1.8752586841583252, val loss 1.9903249740600586\n",
      "step 3974: train loss 1.8809581995010376, val loss 1.9782828092575073\n",
      "step 3975: train loss 1.8738694190979004, val loss 1.9877994060516357\n",
      "step 3976: train loss 1.8742130994796753, val loss 1.9858230352401733\n",
      "step 3977: train loss 1.8793150186538696, val loss 1.994167685508728\n",
      "step 3978: train loss 1.8737553358078003, val loss 1.984148383140564\n",
      "step 3979: train loss 1.8752702474594116, val loss 1.987765908241272\n",
      "step 3980: train loss 1.8715755939483643, val loss 1.9800344705581665\n",
      "step 3981: train loss 1.8762736320495605, val loss 1.9762260913848877\n",
      "step 3982: train loss 1.8700387477874756, val loss 1.9827675819396973\n",
      "step 3983: train loss 1.868239402770996, val loss 1.9821674823760986\n",
      "step 3984: train loss 1.8684184551239014, val loss 1.9816365242004395\n",
      "step 3985: train loss 1.8655918836593628, val loss 1.9852298498153687\n",
      "step 3986: train loss 1.8745771646499634, val loss 1.9891620874404907\n",
      "step 3987: train loss 1.8795726299285889, val loss 1.9769536256790161\n",
      "step 3988: train loss 1.8726691007614136, val loss 1.9905680418014526\n",
      "step 3989: train loss 1.8922091722488403, val loss 1.9841631650924683\n",
      "step 3990: train loss 1.8816088438034058, val loss 1.9794397354125977\n",
      "step 3991: train loss 1.8742117881774902, val loss 1.979972243309021\n",
      "step 3992: train loss 1.8646702766418457, val loss 1.9873255491256714\n",
      "step 3993: train loss 1.884975552558899, val loss 1.9864088296890259\n",
      "step 3994: train loss 1.8793914318084717, val loss 1.9810200929641724\n",
      "step 3995: train loss 1.8746812343597412, val loss 1.978605031967163\n",
      "step 3996: train loss 1.8820587396621704, val loss 1.991241455078125\n",
      "step 3997: train loss 1.8882044553756714, val loss 1.9858088493347168\n",
      "step 3998: train loss 1.8757492303848267, val loss 1.9815740585327148\n",
      "step 3999: train loss 1.8725900650024414, val loss 1.9819705486297607\n",
      "step 4001: train loss 1.877031683921814, val loss 1.9825770854949951\n",
      "step 4002: train loss 1.8749176263809204, val loss 1.980269193649292\n",
      "step 4003: train loss 1.876185655593872, val loss 1.985172986984253\n",
      "step 4004: train loss 1.8817847967147827, val loss 1.9894717931747437\n",
      "step 4005: train loss 1.8826239109039307, val loss 1.9943184852600098\n",
      "step 4006: train loss 1.8819894790649414, val loss 1.9913649559020996\n",
      "step 4007: train loss 1.8841912746429443, val loss 1.9946353435516357\n",
      "step 4008: train loss 1.8732200860977173, val loss 1.981065034866333\n",
      "step 4009: train loss 1.8799395561218262, val loss 1.9787122011184692\n",
      "step 4010: train loss 1.8698360919952393, val loss 1.982190728187561\n",
      "step 4011: train loss 1.8760871887207031, val loss 1.9727154970169067\n",
      "step 4012: train loss 1.8639483451843262, val loss 1.9818841218948364\n",
      "step 4013: train loss 1.8821126222610474, val loss 1.978354811668396\n",
      "step 4014: train loss 1.8712140321731567, val loss 1.983689308166504\n",
      "step 4015: train loss 1.868953824043274, val loss 1.9805294275283813\n",
      "step 4016: train loss 1.8696603775024414, val loss 1.9739749431610107\n",
      "step 4017: train loss 1.8824515342712402, val loss 1.9860824346542358\n",
      "step 4018: train loss 1.8748165369033813, val loss 1.987890362739563\n",
      "step 4019: train loss 1.876561164855957, val loss 1.9916753768920898\n",
      "step 4020: train loss 1.8788344860076904, val loss 2.002068281173706\n",
      "step 4021: train loss 1.8786274194717407, val loss 1.9905617237091064\n",
      "step 4022: train loss 1.8814818859100342, val loss 1.9871115684509277\n",
      "step 4023: train loss 1.879072666168213, val loss 1.991553783416748\n",
      "step 4024: train loss 1.8786126375198364, val loss 1.9859647750854492\n",
      "step 4025: train loss 1.8788102865219116, val loss 1.9969515800476074\n",
      "step 4026: train loss 1.8699760437011719, val loss 1.9924609661102295\n",
      "step 4027: train loss 1.8793548345565796, val loss 1.9834017753601074\n",
      "step 4028: train loss 1.8751778602600098, val loss 1.9925318956375122\n",
      "step 4029: train loss 1.8732781410217285, val loss 1.9960991144180298\n",
      "step 4030: train loss 1.8775672912597656, val loss 1.9791514873504639\n",
      "step 4031: train loss 1.8783156871795654, val loss 1.9819952249526978\n",
      "step 4032: train loss 1.8811436891555786, val loss 1.9888736009597778\n",
      "step 4033: train loss 1.8678189516067505, val loss 1.9853217601776123\n",
      "step 4034: train loss 1.8696871995925903, val loss 1.9915037155151367\n",
      "step 4035: train loss 1.872969388961792, val loss 1.9879212379455566\n",
      "step 4036: train loss 1.8693751096725464, val loss 1.9824986457824707\n",
      "step 4037: train loss 1.8745248317718506, val loss 1.983548641204834\n",
      "step 4038: train loss 1.8751827478408813, val loss 1.9961732625961304\n",
      "step 4039: train loss 1.8695945739746094, val loss 1.9804556369781494\n",
      "step 4040: train loss 1.8737397193908691, val loss 1.984622597694397\n",
      "step 4041: train loss 1.878568172454834, val loss 1.9940567016601562\n",
      "step 4042: train loss 1.8880032300949097, val loss 2.0047945976257324\n",
      "step 4043: train loss 1.886776089668274, val loss 1.9994430541992188\n",
      "step 4044: train loss 1.888826847076416, val loss 2.000251531600952\n",
      "step 4045: train loss 1.8784079551696777, val loss 1.9998823404312134\n",
      "step 4046: train loss 1.8855273723602295, val loss 1.9925711154937744\n",
      "step 4047: train loss 1.881777286529541, val loss 2.0027244091033936\n",
      "step 4048: train loss 1.8798468112945557, val loss 1.9979994297027588\n",
      "step 4049: train loss 1.8762489557266235, val loss 1.9981991052627563\n",
      "step 4050: train loss 1.8815873861312866, val loss 1.9945623874664307\n",
      "step 4051: train loss 1.87982177734375, val loss 1.9889596700668335\n",
      "step 4052: train loss 1.8638732433319092, val loss 1.9860461950302124\n",
      "step 4053: train loss 1.8722766637802124, val loss 1.9794752597808838\n",
      "step 4054: train loss 1.8693227767944336, val loss 1.9882746934890747\n",
      "step 4055: train loss 1.8690474033355713, val loss 1.9825776815414429\n",
      "step 4056: train loss 1.8711004257202148, val loss 1.9847911596298218\n",
      "step 4057: train loss 1.8730804920196533, val loss 1.9761853218078613\n",
      "step 4058: train loss 1.8798434734344482, val loss 1.9914116859436035\n",
      "step 4059: train loss 1.8743170499801636, val loss 1.9769980907440186\n",
      "step 4060: train loss 1.8740549087524414, val loss 1.9829623699188232\n",
      "step 4061: train loss 1.865726351737976, val loss 1.978755235671997\n",
      "step 4062: train loss 1.8734467029571533, val loss 1.9773564338684082\n",
      "step 4063: train loss 1.8695197105407715, val loss 1.9753156900405884\n",
      "step 4064: train loss 1.8691372871398926, val loss 1.9646145105361938\n",
      "step 4065: train loss 1.87424898147583, val loss 1.9814563989639282\n",
      "step 4066: train loss 1.867703914642334, val loss 1.9707541465759277\n",
      "step 4067: train loss 1.8680980205535889, val loss 1.9801057577133179\n",
      "step 4068: train loss 1.8711633682250977, val loss 1.9815913438796997\n",
      "step 4069: train loss 1.870823860168457, val loss 1.9685760736465454\n",
      "step 4070: train loss 1.8552687168121338, val loss 1.9737598896026611\n",
      "step 4071: train loss 1.8714081048965454, val loss 1.969977617263794\n",
      "step 4072: train loss 1.8638370037078857, val loss 1.9862407445907593\n",
      "step 4073: train loss 1.8647375106811523, val loss 1.9758713245391846\n",
      "step 4074: train loss 1.8669335842132568, val loss 1.9781575202941895\n",
      "step 4075: train loss 1.8602999448776245, val loss 1.9845619201660156\n",
      "step 4076: train loss 1.871332049369812, val loss 1.9920549392700195\n",
      "step 4077: train loss 1.861785888671875, val loss 1.9842602014541626\n",
      "step 4078: train loss 1.871352195739746, val loss 1.9795260429382324\n",
      "step 4079: train loss 1.8655693531036377, val loss 1.9843510389328003\n",
      "step 4080: train loss 1.8656216859817505, val loss 1.9836779832839966\n",
      "step 4081: train loss 1.8672118186950684, val loss 1.982099175453186\n",
      "step 4082: train loss 1.865651249885559, val loss 1.9847817420959473\n",
      "step 4083: train loss 1.8676135540008545, val loss 1.9783644676208496\n",
      "step 4084: train loss 1.8645488023757935, val loss 1.978104829788208\n",
      "step 4085: train loss 1.860551118850708, val loss 1.9794714450836182\n",
      "step 4086: train loss 1.8714163303375244, val loss 1.9809684753417969\n",
      "step 4087: train loss 1.8676283359527588, val loss 1.9780070781707764\n",
      "step 4088: train loss 1.8660775423049927, val loss 1.9614498615264893\n",
      "step 4089: train loss 1.8722805976867676, val loss 1.982262134552002\n",
      "step 4090: train loss 1.8757268190383911, val loss 1.978911280632019\n",
      "step 4091: train loss 1.8647332191467285, val loss 1.9836487770080566\n",
      "step 4092: train loss 1.8795695304870605, val loss 1.9855964183807373\n",
      "step 4093: train loss 1.8664973974227905, val loss 1.9766682386398315\n",
      "step 4094: train loss 1.8770767450332642, val loss 1.9860566854476929\n",
      "step 4095: train loss 1.8803002834320068, val loss 1.9774703979492188\n",
      "step 4096: train loss 1.8776443004608154, val loss 1.9837228059768677\n",
      "step 4097: train loss 1.8632087707519531, val loss 1.9865422248840332\n",
      "step 4098: train loss 1.8700664043426514, val loss 1.9785714149475098\n",
      "step 4099: train loss 1.8748211860656738, val loss 1.9827073812484741\n",
      "step 4101: train loss 1.8698598146438599, val loss 1.986695408821106\n",
      "step 4102: train loss 1.877407193183899, val loss 1.9844169616699219\n",
      "step 4103: train loss 1.8761248588562012, val loss 1.9832987785339355\n",
      "step 4104: train loss 1.8747421503067017, val loss 1.9839950799942017\n",
      "step 4105: train loss 1.876014232635498, val loss 1.974685788154602\n",
      "step 4106: train loss 1.8736584186553955, val loss 1.9797463417053223\n",
      "step 4107: train loss 1.8747638463974, val loss 1.9884774684906006\n",
      "step 4108: train loss 1.877873420715332, val loss 1.9746277332305908\n",
      "step 4109: train loss 1.8721528053283691, val loss 1.9818164110183716\n",
      "step 4110: train loss 1.869753360748291, val loss 1.9742705821990967\n",
      "step 4111: train loss 1.877841591835022, val loss 1.985201120376587\n",
      "step 4112: train loss 1.8778235912322998, val loss 1.9807171821594238\n",
      "step 4113: train loss 1.8748317956924438, val loss 1.9764256477355957\n",
      "step 4114: train loss 1.8792495727539062, val loss 1.9797945022583008\n",
      "step 4115: train loss 1.8753105401992798, val loss 1.9936257600784302\n",
      "step 4116: train loss 1.867210030555725, val loss 1.9832937717437744\n",
      "step 4117: train loss 1.8762125968933105, val loss 1.9852246046066284\n",
      "step 4118: train loss 1.869666337966919, val loss 1.9886839389801025\n",
      "step 4119: train loss 1.8775808811187744, val loss 1.9883549213409424\n",
      "step 4120: train loss 1.864740014076233, val loss 1.9836294651031494\n",
      "step 4121: train loss 1.8704925775527954, val loss 1.9831329584121704\n",
      "step 4122: train loss 1.8755929470062256, val loss 1.9892200231552124\n",
      "step 4123: train loss 1.867390751838684, val loss 1.977623462677002\n",
      "step 4124: train loss 1.8669707775115967, val loss 1.9778212308883667\n",
      "step 4125: train loss 1.870104193687439, val loss 1.976240634918213\n",
      "step 4126: train loss 1.8665672540664673, val loss 1.9849326610565186\n",
      "step 4127: train loss 1.8720813989639282, val loss 1.9885287284851074\n",
      "step 4128: train loss 1.8748103380203247, val loss 1.9823745489120483\n",
      "step 4129: train loss 1.8721410036087036, val loss 1.9736326932907104\n",
      "step 4130: train loss 1.8722151517868042, val loss 1.9785734415054321\n",
      "step 4131: train loss 1.865359902381897, val loss 1.9858005046844482\n",
      "step 4132: train loss 1.8815382719039917, val loss 1.983788013458252\n",
      "step 4133: train loss 1.8649383783340454, val loss 1.9817615747451782\n",
      "step 4134: train loss 1.8696497678756714, val loss 1.9817874431610107\n",
      "step 4135: train loss 1.8772902488708496, val loss 1.979096531867981\n",
      "step 4136: train loss 1.8685091733932495, val loss 1.9825968742370605\n",
      "step 4137: train loss 1.8668783903121948, val loss 1.9754704236984253\n",
      "step 4138: train loss 1.8699569702148438, val loss 1.986708641052246\n",
      "step 4139: train loss 1.8641445636749268, val loss 1.9880963563919067\n",
      "step 4140: train loss 1.8707687854766846, val loss 1.9793481826782227\n",
      "step 4141: train loss 1.8656094074249268, val loss 1.9867522716522217\n",
      "step 4142: train loss 1.8690789937973022, val loss 1.9837020635604858\n",
      "step 4143: train loss 1.8727935552597046, val loss 1.9651875495910645\n",
      "step 4144: train loss 1.8785477876663208, val loss 1.9823815822601318\n",
      "step 4145: train loss 1.8733901977539062, val loss 1.9765384197235107\n",
      "step 4146: train loss 1.8714630603790283, val loss 1.9818512201309204\n",
      "step 4147: train loss 1.8833131790161133, val loss 1.971806287765503\n",
      "step 4148: train loss 1.8802075386047363, val loss 1.9867150783538818\n",
      "step 4149: train loss 1.8823940753936768, val loss 1.9874372482299805\n",
      "step 4150: train loss 1.872421383857727, val loss 1.984269380569458\n",
      "step 4151: train loss 1.8757998943328857, val loss 1.9729760885238647\n",
      "step 4152: train loss 1.8779295682907104, val loss 1.9765005111694336\n",
      "step 4153: train loss 1.8649028539657593, val loss 1.9652009010314941\n",
      "step 4154: train loss 1.8766270875930786, val loss 1.984532356262207\n",
      "step 4155: train loss 1.8722416162490845, val loss 1.9856350421905518\n",
      "step 4156: train loss 1.8656492233276367, val loss 1.9803869724273682\n",
      "step 4157: train loss 1.8767290115356445, val loss 1.9843885898590088\n",
      "step 4158: train loss 1.8622479438781738, val loss 1.9736354351043701\n",
      "step 4159: train loss 1.8650151491165161, val loss 1.9800454378128052\n",
      "step 4160: train loss 1.8612757921218872, val loss 1.9689441919326782\n",
      "step 4161: train loss 1.867667555809021, val loss 1.9770642518997192\n",
      "step 4162: train loss 1.8692622184753418, val loss 1.9694000482559204\n",
      "step 4163: train loss 1.864892601966858, val loss 1.9678001403808594\n",
      "step 4164: train loss 1.8608975410461426, val loss 1.9699255228042603\n",
      "step 4165: train loss 1.8595619201660156, val loss 1.9686181545257568\n",
      "step 4166: train loss 1.8656278848648071, val loss 1.9629011154174805\n",
      "step 4167: train loss 1.8499023914337158, val loss 1.9662853479385376\n",
      "step 4168: train loss 1.8636033535003662, val loss 1.9710137844085693\n",
      "step 4169: train loss 1.8648747205734253, val loss 1.983021855354309\n",
      "step 4170: train loss 1.8706538677215576, val loss 1.978959083557129\n",
      "step 4171: train loss 1.8593344688415527, val loss 1.9896399974822998\n",
      "step 4172: train loss 1.8668241500854492, val loss 1.9800102710723877\n",
      "step 4173: train loss 1.8634848594665527, val loss 1.9809789657592773\n",
      "step 4174: train loss 1.8703097105026245, val loss 1.978331446647644\n",
      "step 4175: train loss 1.8743078708648682, val loss 1.9825481176376343\n",
      "step 4176: train loss 1.8650941848754883, val loss 1.9802334308624268\n",
      "step 4177: train loss 1.8708893060684204, val loss 1.9764233827590942\n",
      "step 4178: train loss 1.863507866859436, val loss 1.9824429750442505\n",
      "step 4179: train loss 1.8744679689407349, val loss 1.9813885688781738\n",
      "step 4180: train loss 1.878797173500061, val loss 1.9777010679244995\n",
      "step 4181: train loss 1.8778300285339355, val loss 1.9801424741744995\n",
      "step 4182: train loss 1.8603748083114624, val loss 1.9726083278656006\n",
      "step 4183: train loss 1.862297534942627, val loss 1.9771488904953003\n",
      "step 4184: train loss 1.8584285974502563, val loss 1.975612998008728\n",
      "step 4185: train loss 1.8733978271484375, val loss 1.980072021484375\n",
      "step 4186: train loss 1.8746529817581177, val loss 1.98496413230896\n",
      "step 4187: train loss 1.8670834302902222, val loss 1.9813597202301025\n",
      "step 4188: train loss 1.860593318939209, val loss 1.982667088508606\n",
      "step 4189: train loss 1.8683031797409058, val loss 1.9871200323104858\n",
      "step 4190: train loss 1.8628795146942139, val loss 1.9781253337860107\n",
      "step 4191: train loss 1.861061453819275, val loss 1.9868746995925903\n",
      "step 4192: train loss 1.8636267185211182, val loss 1.9789559841156006\n",
      "step 4193: train loss 1.8534362316131592, val loss 1.988837718963623\n",
      "step 4194: train loss 1.8765579462051392, val loss 1.9731310606002808\n",
      "step 4195: train loss 1.8592660427093506, val loss 1.989444613456726\n",
      "step 4196: train loss 1.860674262046814, val loss 1.9789373874664307\n",
      "step 4197: train loss 1.8654903173446655, val loss 1.9745498895645142\n",
      "step 4198: train loss 1.8551121950149536, val loss 1.9752594232559204\n",
      "step 4199: train loss 1.8562065362930298, val loss 1.9747631549835205\n",
      "step 4201: train loss 1.864820957183838, val loss 1.978727102279663\n",
      "step 4202: train loss 1.86405611038208, val loss 1.9850808382034302\n",
      "step 4203: train loss 1.8703982830047607, val loss 1.9809108972549438\n",
      "step 4204: train loss 1.8629403114318848, val loss 1.9722120761871338\n",
      "step 4205: train loss 1.861128568649292, val loss 1.9853980541229248\n",
      "step 4206: train loss 1.8665722608566284, val loss 1.9770371913909912\n",
      "step 4207: train loss 1.8711057901382446, val loss 1.9746969938278198\n",
      "step 4208: train loss 1.8608204126358032, val loss 1.9592775106430054\n",
      "step 4209: train loss 1.8571217060089111, val loss 1.9637013673782349\n",
      "step 4210: train loss 1.8546048402786255, val loss 1.9668939113616943\n",
      "step 4211: train loss 1.8641585111618042, val loss 1.971125602722168\n",
      "step 4212: train loss 1.858737587928772, val loss 1.9609801769256592\n",
      "step 4213: train loss 1.8613295555114746, val loss 1.9762492179870605\n",
      "step 4214: train loss 1.872739553451538, val loss 1.9785895347595215\n",
      "step 4215: train loss 1.8669015169143677, val loss 1.9823145866394043\n",
      "step 4216: train loss 1.8569337129592896, val loss 1.9732692241668701\n",
      "step 4217: train loss 1.8645944595336914, val loss 1.9777101278305054\n",
      "step 4218: train loss 1.8708654642105103, val loss 1.976743221282959\n",
      "step 4219: train loss 1.8705246448516846, val loss 1.9871652126312256\n",
      "step 4220: train loss 1.8715859651565552, val loss 1.9797255992889404\n",
      "step 4221: train loss 1.8664790391921997, val loss 1.9662894010543823\n",
      "step 4222: train loss 1.8611984252929688, val loss 1.9815545082092285\n",
      "step 4223: train loss 1.8620500564575195, val loss 1.978053092956543\n",
      "step 4224: train loss 1.8674687147140503, val loss 1.977249026298523\n",
      "step 4225: train loss 1.8705453872680664, val loss 1.9740955829620361\n",
      "step 4226: train loss 1.863150954246521, val loss 1.982841968536377\n",
      "step 4227: train loss 1.8593940734863281, val loss 1.9764022827148438\n",
      "step 4228: train loss 1.859643578529358, val loss 1.9763020277023315\n",
      "step 4229: train loss 1.8533387184143066, val loss 1.9692670106887817\n",
      "step 4230: train loss 1.8574386835098267, val loss 1.964839220046997\n",
      "step 4231: train loss 1.8599867820739746, val loss 1.9780831336975098\n",
      "step 4232: train loss 1.8623909950256348, val loss 1.9774516820907593\n",
      "step 4233: train loss 1.858351230621338, val loss 1.985406517982483\n",
      "step 4234: train loss 1.8667209148406982, val loss 1.9645277261734009\n",
      "step 4235: train loss 1.8624147176742554, val loss 1.973493218421936\n",
      "step 4236: train loss 1.8674066066741943, val loss 1.9834164381027222\n",
      "step 4237: train loss 1.8635187149047852, val loss 1.9825283288955688\n",
      "step 4238: train loss 1.8734664916992188, val loss 1.9717837572097778\n",
      "step 4239: train loss 1.8699089288711548, val loss 1.9932360649108887\n",
      "step 4240: train loss 1.86395263671875, val loss 1.976515293121338\n",
      "step 4241: train loss 1.8633090257644653, val loss 1.97859525680542\n",
      "step 4242: train loss 1.8665199279785156, val loss 1.9764546155929565\n",
      "step 4243: train loss 1.8731260299682617, val loss 1.9770272970199585\n",
      "step 4244: train loss 1.8735630512237549, val loss 1.9805835485458374\n",
      "step 4245: train loss 1.869411826133728, val loss 1.9734073877334595\n",
      "step 4246: train loss 1.8664681911468506, val loss 1.96657133102417\n",
      "step 4247: train loss 1.8604005575180054, val loss 1.9564611911773682\n",
      "step 4248: train loss 1.8552830219268799, val loss 1.9628950357437134\n",
      "step 4249: train loss 1.8558518886566162, val loss 1.9528714418411255\n",
      "step 4250: train loss 1.853278636932373, val loss 1.9609817266464233\n",
      "step 4251: train loss 1.8527377843856812, val loss 1.962361454963684\n",
      "step 4252: train loss 1.8569852113723755, val loss 1.9755233526229858\n",
      "step 4253: train loss 1.8563793897628784, val loss 1.9625046253204346\n",
      "step 4254: train loss 1.8537253141403198, val loss 1.9562199115753174\n",
      "step 4255: train loss 1.8583550453186035, val loss 1.9492405652999878\n",
      "step 4256: train loss 1.847110629081726, val loss 1.9662013053894043\n",
      "step 4257: train loss 1.8568246364593506, val loss 1.9604531526565552\n",
      "step 4258: train loss 1.8632556200027466, val loss 1.968860149383545\n",
      "step 4259: train loss 1.8638347387313843, val loss 1.9743292331695557\n",
      "step 4260: train loss 1.8627362251281738, val loss 1.9685956239700317\n",
      "step 4261: train loss 1.860477089881897, val loss 1.9617801904678345\n",
      "step 4262: train loss 1.8747966289520264, val loss 1.9762046337127686\n",
      "step 4263: train loss 1.8558013439178467, val loss 1.9541698694229126\n",
      "step 4264: train loss 1.8573288917541504, val loss 1.9653056859970093\n",
      "step 4265: train loss 1.8553508520126343, val loss 1.9597506523132324\n",
      "step 4266: train loss 1.858466386795044, val loss 1.9754620790481567\n",
      "step 4267: train loss 1.8551439046859741, val loss 1.9664549827575684\n",
      "step 4268: train loss 1.8574588298797607, val loss 1.9606784582138062\n",
      "step 4269: train loss 1.8543038368225098, val loss 1.9777320623397827\n",
      "step 4270: train loss 1.8609539270401, val loss 1.9723596572875977\n",
      "step 4271: train loss 1.861196756362915, val loss 1.9718072414398193\n",
      "step 4272: train loss 1.8662660121917725, val loss 1.9674192667007446\n",
      "step 4273: train loss 1.8660966157913208, val loss 1.9708706140518188\n",
      "step 4274: train loss 1.8611016273498535, val loss 1.9750828742980957\n",
      "step 4275: train loss 1.8566246032714844, val loss 1.9673527479171753\n",
      "step 4276: train loss 1.8571875095367432, val loss 1.9673022031784058\n",
      "step 4277: train loss 1.8574436902999878, val loss 1.9684315919876099\n",
      "step 4278: train loss 1.860507845878601, val loss 1.968178391456604\n",
      "step 4279: train loss 1.8597608804702759, val loss 1.965757131576538\n",
      "step 4280: train loss 1.8585522174835205, val loss 1.9644581079483032\n",
      "step 4281: train loss 1.8510736227035522, val loss 1.9744094610214233\n",
      "step 4282: train loss 1.8607373237609863, val loss 1.965281367301941\n",
      "step 4283: train loss 1.854846477508545, val loss 1.964463233947754\n",
      "step 4284: train loss 1.861207127571106, val loss 1.9683531522750854\n",
      "step 4285: train loss 1.8691072463989258, val loss 1.9742918014526367\n",
      "step 4286: train loss 1.8662850856781006, val loss 1.9767472743988037\n",
      "step 4287: train loss 1.8512861728668213, val loss 1.9741650819778442\n",
      "step 4288: train loss 1.87294602394104, val loss 1.9637401103973389\n",
      "step 4289: train loss 1.8641639947891235, val loss 1.9720417261123657\n",
      "step 4290: train loss 1.8772891759872437, val loss 1.976352334022522\n",
      "step 4291: train loss 1.862829566001892, val loss 1.9803566932678223\n",
      "step 4292: train loss 1.871168851852417, val loss 1.974190354347229\n",
      "step 4293: train loss 1.8623244762420654, val loss 1.9761329889297485\n",
      "step 4294: train loss 1.8587864637374878, val loss 1.9747425317764282\n",
      "step 4295: train loss 1.8644845485687256, val loss 1.9806653261184692\n",
      "step 4296: train loss 1.8571745157241821, val loss 1.9806764125823975\n",
      "step 4297: train loss 1.8621524572372437, val loss 1.977462887763977\n",
      "step 4298: train loss 1.863680362701416, val loss 1.9681683778762817\n",
      "step 4299: train loss 1.8577957153320312, val loss 1.9694098234176636\n",
      "step 4301: train loss 1.8575069904327393, val loss 1.970850944519043\n",
      "step 4302: train loss 1.8571579456329346, val loss 1.963645577430725\n",
      "step 4303: train loss 1.8689980506896973, val loss 1.9728976488113403\n",
      "step 4304: train loss 1.8608158826828003, val loss 1.9726135730743408\n",
      "step 4305: train loss 1.8663644790649414, val loss 1.9681280851364136\n",
      "step 4306: train loss 1.8604769706726074, val loss 1.9595916271209717\n",
      "step 4307: train loss 1.8522834777832031, val loss 1.9658383131027222\n",
      "step 4308: train loss 1.8604618310928345, val loss 1.9726977348327637\n",
      "step 4309: train loss 1.8665846586227417, val loss 1.9761391878128052\n",
      "step 4310: train loss 1.8661998510360718, val loss 1.9751356840133667\n",
      "step 4311: train loss 1.8585035800933838, val loss 1.9775582551956177\n",
      "step 4312: train loss 1.8666919469833374, val loss 1.9732041358947754\n",
      "step 4313: train loss 1.8531302213668823, val loss 1.963219404220581\n",
      "step 4314: train loss 1.8643486499786377, val loss 1.9649494886398315\n",
      "step 4315: train loss 1.8636170625686646, val loss 1.9709783792495728\n",
      "step 4316: train loss 1.8616819381713867, val loss 1.972536325454712\n",
      "step 4317: train loss 1.8631232976913452, val loss 1.978163719177246\n",
      "step 4318: train loss 1.8645524978637695, val loss 1.9699839353561401\n",
      "step 4319: train loss 1.8603057861328125, val loss 1.9741761684417725\n",
      "step 4320: train loss 1.8646583557128906, val loss 1.9649299383163452\n",
      "step 4321: train loss 1.8566772937774658, val loss 1.961936354637146\n",
      "step 4322: train loss 1.8553963899612427, val loss 1.9609407186508179\n",
      "step 4323: train loss 1.8630859851837158, val loss 1.9696348905563354\n",
      "step 4324: train loss 1.851331353187561, val loss 1.9753495454788208\n",
      "step 4325: train loss 1.861362338066101, val loss 1.967523455619812\n",
      "step 4326: train loss 1.8638399839401245, val loss 1.969459891319275\n",
      "step 4327: train loss 1.8628686666488647, val loss 1.970129132270813\n",
      "step 4328: train loss 1.85039484500885, val loss 1.9696366786956787\n",
      "step 4329: train loss 1.8580704927444458, val loss 1.971031904220581\n",
      "step 4330: train loss 1.8598958253860474, val loss 1.9674705266952515\n",
      "step 4331: train loss 1.8509653806686401, val loss 1.9626165628433228\n",
      "step 4332: train loss 1.852200984954834, val loss 1.9598497152328491\n",
      "step 4333: train loss 1.8477282524108887, val loss 1.9705286026000977\n",
      "step 4334: train loss 1.8534873723983765, val loss 1.9682226181030273\n",
      "step 4335: train loss 1.853335976600647, val loss 1.9571294784545898\n",
      "step 4336: train loss 1.8550549745559692, val loss 1.96965754032135\n",
      "step 4337: train loss 1.8521058559417725, val loss 1.9659554958343506\n",
      "step 4338: train loss 1.8550634384155273, val loss 1.9674491882324219\n",
      "step 4339: train loss 1.8584208488464355, val loss 1.9692672491073608\n",
      "step 4340: train loss 1.8641810417175293, val loss 1.9724092483520508\n",
      "step 4341: train loss 1.8561358451843262, val loss 1.9605656862258911\n",
      "step 4342: train loss 1.8632594347000122, val loss 1.9579352140426636\n",
      "step 4343: train loss 1.8563697338104248, val loss 1.958205223083496\n",
      "step 4344: train loss 1.8633124828338623, val loss 1.963271975517273\n",
      "step 4345: train loss 1.8531081676483154, val loss 1.964841604232788\n",
      "step 4346: train loss 1.8510634899139404, val loss 1.9683090448379517\n",
      "step 4347: train loss 1.8536019325256348, val loss 1.9580307006835938\n",
      "step 4348: train loss 1.8529555797576904, val loss 1.9693626165390015\n",
      "step 4349: train loss 1.860990047454834, val loss 1.968940258026123\n",
      "step 4350: train loss 1.8722186088562012, val loss 1.9727996587753296\n",
      "step 4351: train loss 1.8734445571899414, val loss 1.9713815450668335\n",
      "step 4352: train loss 1.8671337366104126, val loss 1.9666545391082764\n",
      "step 4353: train loss 1.8587075471878052, val loss 1.961215853691101\n",
      "step 4354: train loss 1.8611754179000854, val loss 1.974095106124878\n",
      "step 4355: train loss 1.8614754676818848, val loss 1.9673354625701904\n",
      "step 4356: train loss 1.8572218418121338, val loss 1.965185284614563\n",
      "step 4357: train loss 1.856624960899353, val loss 1.9699403047561646\n",
      "step 4358: train loss 1.857293963432312, val loss 1.9737880229949951\n",
      "step 4359: train loss 1.84373140335083, val loss 1.9736390113830566\n",
      "step 4360: train loss 1.8643555641174316, val loss 1.9667959213256836\n",
      "step 4361: train loss 1.849816083908081, val loss 1.9564411640167236\n",
      "step 4362: train loss 1.8612443208694458, val loss 1.9689700603485107\n",
      "step 4363: train loss 1.8583139181137085, val loss 1.9721314907073975\n",
      "step 4364: train loss 1.860242486000061, val loss 1.9726895093917847\n",
      "step 4365: train loss 1.862213134765625, val loss 1.974631905555725\n",
      "step 4366: train loss 1.8636436462402344, val loss 1.9721347093582153\n",
      "step 4367: train loss 1.8583014011383057, val loss 1.9789345264434814\n",
      "step 4368: train loss 1.8659521341323853, val loss 1.9820629358291626\n",
      "step 4369: train loss 1.8720784187316895, val loss 1.9673031568527222\n",
      "step 4370: train loss 1.8597155809402466, val loss 1.9661248922348022\n",
      "step 4371: train loss 1.8562506437301636, val loss 1.9692023992538452\n",
      "step 4372: train loss 1.8557173013687134, val loss 1.9507893323898315\n",
      "step 4373: train loss 1.8571820259094238, val loss 1.9581304788589478\n",
      "step 4374: train loss 1.8609094619750977, val loss 1.9621963500976562\n",
      "step 4375: train loss 1.8585046529769897, val loss 1.9694546461105347\n",
      "step 4376: train loss 1.8648146390914917, val loss 1.9543637037277222\n",
      "step 4377: train loss 1.8482147455215454, val loss 1.9664719104766846\n",
      "step 4378: train loss 1.8532803058624268, val loss 1.959610939025879\n",
      "step 4379: train loss 1.849173903465271, val loss 1.9625799655914307\n",
      "step 4380: train loss 1.8540230989456177, val loss 1.9526280164718628\n",
      "step 4381: train loss 1.8455256223678589, val loss 1.9585293531417847\n",
      "step 4382: train loss 1.8479931354522705, val loss 1.9605135917663574\n",
      "step 4383: train loss 1.847021222114563, val loss 1.9602508544921875\n",
      "step 4384: train loss 1.841064453125, val loss 1.9658427238464355\n",
      "step 4385: train loss 1.8519814014434814, val loss 1.9587360620498657\n",
      "step 4386: train loss 1.8584383726119995, val loss 1.9529452323913574\n",
      "step 4387: train loss 1.8492704629898071, val loss 1.956600308418274\n",
      "step 4388: train loss 1.8551946878433228, val loss 1.9494515657424927\n",
      "step 4389: train loss 1.8444268703460693, val loss 1.956895351409912\n",
      "step 4390: train loss 1.8600730895996094, val loss 1.9515458345413208\n",
      "step 4391: train loss 1.8537876605987549, val loss 1.9457248449325562\n",
      "step 4392: train loss 1.8489128351211548, val loss 1.947695255279541\n",
      "step 4393: train loss 1.858238935470581, val loss 1.9468159675598145\n",
      "step 4394: train loss 1.856442928314209, val loss 1.9556305408477783\n",
      "step 4395: train loss 1.8535770177841187, val loss 1.9583314657211304\n",
      "step 4396: train loss 1.8532872200012207, val loss 1.9578423500061035\n",
      "step 4397: train loss 1.8458479642868042, val loss 1.9673973321914673\n",
      "step 4398: train loss 1.850305438041687, val loss 1.9579997062683105\n",
      "step 4399: train loss 1.844697117805481, val loss 1.962062954902649\n",
      "step 4401: train loss 1.85040283203125, val loss 1.9639136791229248\n",
      "step 4402: train loss 1.8531662225723267, val loss 1.9551270008087158\n",
      "step 4403: train loss 1.8500099182128906, val loss 1.9633738994598389\n",
      "step 4404: train loss 1.8461952209472656, val loss 1.9662671089172363\n",
      "step 4405: train loss 1.8568421602249146, val loss 1.9489001035690308\n",
      "step 4406: train loss 1.8565438985824585, val loss 1.9705758094787598\n",
      "step 4407: train loss 1.8553102016448975, val loss 1.951291799545288\n",
      "step 4408: train loss 1.8534796237945557, val loss 1.9562897682189941\n",
      "step 4409: train loss 1.844925880432129, val loss 1.9652169942855835\n",
      "step 4410: train loss 1.8481990098953247, val loss 1.9582312107086182\n",
      "step 4411: train loss 1.8456840515136719, val loss 1.9632539749145508\n",
      "step 4412: train loss 1.8662844896316528, val loss 1.9650123119354248\n",
      "step 4413: train loss 1.861169695854187, val loss 1.953376293182373\n",
      "step 4414: train loss 1.854441523551941, val loss 1.9698048830032349\n",
      "step 4415: train loss 1.8529307842254639, val loss 1.961421012878418\n",
      "step 4416: train loss 1.8467451333999634, val loss 1.9623440504074097\n",
      "step 4417: train loss 1.843888282775879, val loss 1.9543603658676147\n",
      "step 4418: train loss 1.8481664657592773, val loss 1.961439847946167\n",
      "step 4419: train loss 1.8454126119613647, val loss 1.9567621946334839\n",
      "step 4420: train loss 1.8551348447799683, val loss 1.9551060199737549\n",
      "step 4421: train loss 1.8410056829452515, val loss 1.9589221477508545\n",
      "step 4422: train loss 1.8613063097000122, val loss 1.9598777294158936\n",
      "step 4423: train loss 1.8567293882369995, val loss 1.961649775505066\n",
      "step 4424: train loss 1.850704312324524, val loss 1.9668978452682495\n",
      "step 4425: train loss 1.8470228910446167, val loss 1.9661474227905273\n",
      "step 4426: train loss 1.8556054830551147, val loss 1.978867769241333\n",
      "step 4427: train loss 1.8473756313323975, val loss 1.9712563753128052\n",
      "step 4428: train loss 1.8544464111328125, val loss 1.9622611999511719\n",
      "step 4429: train loss 1.847739577293396, val loss 1.968429684638977\n",
      "step 4430: train loss 1.8557134866714478, val loss 1.9611084461212158\n",
      "step 4431: train loss 1.847827434539795, val loss 1.970589280128479\n",
      "step 4432: train loss 1.8612195253372192, val loss 1.9689303636550903\n",
      "step 4433: train loss 1.860419750213623, val loss 1.9727648496627808\n",
      "step 4434: train loss 1.8637564182281494, val loss 1.9735794067382812\n",
      "step 4435: train loss 1.8665814399719238, val loss 1.9769489765167236\n",
      "step 4436: train loss 1.8621926307678223, val loss 1.9814738035202026\n",
      "step 4437: train loss 1.863785982131958, val loss 1.9704867601394653\n",
      "step 4438: train loss 1.8480838537216187, val loss 1.9668089151382446\n",
      "step 4439: train loss 1.8564397096633911, val loss 1.9559202194213867\n",
      "step 4440: train loss 1.845862865447998, val loss 1.961584210395813\n",
      "step 4441: train loss 1.8461581468582153, val loss 1.9536367654800415\n",
      "step 4442: train loss 1.8498226404190063, val loss 1.9682856798171997\n",
      "step 4443: train loss 1.8559653759002686, val loss 1.9633495807647705\n",
      "step 4444: train loss 1.8485609292984009, val loss 1.9648481607437134\n",
      "step 4445: train loss 1.8545279502868652, val loss 1.9736049175262451\n",
      "step 4446: train loss 1.855104684829712, val loss 1.9694503545761108\n",
      "step 4447: train loss 1.8497811555862427, val loss 1.9681789875030518\n",
      "step 4448: train loss 1.8565672636032104, val loss 1.9614278078079224\n",
      "step 4449: train loss 1.8487181663513184, val loss 1.9708104133605957\n",
      "step 4450: train loss 1.8449631929397583, val loss 1.9665640592575073\n",
      "step 4451: train loss 1.8531309366226196, val loss 1.970977783203125\n",
      "step 4452: train loss 1.8407025337219238, val loss 1.960624098777771\n",
      "step 4453: train loss 1.8432931900024414, val loss 1.9635624885559082\n",
      "step 4454: train loss 1.842556118965149, val loss 1.9643487930297852\n",
      "step 4455: train loss 1.8481138944625854, val loss 1.9648301601409912\n",
      "step 4456: train loss 1.861716628074646, val loss 1.9647128582000732\n",
      "step 4457: train loss 1.8642741441726685, val loss 1.96766996383667\n",
      "step 4458: train loss 1.856817603111267, val loss 1.9713873863220215\n",
      "step 4459: train loss 1.853540301322937, val loss 1.97016179561615\n",
      "step 4460: train loss 1.845605731010437, val loss 1.9682114124298096\n",
      "step 4461: train loss 1.8437895774841309, val loss 1.9585158824920654\n",
      "step 4462: train loss 1.8532097339630127, val loss 1.9671133756637573\n",
      "step 4463: train loss 1.8569042682647705, val loss 1.9612531661987305\n",
      "step 4464: train loss 1.8526486158370972, val loss 1.9730099439620972\n",
      "step 4465: train loss 1.8444839715957642, val loss 1.9713973999023438\n",
      "step 4466: train loss 1.8524787425994873, val loss 1.9683406352996826\n",
      "step 4467: train loss 1.8440600633621216, val loss 1.9598318338394165\n",
      "step 4468: train loss 1.8476150035858154, val loss 1.9647070169448853\n",
      "step 4469: train loss 1.8456782102584839, val loss 1.953887939453125\n",
      "step 4470: train loss 1.8479794263839722, val loss 1.962927222251892\n",
      "step 4471: train loss 1.850941777229309, val loss 1.9633395671844482\n",
      "step 4472: train loss 1.8474311828613281, val loss 1.956091284751892\n",
      "step 4473: train loss 1.845963478088379, val loss 1.9603826999664307\n",
      "step 4474: train loss 1.8455194234848022, val loss 1.9615237712860107\n",
      "step 4475: train loss 1.8432735204696655, val loss 1.946262240409851\n",
      "step 4476: train loss 1.839112401008606, val loss 1.9632482528686523\n",
      "step 4477: train loss 1.8500694036483765, val loss 1.9714471101760864\n",
      "step 4478: train loss 1.8497127294540405, val loss 1.9645980596542358\n",
      "step 4479: train loss 1.8620648384094238, val loss 1.9771729707717896\n",
      "step 4480: train loss 1.8622032403945923, val loss 1.9763418436050415\n",
      "step 4481: train loss 1.8556195497512817, val loss 1.9739980697631836\n",
      "step 4482: train loss 1.8373119831085205, val loss 1.9644726514816284\n",
      "step 4483: train loss 1.856595754623413, val loss 1.9627842903137207\n",
      "step 4484: train loss 1.8599917888641357, val loss 1.9666475057601929\n",
      "step 4485: train loss 1.8532624244689941, val loss 1.9675288200378418\n",
      "step 4486: train loss 1.857236385345459, val loss 1.9680662155151367\n",
      "step 4487: train loss 1.858397364616394, val loss 1.9727247953414917\n",
      "step 4488: train loss 1.8554463386535645, val loss 1.9725546836853027\n",
      "step 4489: train loss 1.857892632484436, val loss 1.9689722061157227\n",
      "step 4490: train loss 1.8565362691879272, val loss 1.9623898267745972\n",
      "step 4491: train loss 1.8412823677062988, val loss 1.9547675848007202\n",
      "step 4492: train loss 1.8478453159332275, val loss 1.952675461769104\n",
      "step 4493: train loss 1.8510966300964355, val loss 1.959882378578186\n",
      "step 4494: train loss 1.849524736404419, val loss 1.9597268104553223\n",
      "step 4495: train loss 1.8557909727096558, val loss 1.9674770832061768\n",
      "step 4496: train loss 1.8581554889678955, val loss 1.9601199626922607\n",
      "step 4497: train loss 1.8579988479614258, val loss 1.9622215032577515\n",
      "step 4498: train loss 1.8484011888504028, val loss 1.9657877683639526\n",
      "step 4499: train loss 1.8358123302459717, val loss 1.9538235664367676\n",
      "step 4501: train loss 1.8360766172409058, val loss 1.9611871242523193\n",
      "step 4502: train loss 1.8436273336410522, val loss 1.9654918909072876\n",
      "step 4503: train loss 1.8486508131027222, val loss 1.9692177772521973\n",
      "step 4504: train loss 1.8514535427093506, val loss 1.9624512195587158\n",
      "step 4505: train loss 1.8431543111801147, val loss 1.9586262702941895\n",
      "step 4506: train loss 1.8481863737106323, val loss 1.9627258777618408\n",
      "step 4507: train loss 1.8416117429733276, val loss 1.9555567502975464\n",
      "step 4508: train loss 1.8414905071258545, val loss 1.9409263134002686\n",
      "step 4509: train loss 1.8424537181854248, val loss 1.9426623582839966\n",
      "step 4510: train loss 1.8396354913711548, val loss 1.9413355588912964\n",
      "step 4511: train loss 1.839245319366455, val loss 1.9441217184066772\n",
      "step 4512: train loss 1.8450840711593628, val loss 1.9453445672988892\n",
      "step 4513: train loss 1.8480318784713745, val loss 1.9456766843795776\n",
      "step 4514: train loss 1.8470953702926636, val loss 1.949181318283081\n",
      "step 4515: train loss 1.8522709608078003, val loss 1.9455317258834839\n",
      "step 4516: train loss 1.8477171659469604, val loss 1.953305721282959\n",
      "step 4517: train loss 1.8590803146362305, val loss 1.962663173675537\n",
      "step 4518: train loss 1.8442949056625366, val loss 1.9425662755966187\n",
      "step 4519: train loss 1.841640830039978, val loss 1.9564247131347656\n",
      "step 4520: train loss 1.8426457643508911, val loss 1.94853937625885\n",
      "step 4521: train loss 1.8473318815231323, val loss 1.951035976409912\n",
      "step 4522: train loss 1.8435978889465332, val loss 1.9505847692489624\n",
      "step 4523: train loss 1.8496476411819458, val loss 1.95249342918396\n",
      "step 4524: train loss 1.8397119045257568, val loss 1.9474225044250488\n",
      "step 4525: train loss 1.839792013168335, val loss 1.950515866279602\n",
      "step 4526: train loss 1.8364250659942627, val loss 1.958588719367981\n",
      "step 4527: train loss 1.8286019563674927, val loss 1.9579362869262695\n",
      "step 4528: train loss 1.8413180112838745, val loss 1.9520933628082275\n",
      "step 4529: train loss 1.8379684686660767, val loss 1.9622291326522827\n",
      "step 4530: train loss 1.8455450534820557, val loss 1.9659019708633423\n",
      "step 4531: train loss 1.8502213954925537, val loss 1.9675952196121216\n",
      "step 4532: train loss 1.856979250907898, val loss 1.9586308002471924\n",
      "step 4533: train loss 1.841387152671814, val loss 1.9630719423294067\n",
      "step 4534: train loss 1.839939832687378, val loss 1.9603276252746582\n",
      "step 4535: train loss 1.838782787322998, val loss 1.950868844985962\n",
      "step 4536: train loss 1.8457963466644287, val loss 1.9594943523406982\n",
      "step 4537: train loss 1.8379992246627808, val loss 1.9512357711791992\n",
      "step 4538: train loss 1.8423614501953125, val loss 1.965691328048706\n",
      "step 4539: train loss 1.8456826210021973, val loss 1.9618815183639526\n",
      "step 4540: train loss 1.8480323553085327, val loss 1.9607783555984497\n",
      "step 4541: train loss 1.8485020399093628, val loss 1.963802456855774\n",
      "step 4542: train loss 1.857817530632019, val loss 1.9658352136611938\n",
      "step 4543: train loss 1.8395755290985107, val loss 1.9569774866104126\n",
      "step 4544: train loss 1.8435616493225098, val loss 1.948166847229004\n",
      "step 4545: train loss 1.8356513977050781, val loss 1.9626460075378418\n",
      "step 4546: train loss 1.844661831855774, val loss 1.9591684341430664\n",
      "step 4547: train loss 1.845288872718811, val loss 1.9576095342636108\n",
      "step 4548: train loss 1.8427681922912598, val loss 1.9585561752319336\n",
      "step 4549: train loss 1.8461782932281494, val loss 1.9610515832901\n",
      "step 4550: train loss 1.849950909614563, val loss 1.9631898403167725\n",
      "step 4551: train loss 1.8418086767196655, val loss 1.9547369480133057\n",
      "step 4552: train loss 1.8444480895996094, val loss 1.9641963243484497\n",
      "step 4553: train loss 1.8438752889633179, val loss 1.9554460048675537\n",
      "step 4554: train loss 1.8404762744903564, val loss 1.9549834728240967\n",
      "step 4555: train loss 1.8366954326629639, val loss 1.9634766578674316\n",
      "step 4556: train loss 1.8410651683807373, val loss 1.9588751792907715\n",
      "step 4557: train loss 1.8392382860183716, val loss 1.957977294921875\n",
      "step 4558: train loss 1.8422610759735107, val loss 1.9705302715301514\n",
      "step 4559: train loss 1.8506293296813965, val loss 1.9676862955093384\n",
      "step 4560: train loss 1.8454629182815552, val loss 1.9664945602416992\n",
      "step 4561: train loss 1.8465150594711304, val loss 1.9596705436706543\n",
      "step 4562: train loss 1.8475699424743652, val loss 1.9607936143875122\n",
      "step 4563: train loss 1.8421992063522339, val loss 1.964300513267517\n",
      "step 4564: train loss 1.8399525880813599, val loss 1.965887188911438\n",
      "step 4565: train loss 1.8415489196777344, val loss 1.963693380355835\n",
      "step 4566: train loss 1.8365727663040161, val loss 1.9632599353790283\n",
      "step 4567: train loss 1.8529623746871948, val loss 1.9578548669815063\n",
      "step 4568: train loss 1.8400951623916626, val loss 1.9670666456222534\n",
      "step 4569: train loss 1.8442758321762085, val loss 1.9615566730499268\n",
      "step 4570: train loss 1.83030366897583, val loss 1.9637856483459473\n",
      "step 4571: train loss 1.8381156921386719, val loss 1.9596307277679443\n",
      "step 4572: train loss 1.8341537714004517, val loss 1.9536021947860718\n",
      "step 4573: train loss 1.8361438512802124, val loss 1.9494801759719849\n",
      "step 4574: train loss 1.8410322666168213, val loss 1.9595173597335815\n",
      "step 4575: train loss 1.843997597694397, val loss 1.957023024559021\n",
      "step 4576: train loss 1.8354419469833374, val loss 1.9550392627716064\n",
      "step 4577: train loss 1.853796362876892, val loss 1.95097017288208\n",
      "step 4578: train loss 1.843422532081604, val loss 1.959771990776062\n",
      "step 4579: train loss 1.8256040811538696, val loss 1.9563864469528198\n",
      "step 4580: train loss 1.851237177848816, val loss 1.956272006034851\n",
      "step 4581: train loss 1.8401401042938232, val loss 1.9568164348602295\n",
      "step 4582: train loss 1.845890760421753, val loss 1.9557561874389648\n",
      "step 4583: train loss 1.8438442945480347, val loss 1.9699108600616455\n",
      "step 4584: train loss 1.843273639678955, val loss 1.965822696685791\n",
      "step 4585: train loss 1.8382365703582764, val loss 1.975268840789795\n",
      "step 4586: train loss 1.843031644821167, val loss 1.9536662101745605\n",
      "step 4587: train loss 1.8399231433868408, val loss 1.9617698192596436\n",
      "step 4588: train loss 1.8452547788619995, val loss 1.9548239707946777\n",
      "step 4589: train loss 1.83058500289917, val loss 1.9489331245422363\n",
      "step 4590: train loss 1.8335696458816528, val loss 1.957961082458496\n",
      "step 4591: train loss 1.8336809873580933, val loss 1.9551570415496826\n",
      "step 4592: train loss 1.8478468656539917, val loss 1.9668858051300049\n",
      "step 4593: train loss 1.8420337438583374, val loss 1.9582706689834595\n",
      "step 4594: train loss 1.8483108282089233, val loss 1.9561554193496704\n",
      "step 4595: train loss 1.8467004299163818, val loss 1.9539198875427246\n",
      "step 4596: train loss 1.8417913913726807, val loss 1.9466540813446045\n",
      "step 4597: train loss 1.8417305946350098, val loss 1.9516535997390747\n",
      "step 4598: train loss 1.8358628749847412, val loss 1.94342839717865\n",
      "step 4599: train loss 1.8321729898452759, val loss 1.9429658651351929\n",
      "step 4601: train loss 1.841010332107544, val loss 1.9485470056533813\n",
      "step 4602: train loss 1.8362946510314941, val loss 1.9528902769088745\n",
      "step 4603: train loss 1.8315017223358154, val loss 1.9494796991348267\n",
      "step 4604: train loss 1.8357583284378052, val loss 1.952022671699524\n",
      "step 4605: train loss 1.842844843864441, val loss 1.9512478113174438\n",
      "step 4606: train loss 1.8432621955871582, val loss 1.9650381803512573\n",
      "step 4607: train loss 1.8336292505264282, val loss 1.9557026624679565\n",
      "step 4608: train loss 1.8426858186721802, val loss 1.9558185338974\n",
      "step 4609: train loss 1.8310637474060059, val loss 1.9579765796661377\n",
      "step 4610: train loss 1.8377833366394043, val loss 1.9612981081008911\n",
      "step 4611: train loss 1.8383218050003052, val loss 1.9526764154434204\n",
      "step 4612: train loss 1.8424736261367798, val loss 1.9532562494277954\n",
      "step 4613: train loss 1.8239388465881348, val loss 1.9533636569976807\n",
      "step 4614: train loss 1.8452086448669434, val loss 1.9537967443466187\n",
      "step 4615: train loss 1.842105746269226, val loss 1.9685794115066528\n",
      "step 4616: train loss 1.8388395309448242, val loss 1.964766502380371\n",
      "step 4617: train loss 1.834298849105835, val loss 1.9638872146606445\n",
      "step 4618: train loss 1.8425495624542236, val loss 1.9588384628295898\n",
      "step 4619: train loss 1.8268409967422485, val loss 1.9599997997283936\n",
      "step 4620: train loss 1.8387848138809204, val loss 1.9773712158203125\n",
      "step 4621: train loss 1.8502928018569946, val loss 1.9631427526474\n",
      "step 4622: train loss 1.8428317308425903, val loss 1.970369815826416\n",
      "step 4623: train loss 1.8531891107559204, val loss 1.9738253355026245\n",
      "step 4624: train loss 1.8357356786727905, val loss 1.9553627967834473\n",
      "step 4625: train loss 1.8505383729934692, val loss 1.9667564630508423\n",
      "step 4626: train loss 1.8466432094573975, val loss 1.9525436162948608\n",
      "step 4627: train loss 1.837680697441101, val loss 1.9518910646438599\n",
      "step 4628: train loss 1.8411442041397095, val loss 1.9633251428604126\n",
      "step 4629: train loss 1.843534231185913, val loss 1.9606432914733887\n",
      "step 4630: train loss 1.8471014499664307, val loss 1.959952712059021\n",
      "step 4631: train loss 1.8374736309051514, val loss 1.9564690589904785\n",
      "step 4632: train loss 1.843571424484253, val loss 1.967168927192688\n",
      "step 4633: train loss 1.8375256061553955, val loss 1.9602928161621094\n",
      "step 4634: train loss 1.8406485319137573, val loss 1.965603232383728\n",
      "step 4635: train loss 1.8436172008514404, val loss 1.9638512134552002\n",
      "step 4636: train loss 1.844772219657898, val loss 1.9634262323379517\n",
      "step 4637: train loss 1.832724928855896, val loss 1.9654229879379272\n",
      "step 4638: train loss 1.8355292081832886, val loss 1.9638932943344116\n",
      "step 4639: train loss 1.832713007926941, val loss 1.9628205299377441\n",
      "step 4640: train loss 1.8450682163238525, val loss 1.957899808883667\n",
      "step 4641: train loss 1.829585313796997, val loss 1.9515386819839478\n",
      "step 4642: train loss 1.828018307685852, val loss 1.953951120376587\n",
      "step 4643: train loss 1.832969069480896, val loss 1.9567681550979614\n",
      "step 4644: train loss 1.8324339389801025, val loss 1.948262333869934\n",
      "step 4645: train loss 1.837656855583191, val loss 1.971885323524475\n",
      "step 4646: train loss 1.8363898992538452, val loss 1.9565346240997314\n",
      "step 4647: train loss 1.8306390047073364, val loss 1.9637043476104736\n",
      "step 4648: train loss 1.8356657028198242, val loss 1.9625134468078613\n",
      "step 4649: train loss 1.82599675655365, val loss 1.9568954706192017\n",
      "step 4650: train loss 1.8373544216156006, val loss 1.95493483543396\n",
      "step 4651: train loss 1.827215313911438, val loss 1.9542843103408813\n",
      "step 4652: train loss 1.8393176794052124, val loss 1.954014778137207\n",
      "step 4653: train loss 1.836114525794983, val loss 1.9592161178588867\n",
      "step 4654: train loss 1.8504067659378052, val loss 1.9658137559890747\n",
      "step 4655: train loss 1.8355064392089844, val loss 1.9605536460876465\n",
      "step 4656: train loss 1.8443881273269653, val loss 1.9608111381530762\n",
      "step 4657: train loss 1.8386613130569458, val loss 1.9584022760391235\n",
      "step 4658: train loss 1.8366161584854126, val loss 1.9541763067245483\n",
      "step 4659: train loss 1.830480694770813, val loss 1.9671118259429932\n",
      "step 4660: train loss 1.8288390636444092, val loss 1.9610995054244995\n",
      "step 4661: train loss 1.8344513177871704, val loss 1.9582204818725586\n",
      "step 4662: train loss 1.8299506902694702, val loss 1.9611303806304932\n",
      "step 4663: train loss 1.8400628566741943, val loss 1.9633103609085083\n",
      "step 4664: train loss 1.8275842666625977, val loss 1.9545648097991943\n",
      "step 4665: train loss 1.8302319049835205, val loss 1.9595680236816406\n",
      "step 4666: train loss 1.8326421976089478, val loss 1.9639892578125\n",
      "step 4667: train loss 1.8356224298477173, val loss 1.9603703022003174\n",
      "step 4668: train loss 1.8379522562026978, val loss 1.9504799842834473\n",
      "step 4669: train loss 1.8387295007705688, val loss 1.9625451564788818\n",
      "step 4670: train loss 1.8351714611053467, val loss 1.9559884071350098\n",
      "step 4671: train loss 1.8404284715652466, val loss 1.966485619544983\n",
      "step 4672: train loss 1.8484561443328857, val loss 1.967818260192871\n",
      "step 4673: train loss 1.8423725366592407, val loss 1.9690346717834473\n",
      "step 4674: train loss 1.8336039781570435, val loss 1.965999960899353\n",
      "step 4675: train loss 1.8381811380386353, val loss 1.9670406579971313\n",
      "step 4676: train loss 1.8316351175308228, val loss 1.9703359603881836\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval or iter == max_iters - 1:\n",
    "        losses_train_val = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses_train_val['train']:.4f}, val loss {losses_train_val['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(decode(m\u001b[38;5;241m.\u001b[39mgenerate(context, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=200)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
